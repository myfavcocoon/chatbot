{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4df638a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4df638a0",
        "outputId": "55afcc6d-6fcb-4655-acd3-36caf7dcb1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "yAB_C6g2VSTn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAB_C6g2VSTn",
        "outputId": "7ac0bc8b-5635-4c1a-c1d3-22ac88f033a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/chatbot\n",
            "app_gradio.ipynb\t eval_result\t\t README.md\n",
            "app_gradio_server.ipynb  LICENSE\t\t src\n",
            "data\t\t\t model_evaluation.ipynb  train_models.ipynb\n",
            "data_generating\t\t models\n",
            "data_viz.ipynb\t\t rag_evaluation.ipynb\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1️⃣ Chuyển working directory tới root project (nơi có thư mục src)\n",
        "%cd /content/drive/MyDrive/chatbot\n",
        "\n",
        "# 2️⃣ Thêm folder root vào sys.path để Python tìm src\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "# 3️⃣ Kiểm tra\n",
        "!ls\n",
        "# Nên thấy thư mục src, app_gradio.py, notebooks, v.v.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "iTKvwrjTtSwA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTKvwrjTtSwA",
        "outputId": "ea43d332-c492-4498-9db3-1adb7a775313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/146.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/598.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m745.9/745.9 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.9/280.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 0.0.1 requires google-generativeai<0.4.0,>=0.3.1, but you have google-generativeai 0.8.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-google-genai google-generativeai transformers torch sentence-transformers pinecone gradio peft bitsandbytes accelerate pymongo --quiet\n",
        "!pip install pinecone --upgrade --quiet\n",
        "!pip install -U bitsandbytes --quiet\n",
        "!pip install rank-bm25 unidecode -q\n",
        "!pip install rouge-score bert-score sacrebleu -q\n",
        "# !pip install ragas\n",
        "# !pip install deepeval\n",
        "!pip install --upgrade google-generativeai -q\n",
        "!python -m pip install protobuf==4.25.3 -q\n",
        "!pip install rapidfuzz -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K4hTE-oSoCEi",
      "metadata": {
        "id": "K4hTE-oSoCEi"
      },
      "source": [
        "# Make Response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DSj41yk3ourX",
      "metadata": {
        "id": "DSj41yk3ourX"
      },
      "source": [
        "### Base Qwen2-3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "huFBu9Dfop1B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "95844ba924e6411b848700ba2d509926",
            "7f813c407dfb4ee89dfe79ec99384da4",
            "452a1efbb1d04c72921c82243dc77f83",
            "af8f8b1513184ab7aba07d347cfdb706",
            "137e63b93f644eca99ebf661a4ac72b1",
            "73bc661b9af24612b3a042930596b5b4",
            "1b6399de21d449f9aad5df3c9af038f8",
            "5809fba40e504616b7e8f99b754ef971",
            "0bef3b85ad5744d9bf8a1f7a1299f2fc",
            "15b3b2bd17ed43b889eacbe47cbfb00f",
            "af18bed4a48349b6b77bdfce7f1589fa"
          ]
        },
        "id": "huFBu9Dfop1B",
        "outputId": "a3e93ac9-c6fa-43de-df23-cb6aefb34ebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from Hugging Face in 4-bit mode...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1041: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95844ba924e6411b848700ba2d509926",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "  3%|▎         | 10/300 [04:23<2:37:54, 32.67s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "100%|██████████| 300/300 [2:32:15<00:00, 30.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 300 rows to data/qwen2_3b_4bit_eval_data.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "from src.logic_module import bm25_retriever, embedding_model, custom_template, build_context\n",
        "from src.config import HF_TOKEN  # Hugging Face token\n",
        "\n",
        "# -------------------- Config --------------------\n",
        "MODEL_HF = \"/AITeamVN/Vi-Qwen2-3B-RAG\" \n",
        "INPUT_CSV = \"data/merged_all.csv\"\n",
        "OUTPUT_CSV = \"data/base_qwen2_3b_eval_data.csv\"\n",
        "NUM_ROWS = 300       # số dòng muốn chạy\n",
        "SEED = 42\n",
        "MAX_TOKENS = 512\n",
        "\n",
        "# -------------------- Load data --------------------\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "random.seed(SEED)\n",
        "sampled_rows = df.sample(n=min(NUM_ROWS, len(df)), random_state=SEED)\n",
        "\n",
        "# -------------------- Load HF model & tokenizer (4-bit) --------------------\n",
        "print(\"Loading model from Hugging Face in 4-bit mode...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        MODEL_HF,\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=HF_TOKEN\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_HF,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",   # auto map GPU\n",
        "        load_in_4bit=True,   # 4-bit quantization\n",
        "        use_auth_token=HF_TOKEN\n",
        "    )\n",
        "    # Khi dùng device_map + 4bit, không truyền device vào pipeline\n",
        "    gen_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=MAX_TOKENS\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model in 4-bit GPU, fallback to CPU: {e}\")\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        MODEL_HF,\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=HF_TOKEN\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_HF,\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=HF_TOKEN\n",
        "    )\n",
        "    gen_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=MAX_TOKENS,\n",
        "        device=-1  # CPU\n",
        "    )\n",
        "\n",
        "# -------------------- Run inference --------------------\n",
        "results = []\n",
        "retrieval_cache = []\n",
        "\n",
        "for _, row in tqdm(sampled_rows.iterrows(), total=len(sampled_rows)):\n",
        "    user_input = row.get(\"text\") or row.get(\"question\") or \"\"\n",
        "    if not user_input.strip():\n",
        "        continue\n",
        "\n",
        "    # Build context using BM25 + embeddings\n",
        "    context, refs, retrieval_cache, _, _ = build_context(\n",
        "        user_input,\n",
        "        retrieval_cache=retrieval_cache,\n",
        "        bm25_retriever=bm25_retriever,\n",
        "        embedding_model=embedding_model\n",
        "    )\n",
        "\n",
        "    # Build full prompt with template\n",
        "    full_prompt = custom_template.format(\n",
        "        context=context,\n",
        "        history=\"Không có hội thoại trước.\",\n",
        "        input=user_input\n",
        "    )\n",
        "\n",
        "    # Generate response\n",
        "    try:\n",
        "        ans_full = gen_pipe(full_prompt)[0][\"generated_text\"]\n",
        "        split_token = \"### Trả lời:\"\n",
        "        ans = ans_full.split(split_token, 1)[1].strip() if split_token in ans_full else ans_full.strip()\n",
        "    except Exception as e:\n",
        "        ans = f\"[ERROR generating response: {e}]\"\n",
        "\n",
        "    results.append({\n",
        "        \"input\": user_input,\n",
        "        \"context\": context,\n",
        "        \"response\": ans\n",
        "    })\n",
        "\n",
        "# -------------------- Save output --------------------\n",
        "pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"Saved {len(results)} rows to {OUTPUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OyLr_L05qV5r",
      "metadata": {
        "id": "OyLr_L05qV5r"
      },
      "source": [
        "### Qwen2-3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fk3hsxXHoBq1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "5d4ea5df5b604643a5d5f86f98253b81"
          ]
        },
        "id": "Fk3hsxXHoBq1",
        "outputId": "0ed443a3-4910-4356-8dd3-475f289f60aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1041: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d4ea5df5b604643a5d5f86f98253b81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n",
            "  3%|▎         | 10/300 [01:33<45:10,  9.35s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "100%|██████████| 300/300 [47:09<00:00,  9.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 300 rows to data/qwen2-3b_eval_data.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "from src.logic_module import build_pipeline, bm25_retriever, embedding_model, custom_template, build_context\n",
        "\n",
        "# -------------------- Config --------------------\n",
        "MODEL_KEY = \"qwen2-3b\"\n",
        "INPUT_CSV = \"data/merged_all.csv\"\n",
        "OUTPUT_CSV = f\"data/{MODEL_KEY}_eval_data.csv\"\n",
        "NUM_ROWS = 300       # số dòng muốn chạy\n",
        "SEED = 42\n",
        "MAX_TOKENS = 512\n",
        "\n",
        "# -------------------- Load data --------------------\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "random.seed(SEED)\n",
        "sampled_rows = df.sample(n=min(NUM_ROWS, len(df)), random_state=SEED)\n",
        "\n",
        "# -------------------- Load model pipeline --------------------\n",
        "# Chạy CPU safe (nếu GPU không đủ VRAM)\n",
        "try:\n",
        "    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n",
        "except Exception as e:\n",
        "    print(\"Error loading pipeline, fallback to CPU\")\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable GPU\n",
        "    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n",
        "\n",
        "# -------------------- Run inference --------------------\n",
        "results = []\n",
        "retrieval_cache = []  # cache phải là list để build_context append được\n",
        "\n",
        "for _, row in tqdm(sampled_rows.iterrows(), total=len(sampled_rows)):\n",
        "    # Giả sử có cột 'text' hoặc 'question'\n",
        "    user_input = row.get(\"text\") or row.get(\"question\") or \"\"\n",
        "    if not user_input.strip():\n",
        "        continue\n",
        "\n",
        "    # Build context\n",
        "    context, refs, retrieval_cache, _, _ = build_context(\n",
        "        user_input,\n",
        "        retrieval_cache=retrieval_cache,\n",
        "        bm25_retriever=bm25_retriever,\n",
        "        embedding_model=embedding_model\n",
        "    )\n",
        "\n",
        "    # Build full prompt\n",
        "    full_prompt = custom_template.format(\n",
        "        context=context,\n",
        "        history=\"Không có hội thoại trước.\",\n",
        "        input=user_input\n",
        "    )\n",
        "\n",
        "    # Generate response\n",
        "    try:\n",
        "        ans_full = gen_pipe(full_prompt)[0][\"generated_text\"]\n",
        "        split_token = \"### Trả lời:\"\n",
        "        ans = ans_full.split(split_token, 1)[1].strip() if split_token in ans_full else ans_full.strip()\n",
        "    except Exception as e:\n",
        "        ans = f\"[ERROR generating response: {e}]\"\n",
        "\n",
        "    results.append({\n",
        "        \"input\": user_input,\n",
        "        \"context\": context,\n",
        "        \"response\": ans\n",
        "    })\n",
        "\n",
        "# -------------------- Save output --------------------\n",
        "pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"Saved {len(results)} rows to {OUTPUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MPpC57BbqaTt",
      "metadata": {
        "id": "MPpC57BbqaTt"
      },
      "source": [
        "### LLama-3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1T-lwmIqY1Z",
      "metadata": {
        "id": "c1T-lwmIqY1Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "from src.logic_module import build_pipeline, bm25_retriever, embedding_model, custom_template, build_context\n",
        "\n",
        "# -------------------- Config --------------------\n",
        "MODEL_KEY = \"llama-3b\"\n",
        "INPUT_CSV = \"data/merged_all.csv\"\n",
        "OUTPUT_CSV = f\"data/{MODEL_KEY}_eval_data.csv\"\n",
        "NUM_ROWS = 300       # số dòng muốn chạy\n",
        "SEED = 42\n",
        "MAX_TOKENS = 512     # giảm để tránh OOM trên GPU/CPU\n",
        "\n",
        "# -------------------- Load data --------------------\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "random.seed(SEED)\n",
        "sampled_rows = df.sample(n=min(NUM_ROWS, len(df)), random_state=SEED)\n",
        "\n",
        "# -------------------- Load model pipeline --------------------\n",
        "# Chạy CPU safe (nếu GPU không đủ VRAM)\n",
        "try:\n",
        "    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n",
        "except Exception as e:\n",
        "    print(\"Error loading pipeline, fallback to CPU\")\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable GPU\n",
        "    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n",
        "\n",
        "# -------------------- Run inference --------------------\n",
        "results = []\n",
        "retrieval_cache = []  # cache phải là list để build_context append được\n",
        "\n",
        "for _, row in tqdm(sampled_rows.iterrows(), total=len(sampled_rows)):\n",
        "    # Giả sử có cột 'text' hoặc 'question'\n",
        "    user_input = row.get(\"text\") or row.get(\"question\") or \"\"\n",
        "    if not user_input.strip():\n",
        "        continue\n",
        "\n",
        "    # Build context\n",
        "    context, refs, retrieval_cache, _, _ = build_context(\n",
        "        user_input,\n",
        "        retrieval_cache=retrieval_cache,\n",
        "        bm25_retriever=bm25_retriever,\n",
        "        embedding_model=embedding_model\n",
        "    )\n",
        "\n",
        "    # Build full prompt\n",
        "    full_prompt = custom_template.format(\n",
        "        context=context,\n",
        "        history=\"Không có hội thoại trước.\",\n",
        "        input=user_input\n",
        "    )\n",
        "\n",
        "    # Generate response\n",
        "    try:\n",
        "        ans_full = gen_pipe(full_prompt)[0][\"generated_text\"]\n",
        "        split_token = \"### Trả lời:\"\n",
        "        ans = ans_full.split(split_token, 1)[1].strip() if split_token in ans_full else ans_full.strip()\n",
        "    except Exception as e:\n",
        "        ans = f\"[ERROR generating response: {e}]\"\n",
        "\n",
        "    results.append({\n",
        "        \"input\": user_input,\n",
        "        \"context\": context,\n",
        "        \"response\": ans\n",
        "    })\n",
        "\n",
        "# -------------------- Save output --------------------\n",
        "pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"Saved {len(results)} rows to {OUTPUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WPVcyXxU3-VR",
      "metadata": {
        "id": "WPVcyXxU3-VR"
      },
      "source": [
        "### Qwen2-7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AsooHbZaqd5u",
      "metadata": {
        "id": "AsooHbZaqd5u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "from src.logic_module import build_pipeline, bm25_retriever, embedding_model, custom_template, build_context\n",
        "\n",
        "# -------------------- Config --------------------\n",
        "MODEL_KEY = \"qwen2-7b\"\n",
        "INPUT_CSV = \"data/merged_all.csv\"\n",
        "OUTPUT_CSV = f\"data/{MODEL_KEY}_eval_data.csv\"\n",
        "NUM_ROWS = 300       # số dòng muốn chạy\n",
        "SEED = 42\n",
        "MAX_TOKENS = 512     # giảm để tránh OOM trên GPU/CPU\n",
        "\n",
        "# -------------------- Load data --------------------\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "random.seed(SEED)\n",
        "sampled_rows = df.sample(n=min(NUM_ROWS, len(df)), random_state=SEED)\n",
        "\n",
        "# -------------------- Load model pipeline --------------------\n",
        "# Chạy CPU safe (nếu GPU không đủ VRAM)\n",
        "try:\n",
        "    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n",
        "except Exception as e:\n",
        "    print(\"Error loading pipeline, fallback to CPU\")\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable GPU\n",
        "    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n",
        "\n",
        "# -------------------- Run inference --------------------\n",
        "results = []\n",
        "retrieval_cache = []  # cache phải là list để build_context append được\n",
        "\n",
        "for _, row in tqdm(sampled_rows.iterrows(), total=len(sampled_rows)):\n",
        "    # Giả sử có cột 'text' hoặc 'question'\n",
        "    user_input = row.get(\"text\") or row.get(\"question\") or \"\"\n",
        "    if not user_input.strip():\n",
        "        continue\n",
        "\n",
        "    # Build context\n",
        "    context, refs, retrieval_cache, _, _ = build_context(\n",
        "        user_input,\n",
        "        retrieval_cache=retrieval_cache,\n",
        "        bm25_retriever=bm25_retriever,\n",
        "        embedding_model=embedding_model\n",
        "    )\n",
        "\n",
        "    # Build full prompt\n",
        "    full_prompt = custom_template.format(\n",
        "        context=context,\n",
        "        history=\"Không có hội thoại trước.\",\n",
        "        input=user_input\n",
        "    )\n",
        "\n",
        "    # Generate response\n",
        "    try:\n",
        "        ans_full = gen_pipe(full_prompt)[0][\"generated_text\"]\n",
        "        split_token = \"### Trả lời:\"\n",
        "        ans = ans_full.split(split_token, 1)[1].strip() if split_token in ans_full else ans_full.strip()\n",
        "    except Exception as e:\n",
        "        ans = f\"[ERROR generating response: {e}]\"\n",
        "\n",
        "    results.append({\n",
        "        \"input\": user_input,\n",
        "        \"context\": context,\n",
        "        \"response\": ans\n",
        "    })\n",
        "\n",
        "# -------------------- Save output --------------------\n",
        "pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"Saved {len(results)} rows to {OUTPUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r9a9jvX8xWbw",
      "metadata": {
        "id": "r9a9jvX8xWbw"
      },
      "source": [
        "# Response Groundedness"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XmE4g7ZnOi_S",
      "metadata": {
        "id": "XmE4g7ZnOi_S"
      },
      "source": [
        "### Base qwen2-3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6Oukc9KhOiSF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6Oukc9KhOiSF",
        "outputId": "dcaf81a6-4e9f-4276-fbd2-9a92f50f84cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 0, Groundedness score: 1.0\n",
            "Index: 1, Groundedness score: 1.0\n",
            "Index: 2, Groundedness score: 0.5\n",
            "Index: 3, Groundedness score: 1.0\n",
            "Index: 4, Groundedness score: 1.0\n",
            "Index: 5, Groundedness score: 1.0\n",
            "Index: 6, Groundedness score: 1.0\n",
            "Index: 7, Groundedness score: 1.0\n",
            "Index: 8, Groundedness score: 1.0\n",
            "Index: 9, Groundedness score: 0.5\n",
            "Saved batch ending at index 9 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 10, Groundedness score: 0.75\n",
            "Index: 11, Groundedness score: 0.25\n",
            "Index: 12, Groundedness score: 0.0\n",
            "Index: 13, Groundedness score: 1.0\n",
            "Index: 14, Groundedness score: 0.0\n",
            "Index: 15, Groundedness score: 0.75\n",
            "Index: 16, Groundedness score: 0.0\n",
            "Index: 17, Groundedness score: 1.0\n",
            "Index: 18, Groundedness score: 1.0\n",
            "Index: 19, Groundedness score: 1.0\n",
            "Saved batch ending at index 19 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 20, Groundedness score: 0.25\n",
            "Index: 21, Groundedness score: 0.5\n",
            "Index: 22, Groundedness score: 0.25\n",
            "Index: 23, Groundedness score: 1.0\n",
            "Index: 24, Groundedness score: 1.0\n",
            "Index: 25, Groundedness score: 0.0\n",
            "Index: 26, Groundedness score: 1.0\n",
            "Index: 27, Groundedness score: 0.5\n",
            "Index: 28, Groundedness score: 1.0\n",
            "Index: 29, Groundedness score: 1.0\n",
            "Saved batch ending at index 29 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 30, Groundedness score: 1.0\n",
            "Index: 31, Groundedness score: 0.0\n",
            "Index: 32, Groundedness score: 1.0\n",
            "Index: 33, Groundedness score: 0.5\n",
            "Index: 34, Groundedness score: 0.75\n",
            "Index: 35, Groundedness score: 1.0\n",
            "Index: 36, Groundedness score: 1.0\n",
            "Index: 37, Groundedness score: 1.0\n",
            "Index: 38, Groundedness score: 1.0\n",
            "Index: 39, Groundedness score: 0.0\n",
            "Saved batch ending at index 39 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 40, Groundedness score: 0.0\n",
            "Index: 41, Groundedness score: 0.0\n",
            "Index: 42, Groundedness score: 1.0\n",
            "Index: 43, Groundedness score: 0.0\n",
            "Index: 44, Groundedness score: 0.75\n",
            "Index: 45, Groundedness score: 0.5\n",
            "Index: 46, Groundedness score: 0.0\n",
            "Index: 47, Groundedness score: 1.0\n",
            "Index: 48, Groundedness score: 0.5\n",
            "Index: 49, Groundedness score: 0.0\n",
            "Saved batch ending at index 49 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 50, Groundedness score: 0.5\n",
            "Index: 51, Groundedness score: 0.5\n",
            "Index: 52, Groundedness score: 0.25\n",
            "Index: 53, Groundedness score: 1.0\n",
            "Index: 54, Groundedness score: 1.0\n",
            "Index: 55, Groundedness score: 0.25\n",
            "Index: 56, Groundedness score: 1.0\n",
            "Index: 57, Groundedness score: 1.0\n",
            "Index: 58, Groundedness score: 1.0\n",
            "Index: 59, Groundedness score: 0.25\n",
            "Saved batch ending at index 59 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 60, Groundedness score: 1.0\n",
            "Index: 61, Groundedness score: 1.0\n",
            "Index: 62, Groundedness score: 0.5\n",
            "Index: 63, Groundedness score: 1.0\n",
            "Index: 64, Groundedness score: 0.5\n",
            "Index: 65, Groundedness score: 0.5\n",
            "Index: 66, Groundedness score: 1.0\n",
            "Index: 67, Groundedness score: 0.5\n",
            "Index: 68, Groundedness score: 1.0\n",
            "Index: 69, Groundedness score: 0.5\n",
            "Saved batch ending at index 69 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 70, Groundedness score: 1.0\n",
            "Index: 71, Groundedness score: 1.0\n",
            "Index: 72, Groundedness score: 0.0\n",
            "Index: 73, Groundedness score: 0.25\n",
            "Index: 74, Groundedness score: 1.0\n",
            "Index: 75, Groundedness score: 0.0\n",
            "Index: 76, Groundedness score: 0.25\n",
            "Index: 77, Groundedness score: 1.0\n",
            "Index: 78, Groundedness score: 1.0\n",
            "Index: 79, Groundedness score: 1.0\n",
            "Saved batch ending at index 79 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 80, Groundedness score: 0.0\n",
            "Index: 81, Groundedness score: 1.0\n",
            "Index: 82, Groundedness score: 0.0\n",
            "Index: 83, Groundedness score: 1.0\n",
            "Index: 84, Groundedness score: 0.25\n",
            "Index: 85, Groundedness score: 1.0\n",
            "Index: 86, Groundedness score: 1.0\n",
            "Index: 87, Groundedness score: 0.75\n",
            "Index: 88, Groundedness score: 0.5\n",
            "Index: 89, Groundedness score: 0.0\n",
            "Saved batch ending at index 89 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 90, Groundedness score: 0.0\n",
            "Index: 91, Groundedness score: 1.0\n",
            "Index: 92, Groundedness score: 0.0\n",
            "Index: 93, Groundedness score: 1.0\n",
            "Index: 94, Groundedness score: 1.0\n",
            "Index: 95, Groundedness score: 0.25\n",
            "Index: 96, Groundedness score: 0.0\n",
            "Index: 97, Groundedness score: 0.0\n",
            "Index: 98, Groundedness score: 1.0\n",
            "Index: 99, Groundedness score: 1.0\n",
            "Saved batch ending at index 99 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 100, Groundedness score: 1.0\n",
            "Index: 101, Groundedness score: 0.0\n",
            "Index: 102, Groundedness score: 0.0\n",
            "Index: 103, Groundedness score: 0.0\n",
            "Index: 104, Groundedness score: 0.5\n",
            "Index: 105, Groundedness score: 0.5\n",
            "Index: 106, Groundedness score: 0.0\n",
            "Index: 107, Groundedness score: 0.5\n",
            "Index: 108, Groundedness score: 0.5\n",
            "Index: 109, Groundedness score: 0.0\n",
            "Saved batch ending at index 109 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 110, Groundedness score: 0.5\n",
            "Index: 111, Groundedness score: 1.0\n",
            "Index: 112, Groundedness score: 0.75\n",
            "Index: 113, Groundedness score: 0.5\n",
            "Index: 114, Groundedness score: 0.0\n",
            "Index: 115, Groundedness score: 0.25\n",
            "Index: 116, Groundedness score: 0.0\n",
            "Index: 117, Groundedness score: 0.0\n",
            "Index: 118, Groundedness score: 0.25\n",
            "Index: 119, Groundedness score: 1.0\n",
            "Saved batch ending at index 119 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 120, Groundedness score: 1.0\n",
            "Index: 121, Groundedness score: 0.0\n",
            "Index: 122, Groundedness score: 1.0\n",
            "Index: 123, Groundedness score: 0.5\n",
            "Index: 124, Groundedness score: 0.0\n",
            "Index: 125, Groundedness score: 0.5\n",
            "Index: 126, Groundedness score: 0.75\n",
            "Index: 127, Groundedness score: 0.0\n",
            "Index: 128, Groundedness score: 0.0\n",
            "Index: 129, Groundedness score: 1.0\n",
            "Saved batch ending at index 129 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 130, Groundedness score: 0.0\n",
            "Index: 131, Groundedness score: 1.0\n",
            "Index: 132, Groundedness score: 1.0\n",
            "Index: 133, Groundedness score: 0.0\n",
            "Index: 134, Groundedness score: 1.0\n",
            "Index: 135, Groundedness score: 1.0\n",
            "Index: 136, Groundedness score: 0.5\n",
            "Index: 137, Groundedness score: 0.75\n",
            "Index: 138, Groundedness score: 1.0\n",
            "Index: 139, Groundedness score: 1.0\n",
            "Saved batch ending at index 139 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "Index: 140, Groundedness score: 0.0\n",
            "Index: 141, Groundedness score: 0.5\n",
            "Index: 142, Groundedness score: 0.0\n",
            "Index: 143, Groundedness score: 0.25\n",
            "Index: 144, Groundedness score: 0.25\n",
            "Index: 145, Groundedness score: 0.0\n",
            "Index: 146, Groundedness score: 1.0\n",
            "Index: 147, Groundedness score: 1.0\n",
            "Index: 148, Groundedness score: 0.75\n",
            "Index: 149, Groundedness score: 0.5\n",
            "Saved batch ending at index 149 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.26ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 150, Groundedness score: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.96ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.61ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 151, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.29ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.64ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 152, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.54ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 335.56ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 153, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.23ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.51ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 154, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.88ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.07ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 155, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.51ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.45ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 156, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.60ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 406.44ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 157, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.83ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 407.01ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 158, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.16ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.62ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 159, Groundedness score: 0.0\n",
            "Saved batch ending at index 159 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.77ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.21ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 160, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 405.43ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.05ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 161, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.44ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.97ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 162, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.56ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.99ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 163, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 406.34ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.58ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 164, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.62ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.42ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 165, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.50ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.79ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 166, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.99ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.58ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 167, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.30ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 385.25ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 168, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.21ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.80ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 169, Groundedness score: 0.0\n",
            "Saved batch ending at index 169 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.03ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 359.42ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 170, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.32ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.01ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 171, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.72ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.97ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 172, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.10ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.09ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 173, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.50ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.94ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 174, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 329.82ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.32ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 175, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.03ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.73ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 176, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.76ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 329.83ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 177, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.37ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.10ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 178, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.18ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 379.93ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 179, Groundedness score: 0.0\n",
            "Saved batch ending at index 179 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.18ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.85ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 180, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.27ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.87ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 181, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.96ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.87ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 182, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 406.00ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.57ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 183, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.82ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.18ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 184, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.53ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.34ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 185, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.44ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.61ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 186, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.27ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.41ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 187, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.84ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 406.03ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 188, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.63ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.74ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 189, Groundedness score: 0.0\n",
            "Saved batch ending at index 189 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.91ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.87ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 190, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.67ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.04ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 191, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.18ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.06ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 192, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.84ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.23ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 193, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.69ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.54ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 194, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.05ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 361.52ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 195, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.80ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.50ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 196, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.01ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.58ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 197, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.55ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.11ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 198, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.58ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.55ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 199, Groundedness score: 0.0\n",
            "Saved batch ending at index 199 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.86ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.40ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 200, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.07ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.47ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 201, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 434.50ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 407.83ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 202, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.10ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.75ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 203, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.87ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.96ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 204, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.76ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.67ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 205, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.24ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.75ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 206, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.39ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.99ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 207, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.56ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.97ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 208, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.36ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.35ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 209, Groundedness score: 0.0\n",
            "Saved batch ending at index 209 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.39ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.75ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 210, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.07ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.15ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 211, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.47ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.66ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 212, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 405.89ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.94ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 213, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.01ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.15ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 214, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.31ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.87ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 215, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.00ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.16ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 216, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.90ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.48ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 217, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.96ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.92ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 218, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.45ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.55ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 219, Groundedness score: 0.0\n",
            "Saved batch ending at index 219 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 359.29ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 363.07ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 220, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.18ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 360.50ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 221, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.03ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.31ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 222, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.81ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.90ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 223, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.35ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.41ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 224, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.01ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.72ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 225, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.08ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.09ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 226, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.27ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.12ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 227, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.21ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.24ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 228, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 456.18ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 407.10ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 229, Groundedness score: 0.0\n",
            "Saved batch ending at index 229 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.26ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.13ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 230, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.63ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.12ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 231, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.13ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.37ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 232, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.52ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.27ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 233, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.02ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.79ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 234, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 405.45ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.99ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 235, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.32ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 438.19ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 236, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.99ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.48ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 237, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.60ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 385.54ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 238, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.11ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.18ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 239, Groundedness score: 0.0\n",
            "Saved batch ending at index 239 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.34ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.61ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 240, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.66ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.99ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 241, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.02ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.74ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 242, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 407.28ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.79ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 243, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.05ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.22ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 244, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.28ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.59ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 245, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.37ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.09ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 246, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.96ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.05ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 247, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.65ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.75ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 248, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.76ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.39ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 249, Groundedness score: 0.0\n",
            "Saved batch ending at index 249 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.73ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.98ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 250, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 405.36ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.09ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 251, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.88ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.13ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 252, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.51ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.44ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 253, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.34ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 385.35ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 254, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.15ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.23ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 255, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.54ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.36ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 256, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.75ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.47ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 257, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.12ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.25ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 258, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.22ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.40ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 259, Groundedness score: 0.0\n",
            "Saved batch ending at index 259 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.74ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.13ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 260, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.81ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.04ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 261, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.78ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.84ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 262, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.21ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.74ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 263, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.12ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 456.19ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 264, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.28ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.91ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 265, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.68ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.88ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 266, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.47ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.84ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 267, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.75ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.36ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 268, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 384.38ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 408.05ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 269, Groundedness score: 0.0\n",
            "Saved batch ending at index 269 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 360.72ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.97ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 270, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 359.95ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 385.10ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 271, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.00ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.98ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 272, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.22ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.68ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 273, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.10ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.94ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 274, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.74ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.33ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 275, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.69ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.36ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 276, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.76ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.25ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 277, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.66ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 407.12ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 278, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.83ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.44ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 279, Groundedness score: 0.0\n",
            "Saved batch ending at index 279 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.05ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.24ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 280, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.28ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.62ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 281, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.16ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 405.88ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 282, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.41ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.67ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 283, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.47ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.84ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 284, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.92ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.24ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 285, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.30ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 385.21ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 286, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.16ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 384.05ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 287, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 387.07ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.66ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 288, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.32ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.03ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 289, Groundedness score: 0.0\n",
            "Saved batch ending at index 289 to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.12ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.02ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 290, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.21ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.90ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 291, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.19ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.00ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 292, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 406.18ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.36ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 293, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.93ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.95ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 294, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.89ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.58ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 295, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 379.84ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.63ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 296, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.34ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 381.85ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 297, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.57ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.80ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 298, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.34ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 354.99ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_requests_per_model_per_day, limit: 0\n",
            "Index: 299, Groundedness score: 0.0\n",
            "Saved batch ending at index 299 to eval_result/base_qwen2_3b_groundedness.csv\n",
            "\n",
            "=== Groundedness Score Distribution ===\n",
            "Score 0.0: 186 responses\n",
            "Score 0.25: 14 responses\n",
            "Score 0.5: 26 responses\n",
            "Score 0.75: 9 responses\n",
            "Score 1.0: 65 responses\n",
            "\n",
            "Total Groundedness score (sum): 88.25\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from collections import Counter\n",
        "from time import sleep\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Load API key từ .env\n",
        "# -------------------------------\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Tạo Gemini 2.5 Flash instance\n",
        "# -------------------------------\n",
        "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Prompt functions (Judge 1 & Judge 2)\n",
        "# -------------------------------\n",
        "def response_groundedness_judge1_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"### Instruction\n",
        "You are a world class expert designed to evaluate the groundedness of an assertion.\n",
        "You will be provided with an assertion and a context.\n",
        "Your task is to determine if the assertion is supported by the context.\n",
        "Follow the instructions below:\n",
        "A. If the assertion or context is empty, say 0.\n",
        "B. If the assertion is not supported by the context, say 0.\n",
        "C. If the assertion is partially supported by the context, say 1.\n",
        "D. If the assertion is fully supported by the context, say 2.\n",
        "You must provide a rating of 0, 1, or 2, nothing else.\n",
        "\n",
        "### Context:\n",
        "<{context}>\n",
        "\n",
        "### Assertion:\n",
        "<{response}>\n",
        "\n",
        "Analyzing Context and Response, the Groundedness score is \"\"\"\n",
        "\n",
        "def response_groundedness_judge2_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"As a specialist in assessing the strength of connections between statements and their given contexts, I will evaluate the level of support an assertion receives from the provided context. Follow these guidelines:\n",
        "\n",
        "* If the assertion or context is empty or assertion is not supported, assign a score of 0.\n",
        "* If the assertion is partially supported, assign a score of 1.\n",
        "* If the assertion is fully supported, assign a score of 2.\n",
        "\n",
        "I will provide a rating of 0, 1, or 2, without any additional information.\n",
        "\n",
        "---\n",
        "**Context:**\n",
        "[{context}]\n",
        "\n",
        "**Assertion:**\n",
        "[{response}]\n",
        "\n",
        "Do not explain. Based on the provided context and response, the Groundedness score is:\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Helper function gọi Gemini Flash và chuẩn hóa\n",
        "# -------------------------------\n",
        "def call_gemini(prompt: str) -> float:\n",
        "    \"\"\"Gọi Gemini Flash, parse output thành float 0,1,2\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        text = response.text.strip()\n",
        "        score = int(text)\n",
        "        if score in [0, 1, 2]:\n",
        "            return score\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR call_gemini]: {e}\")\n",
        "    return None\n",
        "\n",
        "def compute_groundedness(response: str, context: str) -> float:\n",
        "    \"\"\"Tính điểm groundedness dựa trên 2 judge\"\"\"\n",
        "    score1 = call_gemini(response_groundedness_judge1_prompt(response, context))\n",
        "    score2 = call_gemini(response_groundedness_judge2_prompt(response, context))\n",
        "\n",
        "    normalized_scores = []\n",
        "    if score1 is not None:\n",
        "        normalized_scores.append(score1 / 2)\n",
        "    if score2 is not None:\n",
        "        normalized_scores.append(score2 / 2)\n",
        "\n",
        "    if not normalized_scores:\n",
        "        return 0.0\n",
        "    return sum(normalized_scores) / len(normalized_scores)\n",
        "\n",
        "# -------------------------------\n",
        "# 5️⃣ Load data từ CSV (toàn bộ, không sample)\n",
        "# -------------------------------\n",
        "csv_path =\"data/base_qwen2_3b_eval_data.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# -------------------------------\n",
        "# 6️⃣ Chuẩn bị output\n",
        "# -------------------------------\n",
        "os.makedirs(\"eval_result\", exist_ok=True)\n",
        "output_csv = \"eval_result/base_qwen2_3b_groundedness.csv\"\n",
        "\n",
        "# Nếu file đã tồn tại, xóa để ghi batch mới\n",
        "if os.path.exists(output_csv):\n",
        "    os.remove(output_csv)\n",
        "\n",
        "# -------------------------------\n",
        "# 7️⃣ Chạy evaluation theo batch 10 dòng\n",
        "# -------------------------------\n",
        "BATCH_SIZE = 10\n",
        "batch_results = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    response = str(row[\"response\"])\n",
        "    context = str(row[\"context\"])\n",
        "    score = compute_groundedness(response, context)\n",
        "    print(f\"Index: {idx}, Groundedness score: {score}\")\n",
        "\n",
        "    batch_results.append({\n",
        "        \"question\": row.get(\"question\", \"\"),\n",
        "        \"context\": context,\n",
        "        \"response\": response,\n",
        "        \"groundedness_score\": score\n",
        "    })\n",
        "\n",
        "    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n",
        "    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n",
        "        df_batch = pd.DataFrame(batch_results)\n",
        "        # Append nếu file đã tồn tại, else write mới\n",
        "        if os.path.exists(output_csv):\n",
        "            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n",
        "        else:\n",
        "            df_batch.to_csv(output_csv, index=False)\n",
        "        batch_results = []  # reset batch\n",
        "        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8️⃣ Thống kê tổng số điểm\n",
        "# -------------------------------\n",
        "df_final = pd.read_csv(output_csv)\n",
        "score_counts = Counter(df_final[\"groundedness_score\"])\n",
        "print(\"\\n=== Groundedness Score Distribution ===\")\n",
        "for score_level in sorted(score_counts.keys()):\n",
        "    print(f\"Score {score_level}: {score_counts[score_level]} responses\")\n",
        "\n",
        "total_score = df_final[\"groundedness_score\"].sum()\n",
        "print(f\"\\nTotal Groundedness score (sum): {total_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tSKFOWSji9bz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tSKFOWSji9bz",
        "outputId": "67b259d8-3325-4b98-b6a0-c4935cc8e313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed up to index 150 (original CSV index)\n",
            "Processed up to index 151 (original CSV index)\n",
            "Processed up to index 152 (original CSV index)\n",
            "Processed up to index 153 (original CSV index)\n",
            "Processed up to index 154 (original CSV index)\n",
            "Processed up to index 155 (original CSV index)\n",
            "Processed up to index 156 (original CSV index)\n",
            "Processed up to index 157 (original CSV index)\n",
            "Processed up to index 158 (original CSV index)\n",
            "Processed up to index 159 (original CSV index)\n",
            "Processed up to index 160 (original CSV index)\n",
            "Processed up to index 161 (original CSV index)\n",
            "Processed up to index 162 (original CSV index)\n",
            "Processed up to index 163 (original CSV index)\n",
            "Processed up to index 164 (original CSV index)\n",
            "Processed up to index 165 (original CSV index)\n",
            "Processed up to index 166 (original CSV index)\n",
            "Processed up to index 167 (original CSV index)\n",
            "Processed up to index 168 (original CSV index)\n",
            "Processed up to index 169 (original CSV index)\n",
            "Processed up to index 170 (original CSV index)\n",
            "Processed up to index 171 (original CSV index)\n",
            "Processed up to index 172 (original CSV index)\n",
            "Processed up to index 173 (original CSV index)\n",
            "Processed up to index 174 (original CSV index)\n",
            "Processed up to index 175 (original CSV index)\n",
            "Processed up to index 176 (original CSV index)\n",
            "Processed up to index 177 (original CSV index)\n",
            "Processed up to index 178 (original CSV index)\n",
            "Processed up to index 179 (original CSV index)\n",
            "Processed up to index 180 (original CSV index)\n",
            "Processed up to index 181 (original CSV index)\n",
            "Processed up to index 182 (original CSV index)\n",
            "Processed up to index 183 (original CSV index)\n",
            "Processed up to index 184 (original CSV index)\n",
            "Processed up to index 185 (original CSV index)\n",
            "Processed up to index 186 (original CSV index)\n",
            "Processed up to index 187 (original CSV index)\n",
            "Processed up to index 188 (original CSV index)\n",
            "Processed up to index 189 (original CSV index)\n",
            "Processed up to index 190 (original CSV index)\n",
            "Processed up to index 191 (original CSV index)\n",
            "Processed up to index 192 (original CSV index)\n",
            "Processed up to index 193 (original CSV index)\n",
            "Processed up to index 194 (original CSV index)\n",
            "Processed up to index 195 (original CSV index)\n",
            "Processed up to index 196 (original CSV index)\n",
            "Processed up to index 197 (original CSV index)\n",
            "Processed up to index 198 (original CSV index)\n",
            "Processed up to index 199 (original CSV index)\n",
            "Processed up to index 200 (original CSV index)\n",
            "Processed up to index 201 (original CSV index)\n",
            "Processed up to index 202 (original CSV index)\n",
            "Processed up to index 203 (original CSV index)\n",
            "Processed up to index 204 (original CSV index)\n",
            "Processed up to index 205 (original CSV index)\n",
            "Processed up to index 206 (original CSV index)\n",
            "Processed up to index 207 (original CSV index)\n",
            "Processed up to index 208 (original CSV index)\n",
            "Processed up to index 209 (original CSV index)\n",
            "Processed up to index 210 (original CSV index)\n",
            "Processed up to index 211 (original CSV index)\n",
            "Processed up to index 212 (original CSV index)\n",
            "Processed up to index 213 (original CSV index)\n",
            "Processed up to index 214 (original CSV index)\n",
            "Processed up to index 215 (original CSV index)\n",
            "Processed up to index 216 (original CSV index)\n",
            "Processed up to index 217 (original CSV index)\n",
            "Processed up to index 218 (original CSV index)\n",
            "Processed up to index 219 (original CSV index)\n",
            "Processed up to index 220 (original CSV index)\n",
            "Processed up to index 221 (original CSV index)\n",
            "Processed up to index 222 (original CSV index)\n",
            "Processed up to index 223 (original CSV index)\n",
            "Processed up to index 224 (original CSV index)\n",
            "Processed up to index 225 (original CSV index)\n",
            "Processed up to index 226 (original CSV index)\n",
            "Processed up to index 227 (original CSV index)\n",
            "Processed up to index 228 (original CSV index)\n",
            "Processed up to index 229 (original CSV index)\n",
            "Processed up to index 230 (original CSV index)\n",
            "Processed up to index 231 (original CSV index)\n",
            "Processed up to index 232 (original CSV index)\n",
            "Processed up to index 233 (original CSV index)\n",
            "Processed up to index 234 (original CSV index)\n",
            "Processed up to index 235 (original CSV index)\n",
            "Processed up to index 236 (original CSV index)\n",
            "Processed up to index 237 (original CSV index)\n",
            "Processed up to index 238 (original CSV index)\n",
            "Processed up to index 239 (original CSV index)\n",
            "Processed up to index 240 (original CSV index)\n",
            "Processed up to index 241 (original CSV index)\n",
            "Processed up to index 242 (original CSV index)\n",
            "Processed up to index 243 (original CSV index)\n",
            "Processed up to index 244 (original CSV index)\n",
            "Processed up to index 245 (original CSV index)\n",
            "Processed up to index 246 (original CSV index)\n",
            "Processed up to index 247 (original CSV index)\n",
            "Processed up to index 248 (original CSV index)\n",
            "Processed up to index 249 (original CSV index)\n",
            "Processed up to index 250 (original CSV index)\n",
            "Processed up to index 251 (original CSV index)\n",
            "Processed up to index 252 (original CSV index)\n",
            "Processed up to index 253 (original CSV index)\n",
            "Processed up to index 254 (original CSV index)\n",
            "Processed up to index 255 (original CSV index)\n",
            "Processed up to index 256 (original CSV index)\n",
            "Processed up to index 257 (original CSV index)\n",
            "Processed up to index 258 (original CSV index)\n",
            "Processed up to index 259 (original CSV index)\n",
            "Processed up to index 260 (original CSV index)\n",
            "Processed up to index 261 (original CSV index)\n",
            "Processed up to index 262 (original CSV index)\n",
            "Processed up to index 263 (original CSV index)\n",
            "Processed up to index 264 (original CSV index)\n",
            "Processed up to index 265 (original CSV index)\n",
            "Processed up to index 266 (original CSV index)\n",
            "Processed up to index 267 (original CSV index)\n",
            "Processed up to index 268 (original CSV index)\n",
            "Processed up to index 269 (original CSV index)\n",
            "Processed up to index 270 (original CSV index)\n",
            "Processed up to index 271 (original CSV index)\n",
            "Processed up to index 272 (original CSV index)\n",
            "Processed up to index 273 (original CSV index)\n",
            "Processed up to index 274 (original CSV index)\n",
            "Processed up to index 275 (original CSV index)\n",
            "Processed up to index 276 (original CSV index)\n",
            "Processed up to index 277 (original CSV index)\n",
            "Processed up to index 278 (original CSV index)\n",
            "Processed up to index 279 (original CSV index)\n",
            "Processed up to index 280 (original CSV index)\n",
            "Processed up to index 281 (original CSV index)\n",
            "Processed up to index 282 (original CSV index)\n",
            "Processed up to index 283 (original CSV index)\n",
            "Processed up to index 284 (original CSV index)\n",
            "Processed up to index 285 (original CSV index)\n",
            "Processed up to index 286 (original CSV index)\n",
            "Processed up to index 287 (original CSV index)\n",
            "Processed up to index 288 (original CSV index)\n",
            "Processed up to index 289 (original CSV index)\n",
            "Processed up to index 290 (original CSV index)\n",
            "Processed up to index 291 (original CSV index)\n",
            "Processed up to index 292 (original CSV index)\n",
            "Processed up to index 293 (original CSV index)\n",
            "Processed up to index 294 (original CSV index)\n",
            "Processed up to index 295 (original CSV index)\n",
            "Processed up to index 296 (original CSV index)\n",
            "Processed up to index 297 (original CSV index)\n",
            "Processed up to index 298 (original CSV index)\n",
            "Processed up to index 299 (original CSV index)\n",
            "\n",
            "Evaluation finished. Results saved to eval_result/base_qwen2_3b_groundedness.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from time import sleep\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Load API key từ .env\n",
        "# -------------------------------\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Tạo Gemini 2.5 Flash instance\n",
        "# -------------------------------\n",
        "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Prompt functions (Judge 1 & Judge 2)\n",
        "# -------------------------------\n",
        "def response_groundedness_judge1_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"### Instruction\n",
        "You are a world class expert designed to evaluate the groundedness of an assertion.\n",
        "You will be provided with an assertion and a context.\n",
        "Your task is to determine if the assertion is supported by the context.\n",
        "Follow the instructions below:\n",
        "A. If the assertion or context is empty, say 0.\n",
        "B. If the assertion is not supported by the context, say 0.\n",
        "C. If the assertion is partially supported by the context, say 1.\n",
        "D. If the assertion is fully supported by the context, say 2.\n",
        "You must provide a rating of 0, 1, or 2, nothing else.\n",
        "\n",
        "### Context:\n",
        "<{context}>\n",
        "\n",
        "### Assertion:\n",
        "<{response}>\n",
        "\n",
        "Analyzing Context and Response, the Groundedness score is \"\"\"\n",
        "\n",
        "def response_groundedness_judge2_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"As a specialist in assessing the strength of connections between statements and their given contexts, I will evaluate the level of support an assertion receives from the provided context. Follow these guidelines:\n",
        "\n",
        "* If the assertion or context is empty or assertion is not supported, assign a score of 0.\n",
        "* If the assertion is partially supported, assign a score of 1.\n",
        "* If the assertion is fully supported, assign a score of 2.\n",
        "\n",
        "I will provide a rating of 0, 1, or 2, without any additional information.\n",
        "\n",
        "---\n",
        "**Context:**\n",
        "[{context}]\n",
        "\n",
        "**Assertion:**\n",
        "[{response}]\n",
        "\n",
        "Do not explain. Based on the provided context and response, the Groundedness score is:\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Helper function gọi Gemini Flash và retry\n",
        "# -------------------------------\n",
        "def call_gemini(prompt: str, max_retries: int = 3, wait_sec: int = 5) -> int:\n",
        "    \"\"\"Gọi Gemini Flash, retry nếu lỗi, trả về int 0,1,2\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            text = response.text.strip()\n",
        "            score = int(text)\n",
        "            if score in [0, 1, 2]:\n",
        "                return score\n",
        "        except Exception as e:\n",
        "            print(f\"[Gemini error, attempt {attempt+1}/{max_retries}]: {e}. Retrying in {wait_sec}s...\")\n",
        "            sleep(wait_sec)\n",
        "    # Nếu vẫn lỗi sau retries, trả về 0\n",
        "    print(f\"[Gemini failed after {max_retries} retries. Defaulting score=0]\")\n",
        "    return 0\n",
        "\n",
        "def compute_groundedness(response: str, context: str) -> float:\n",
        "    \"\"\"Tính điểm groundedness dựa trên 2 judge\"\"\"\n",
        "    score1 = call_gemini(response_groundedness_judge1_prompt(response, context))\n",
        "    score2 = call_gemini(response_groundedness_judge2_prompt(response, context))\n",
        "\n",
        "    normalized_scores = [s/2 for s in [score1, score2]]\n",
        "    return sum(normalized_scores) / len(normalized_scores)\n",
        "\n",
        "# -------------------------------\n",
        "# 5️⃣ Load data từ CSV\n",
        "# -------------------------------\n",
        "csv_path =\"data/base_qwen2_3b_eval_data.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Bắt đầu từ index 150\n",
        "df = df.iloc[150:].reset_index(drop=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 6️⃣ Chuẩn bị output\n",
        "# -------------------------------\n",
        "os.makedirs(\"eval_result\", exist_ok=True)\n",
        "output_csv = \"eval_result/base_qwen2_3b_groundedness.csv\"\n",
        "\n",
        "# Nếu file đã tồn tại, giữ lại dòng 0-149, xóa từ index 150 trở đi\n",
        "if os.path.exists(output_csv):\n",
        "    df_existing = pd.read_csv(output_csv)\n",
        "    if len(df_existing) > 150:\n",
        "        df_existing = df_existing.iloc[:150]  # giữ dòng 0-149\n",
        "        df_existing.to_csv(output_csv, index=False)\n",
        "    # Nếu file <=150 dòng thì giữ nguyên, không xóa gì\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 7️⃣ Chạy evaluation theo batch 10 dòng\n",
        "# -------------------------------\n",
        "BATCH_SIZE = 10\n",
        "batch_results = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    response = str(row[\"response\"])\n",
        "    context = str(row[\"context\"])\n",
        "    score = compute_groundedness(response, context)\n",
        "\n",
        "    batch_results.append({\n",
        "        \"question\": row.get(\"question\", \"\"),\n",
        "        \"context\": context,\n",
        "        \"response\": response,\n",
        "        \"groundedness_score\": score\n",
        "    })\n",
        "\n",
        "    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n",
        "    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n",
        "        df_batch = pd.DataFrame(batch_results)\n",
        "        # Append nếu file đã tồn tại, else write mới\n",
        "        if os.path.exists(output_csv):\n",
        "            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n",
        "        else:\n",
        "            df_batch.to_csv(output_csv, index=False)\n",
        "        batch_results = []  # reset batch\n",
        "\n",
        "    # In tiến trình\n",
        "    print(f\"Processed up to index {idx+150} (original CSV index)\")\n",
        "\n",
        "print(f\"\\nEvaluation finished. Results saved to {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WBnjsESbFMeO",
      "metadata": {
        "id": "WBnjsESbFMeO"
      },
      "source": [
        "### Qwen2-3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wplUCHBu02OH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wplUCHBu02OH",
        "outputId": "385cacd3-752e-464c-a40b-db64f90b74b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 0, Groundedness score: 1.0\n",
            "Index: 1, Groundedness score: 0.5\n",
            "Index: 2, Groundedness score: 1.0\n",
            "Index: 3, Groundedness score: 1.0\n",
            "Index: 4, Groundedness score: 0.5\n",
            "Index: 5, Groundedness score: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2258.44ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 6, Groundedness score: 1.0\n",
            "Index: 7, Groundedness score: 1.0\n",
            "Index: 8, Groundedness score: 1.0\n",
            "Index: 9, Groundedness score: 1.0\n",
            "Saved batch ending at index 9 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 10, Groundedness score: 0.5\n",
            "Index: 11, Groundedness score: 1.0\n",
            "Index: 12, Groundedness score: 0.5\n",
            "Index: 13, Groundedness score: 0.5\n",
            "Index: 14, Groundedness score: 1.0\n",
            "Index: 15, Groundedness score: 0.75\n",
            "Index: 16, Groundedness score: 0.25\n",
            "Index: 17, Groundedness score: 1.0\n",
            "Index: 18, Groundedness score: 1.0\n",
            "Index: 19, Groundedness score: 0.0\n",
            "Saved batch ending at index 19 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 20, Groundedness score: 1.0\n",
            "Index: 21, Groundedness score: 0.5\n",
            "Index: 22, Groundedness score: 0.0\n",
            "Index: 23, Groundedness score: 1.0\n",
            "Index: 24, Groundedness score: 0.5\n",
            "Index: 25, Groundedness score: 0.5\n",
            "Index: 26, Groundedness score: 1.0\n",
            "Index: 27, Groundedness score: 1.0\n",
            "Index: 28, Groundedness score: 0.5\n",
            "Index: 29, Groundedness score: 1.0\n",
            "Saved batch ending at index 29 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 30, Groundedness score: 0.5\n",
            "Index: 31, Groundedness score: 0.5\n",
            "Index: 32, Groundedness score: 1.0\n",
            "Index: 33, Groundedness score: 0.5\n",
            "Index: 34, Groundedness score: 1.0\n",
            "Index: 35, Groundedness score: 1.0\n",
            "Index: 36, Groundedness score: 1.0\n",
            "Index: 37, Groundedness score: 1.0\n",
            "Index: 38, Groundedness score: 0.5\n",
            "Index: 39, Groundedness score: 0.75\n",
            "Saved batch ending at index 39 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 40, Groundedness score: 1.0\n",
            "Index: 41, Groundedness score: 1.0\n",
            "Index: 42, Groundedness score: 0.75\n",
            "Index: 43, Groundedness score: 0.0\n",
            "Index: 44, Groundedness score: 0.5\n",
            "Index: 45, Groundedness score: 0.5\n",
            "Index: 46, Groundedness score: 1.0\n",
            "Index: 47, Groundedness score: 1.0\n",
            "Index: 48, Groundedness score: 0.0\n",
            "Index: 49, Groundedness score: 0.5\n",
            "Saved batch ending at index 49 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 50, Groundedness score: 1.0\n",
            "Index: 51, Groundedness score: 0.5\n",
            "Index: 52, Groundedness score: 0.5\n",
            "Index: 53, Groundedness score: 1.0\n",
            "Index: 54, Groundedness score: 1.0\n",
            "Index: 55, Groundedness score: 0.0\n",
            "Index: 56, Groundedness score: 0.75\n",
            "Index: 57, Groundedness score: 0.5\n",
            "Index: 58, Groundedness score: 0.0\n",
            "Index: 59, Groundedness score: 0.0\n",
            "Saved batch ending at index 59 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 60, Groundedness score: 1.0\n",
            "Index: 61, Groundedness score: 0.75\n",
            "Index: 62, Groundedness score: 1.0\n",
            "Index: 63, Groundedness score: 1.0\n",
            "Index: 64, Groundedness score: 0.75\n",
            "Index: 65, Groundedness score: 1.0\n",
            "Index: 66, Groundedness score: 0.75\n",
            "Index: 67, Groundedness score: 0.25\n",
            "Index: 68, Groundedness score: 0.75\n",
            "Index: 69, Groundedness score: 0.75\n",
            "Saved batch ending at index 69 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 70, Groundedness score: 0.75\n",
            "Index: 71, Groundedness score: 0.75\n",
            "Index: 72, Groundedness score: 1.0\n",
            "Index: 73, Groundedness score: 1.0\n",
            "Index: 74, Groundedness score: 0.5\n",
            "Index: 75, Groundedness score: 0.5\n",
            "Index: 76, Groundedness score: 0.5\n",
            "Index: 77, Groundedness score: 1.0\n",
            "Index: 78, Groundedness score: 1.0\n",
            "Index: 79, Groundedness score: 0.75\n",
            "Saved batch ending at index 79 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 80, Groundedness score: 0.5\n",
            "Index: 81, Groundedness score: 1.0\n",
            "Index: 82, Groundedness score: 0.0\n",
            "Index: 83, Groundedness score: 0.5\n",
            "Index: 84, Groundedness score: 1.0\n",
            "Index: 85, Groundedness score: 1.0\n",
            "Index: 86, Groundedness score: 1.0\n",
            "Index: 87, Groundedness score: 0.5\n",
            "Index: 88, Groundedness score: 0.5\n",
            "Index: 89, Groundedness score: 0.0\n",
            "Saved batch ending at index 89 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 90, Groundedness score: 0.5\n",
            "Index: 91, Groundedness score: 0.5\n",
            "Index: 92, Groundedness score: 0.0\n",
            "Index: 93, Groundedness score: 0.25\n",
            "Index: 94, Groundedness score: 0.5\n",
            "Index: 95, Groundedness score: 0.0\n",
            "Index: 96, Groundedness score: 0.5\n",
            "Index: 97, Groundedness score: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2665.16ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 98, Groundedness score: 1.0\n",
            "Index: 99, Groundedness score: 1.0\n",
            "Saved batch ending at index 99 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 100, Groundedness score: 0.5\n",
            "Index: 101, Groundedness score: 1.0\n",
            "Index: 102, Groundedness score: 0.5\n",
            "Index: 103, Groundedness score: 1.0\n",
            "Index: 104, Groundedness score: 0.0\n",
            "Index: 105, Groundedness score: 0.0\n",
            "Index: 106, Groundedness score: 1.0\n",
            "Index: 107, Groundedness score: 1.0\n",
            "Index: 108, Groundedness score: 1.0\n",
            "Index: 109, Groundedness score: 1.0\n",
            "Saved batch ending at index 109 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 110, Groundedness score: 0.5\n",
            "Index: 111, Groundedness score: 1.0\n",
            "Index: 112, Groundedness score: 0.5\n",
            "Index: 113, Groundedness score: 1.0\n",
            "Index: 114, Groundedness score: 0.75\n",
            "Index: 115, Groundedness score: 0.25\n",
            "Index: 116, Groundedness score: 1.0\n",
            "Index: 117, Groundedness score: 0.5\n",
            "Index: 118, Groundedness score: 1.0\n",
            "Index: 119, Groundedness score: 1.0\n",
            "Saved batch ending at index 119 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 120, Groundedness score: 1.0\n",
            "Index: 121, Groundedness score: 0.5\n",
            "Index: 122, Groundedness score: 0.0\n",
            "Index: 123, Groundedness score: 1.0\n",
            "Index: 124, Groundedness score: 1.0\n",
            "Index: 125, Groundedness score: 1.0\n",
            "Index: 126, Groundedness score: 0.75\n",
            "Index: 127, Groundedness score: 0.0\n",
            "Index: 128, Groundedness score: 0.0\n",
            "Index: 129, Groundedness score: 1.0\n",
            "Saved batch ending at index 129 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 130, Groundedness score: 0.0\n",
            "Index: 131, Groundedness score: 0.5\n",
            "Index: 132, Groundedness score: 1.0\n",
            "Index: 133, Groundedness score: 1.0\n",
            "Index: 134, Groundedness score: 1.0\n",
            "Index: 135, Groundedness score: 0.5\n",
            "Index: 136, Groundedness score: 1.0\n",
            "Index: 137, Groundedness score: 1.0\n",
            "Index: 138, Groundedness score: 1.0\n",
            "Index: 139, Groundedness score: 0.5\n",
            "Saved batch ending at index 139 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 140, Groundedness score: 0.75\n",
            "Index: 141, Groundedness score: 0.5\n",
            "Index: 142, Groundedness score: 0.5\n",
            "Index: 143, Groundedness score: 1.0\n",
            "Index: 144, Groundedness score: 1.0\n",
            "Index: 145, Groundedness score: 0.5\n",
            "Index: 146, Groundedness score: 1.0\n",
            "Index: 147, Groundedness score: 1.0\n",
            "Index: 148, Groundedness score: 1.0\n",
            "Index: 149, Groundedness score: 1.0\n",
            "Saved batch ending at index 149 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 150, Groundedness score: 0.5\n",
            "Index: 151, Groundedness score: 0.0\n",
            "Index: 152, Groundedness score: 1.0\n",
            "Index: 153, Groundedness score: 0.5\n",
            "Index: 154, Groundedness score: 0.0\n",
            "Index: 155, Groundedness score: 0.5\n",
            "Index: 156, Groundedness score: 1.0\n",
            "Index: 157, Groundedness score: 1.0\n",
            "Index: 158, Groundedness score: 0.5\n",
            "Index: 159, Groundedness score: 1.0\n",
            "Saved batch ending at index 159 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 160, Groundedness score: 0.5\n",
            "Index: 161, Groundedness score: 1.0\n",
            "Index: 162, Groundedness score: 0.5\n",
            "Index: 163, Groundedness score: 0.5\n",
            "Index: 164, Groundedness score: 0.75\n",
            "Index: 165, Groundedness score: 0.5\n",
            "Index: 166, Groundedness score: 1.0\n",
            "Index: 167, Groundedness score: 1.0\n",
            "Index: 168, Groundedness score: 1.0\n",
            "Index: 169, Groundedness score: 1.0\n",
            "Saved batch ending at index 169 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 170, Groundedness score: 0.5\n",
            "Index: 171, Groundedness score: 1.0\n",
            "Index: 172, Groundedness score: 0.0\n",
            "Index: 173, Groundedness score: 1.0\n",
            "Index: 174, Groundedness score: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1628.87ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3630.94ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR call_gemini]: HTTPConnectionPool(host='localhost', port=33031): Read timed out. (read timeout=600.0)\n",
            "Index: 175, Groundedness score: 1.0\n",
            "Index: 176, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1951.70ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 177, Groundedness score: 0.5\n",
            "Index: 178, Groundedness score: 1.0\n",
            "Index: 179, Groundedness score: 1.0\n",
            "Saved batch ending at index 179 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 180, Groundedness score: 0.5\n",
            "Index: 181, Groundedness score: 0.75\n",
            "Index: 182, Groundedness score: 0.5\n",
            "Index: 183, Groundedness score: 1.0\n",
            "Index: 184, Groundedness score: 0.75\n",
            "Index: 185, Groundedness score: 0.0\n",
            "Index: 186, Groundedness score: 0.0\n",
            "Index: 187, Groundedness score: 0.5\n",
            "Index: 188, Groundedness score: 0.75\n",
            "Index: 189, Groundedness score: 0.75\n",
            "Saved batch ending at index 189 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 190, Groundedness score: 1.0\n",
            "Index: 191, Groundedness score: 1.0\n",
            "Index: 192, Groundedness score: 0.5\n",
            "Index: 193, Groundedness score: 0.5\n",
            "Index: 194, Groundedness score: 0.25\n",
            "Index: 195, Groundedness score: 1.0\n",
            "Index: 196, Groundedness score: 0.75\n",
            "Index: 197, Groundedness score: 0.5\n",
            "Index: 198, Groundedness score: 1.0\n",
            "Index: 199, Groundedness score: 0.5\n",
            "Saved batch ending at index 199 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 200, Groundedness score: 1.0\n",
            "Index: 201, Groundedness score: 0.0\n",
            "Index: 202, Groundedness score: 0.5\n",
            "Index: 203, Groundedness score: 0.5\n",
            "Index: 204, Groundedness score: 1.0\n",
            "Index: 205, Groundedness score: 0.75\n",
            "Index: 206, Groundedness score: 1.0\n",
            "Index: 207, Groundedness score: 0.0\n",
            "Index: 208, Groundedness score: 1.0\n",
            "Index: 209, Groundedness score: 0.25\n",
            "Saved batch ending at index 209 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 210, Groundedness score: 0.5\n",
            "Index: 211, Groundedness score: 0.5\n",
            "Index: 212, Groundedness score: 1.0\n",
            "Index: 213, Groundedness score: 1.0\n",
            "Index: 214, Groundedness score: 0.0\n",
            "Index: 215, Groundedness score: 0.25\n",
            "Index: 216, Groundedness score: 0.0\n",
            "Index: 217, Groundedness score: 0.5\n",
            "Index: 218, Groundedness score: 0.0\n",
            "Index: 219, Groundedness score: 0.75\n",
            "Saved batch ending at index 219 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 220, Groundedness score: 0.25\n",
            "Index: 221, Groundedness score: 0.25\n",
            "Index: 222, Groundedness score: 1.0\n",
            "Index: 223, Groundedness score: 1.0\n",
            "Index: 224, Groundedness score: 1.0\n",
            "Index: 225, Groundedness score: 1.0\n",
            "Index: 226, Groundedness score: 0.5\n",
            "Index: 227, Groundedness score: 1.0\n",
            "Index: 228, Groundedness score: 1.0\n",
            "Index: 229, Groundedness score: 0.5\n",
            "Saved batch ending at index 229 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 230, Groundedness score: 0.75\n",
            "Index: 231, Groundedness score: 1.0\n",
            "Index: 232, Groundedness score: 0.5\n",
            "Index: 233, Groundedness score: 1.0\n",
            "Index: 234, Groundedness score: 1.0\n",
            "Index: 235, Groundedness score: 1.0\n",
            "Index: 236, Groundedness score: 1.0\n",
            "Index: 237, Groundedness score: 0.75\n",
            "Index: 238, Groundedness score: 0.5\n",
            "Index: 239, Groundedness score: 0.5\n",
            "Saved batch ending at index 239 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 240, Groundedness score: 0.25\n",
            "Index: 241, Groundedness score: 1.0\n",
            "Index: 242, Groundedness score: 0.5\n",
            "Index: 243, Groundedness score: 1.0\n",
            "Index: 244, Groundedness score: 0.75\n",
            "Index: 245, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 4895.03ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3371.11ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 246, Groundedness score: 1.0\n",
            "Index: 247, Groundedness score: 1.0\n",
            "Index: 248, Groundedness score: 0.0\n",
            "Index: 249, Groundedness score: 1.0\n",
            "Saved batch ending at index 249 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 250, Groundedness score: 0.5\n",
            "Index: 251, Groundedness score: 1.0\n",
            "Index: 252, Groundedness score: 0.5\n",
            "Index: 253, Groundedness score: 0.75\n",
            "Index: 254, Groundedness score: 0.0\n",
            "Index: 255, Groundedness score: 1.0\n",
            "Index: 256, Groundedness score: 0.5\n",
            "Index: 257, Groundedness score: 0.0\n",
            "Index: 258, Groundedness score: 0.5\n",
            "Index: 259, Groundedness score: 0.0\n",
            "Saved batch ending at index 259 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 260, Groundedness score: 1.0\n",
            "Index: 261, Groundedness score: 1.0\n",
            "Index: 262, Groundedness score: 1.0\n",
            "Index: 263, Groundedness score: 1.0\n",
            "Index: 264, Groundedness score: 0.0\n",
            "Index: 265, Groundedness score: 0.5\n",
            "Index: 266, Groundedness score: 1.0\n",
            "Index: 267, Groundedness score: 0.5\n",
            "Index: 268, Groundedness score: 1.0\n",
            "Index: 269, Groundedness score: 0.25\n",
            "Saved batch ending at index 269 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 270, Groundedness score: 0.5\n",
            "Index: 271, Groundedness score: 1.0\n",
            "Index: 272, Groundedness score: 1.0\n",
            "Index: 273, Groundedness score: 1.0\n",
            "Index: 274, Groundedness score: 1.0\n",
            "Index: 275, Groundedness score: 1.0\n",
            "Index: 276, Groundedness score: 1.0\n",
            "Index: 277, Groundedness score: 0.75\n",
            "Index: 278, Groundedness score: 0.25\n",
            "Index: 279, Groundedness score: 1.0\n",
            "Saved batch ending at index 279 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 280, Groundedness score: 0.75\n",
            "Index: 281, Groundedness score: 0.5\n",
            "Index: 282, Groundedness score: 1.0\n",
            "Index: 283, Groundedness score: 0.0\n",
            "Index: 284, Groundedness score: 1.0\n",
            "Index: 285, Groundedness score: 1.0\n",
            "Index: 286, Groundedness score: 1.0\n",
            "Index: 287, Groundedness score: 1.0\n",
            "Index: 288, Groundedness score: 0.75\n",
            "Index: 289, Groundedness score: 1.0\n",
            "Saved batch ending at index 289 to eval_result/qwen2_3b_groundedness.csv\n",
            "Index: 290, Groundedness score: 0.5\n",
            "Index: 291, Groundedness score: 0.5\n",
            "Index: 292, Groundedness score: 0.0\n",
            "Index: 293, Groundedness score: 1.0\n",
            "Index: 294, Groundedness score: 0.0\n",
            "Index: 295, Groundedness score: 0.5\n",
            "Index: 296, Groundedness score: 1.0\n",
            "Index: 297, Groundedness score: 1.0\n",
            "Index: 298, Groundedness score: 0.5\n",
            "Index: 299, Groundedness score: 0.5\n",
            "Saved batch ending at index 299 to eval_result/qwen2_3b_groundedness.csv\n",
            "\n",
            "=== Groundedness Score Distribution ===\n",
            "Score 0.0: 37 responses\n",
            "Score 0.25: 12 responses\n",
            "Score 0.5: 83 responses\n",
            "Score 0.75: 30 responses\n",
            "Score 1.0: 138 responses\n",
            "\n",
            "Total Groundedness score (sum): 205.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from collections import Counter\n",
        "from time import sleep\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Load API key từ .env\n",
        "# -------------------------------\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Tạo Gemini 2.5 Flash instance\n",
        "# -------------------------------\n",
        "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Prompt functions (Judge 1 & Judge 2)\n",
        "# -------------------------------\n",
        "def response_groundedness_judge1_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"### Instruction\n",
        "You are a world class expert designed to evaluate the groundedness of an assertion.\n",
        "You will be provided with an assertion and a context.\n",
        "Your task is to determine if the assertion is supported by the context.\n",
        "Follow the instructions below:\n",
        "A. If the assertion or context is empty, say 0.\n",
        "B. If the assertion is not supported by the context, say 0.\n",
        "C. If the assertion is partially supported by the context, say 1.\n",
        "D. If the assertion is fully supported by the context, say 2.\n",
        "You must provide a rating of 0, 1, or 2, nothing else.\n",
        "\n",
        "### Context:\n",
        "<{context}>\n",
        "\n",
        "### Assertion:\n",
        "<{response}>\n",
        "\n",
        "Analyzing Context and Response, the Groundedness score is \"\"\"\n",
        "\n",
        "def response_groundedness_judge2_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"As a specialist in assessing the strength of connections between statements and their given contexts, I will evaluate the level of support an assertion receives from the provided context. Follow these guidelines:\n",
        "\n",
        "* If the assertion or context is empty or assertion is not supported, assign a score of 0.\n",
        "* If the assertion is partially supported, assign a score of 1.\n",
        "* If the assertion is fully supported, assign a score of 2.\n",
        "\n",
        "I will provide a rating of 0, 1, or 2, without any additional information.\n",
        "\n",
        "---\n",
        "**Context:**\n",
        "[{context}]\n",
        "\n",
        "**Assertion:**\n",
        "[{response}]\n",
        "\n",
        "Do not explain. Based on the provided context and response, the Groundedness score is:\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Helper function gọi Gemini Flash và chuẩn hóa\n",
        "# -------------------------------\n",
        "def call_gemini(prompt: str) -> float:\n",
        "    \"\"\"Gọi Gemini Flash, parse output thành float 0,1,2\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        text = response.text.strip()\n",
        "        score = int(text)\n",
        "        if score in [0, 1, 2]:\n",
        "            return score\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR call_gemini]: {e}\")\n",
        "    return None\n",
        "\n",
        "def compute_groundedness(response: str, context: str) -> float:\n",
        "    \"\"\"Tính điểm groundedness dựa trên 2 judge\"\"\"\n",
        "    score1 = call_gemini(response_groundedness_judge1_prompt(response, context))\n",
        "    score2 = call_gemini(response_groundedness_judge2_prompt(response, context))\n",
        "\n",
        "    normalized_scores = []\n",
        "    if score1 is not None:\n",
        "        normalized_scores.append(score1 / 2)\n",
        "    if score2 is not None:\n",
        "        normalized_scores.append(score2 / 2)\n",
        "\n",
        "    if not normalized_scores:\n",
        "        return 0.0\n",
        "    return sum(normalized_scores) / len(normalized_scores)\n",
        "\n",
        "# -------------------------------\n",
        "# 5️⃣ Load data từ CSV (toàn bộ, không sample)\n",
        "# -------------------------------\n",
        "csv_path = \"data/qwen2-3b_eval_data.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# -------------------------------\n",
        "# 6️⃣ Chuẩn bị output\n",
        "# -------------------------------\n",
        "os.makedirs(\"eval_result\", exist_ok=True)\n",
        "output_csv = \"eval_result/qwen2_3b_groundedness.csv\"\n",
        "\n",
        "# Nếu file đã tồn tại, xóa để ghi batch mới\n",
        "if os.path.exists(output_csv):\n",
        "    os.remove(output_csv)\n",
        "\n",
        "# -------------------------------\n",
        "# 7️⃣ Chạy evaluation theo batch 10 dòng\n",
        "# -------------------------------\n",
        "BATCH_SIZE = 10\n",
        "batch_results = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    response = str(row[\"response\"])\n",
        "    context = str(row[\"context\"])\n",
        "    score = compute_groundedness(response, context)\n",
        "    print(f\"Index: {idx}, Groundedness score: {score}\")\n",
        "\n",
        "    batch_results.append({\n",
        "        \"question\": row.get(\"question\", \"\"),\n",
        "        \"context\": context,\n",
        "        \"response\": response,\n",
        "        \"groundedness_score\": score\n",
        "    })\n",
        "\n",
        "    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n",
        "    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n",
        "        df_batch = pd.DataFrame(batch_results)\n",
        "        # Append nếu file đã tồn tại, else write mới\n",
        "        if os.path.exists(output_csv):\n",
        "            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n",
        "        else:\n",
        "            df_batch.to_csv(output_csv, index=False)\n",
        "        batch_results = []  # reset batch\n",
        "        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8️⃣ Thống kê tổng số điểm\n",
        "# -------------------------------\n",
        "df_final = pd.read_csv(output_csv)\n",
        "score_counts = Counter(df_final[\"groundedness_score\"])\n",
        "print(\"\\n=== Groundedness Score Distribution ===\")\n",
        "for score_level in sorted(score_counts.keys()):\n",
        "    print(f\"Score {score_level}: {score_counts[score_level]} responses\")\n",
        "\n",
        "total_score = df_final[\"groundedness_score\"].sum()\n",
        "print(f\"\\nTotal Groundedness score (sum): {total_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "awG38Pnc0vor",
      "metadata": {
        "id": "awG38Pnc0vor"
      },
      "source": [
        "### LLama 3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ZBxiQUx1lxz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ZBxiQUx1lxz",
        "outputId": "4c39e332-c250-4863-a260-034178bb14d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 0, Groundedness score: 1.0\n",
            "Index: 1, Groundedness score: 1.0\n",
            "Index: 2, Groundedness score: 0.5\n",
            "Index: 3, Groundedness score: 1.0\n",
            "Index: 4, Groundedness score: 0.5\n",
            "Index: 5, Groundedness score: 0.5\n",
            "Index: 6, Groundedness score: 1.0\n",
            "Index: 7, Groundedness score: 1.0\n",
            "Index: 8, Groundedness score: 0.5\n",
            "Index: 9, Groundedness score: 1.0\n",
            "Saved batch ending at index 9 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 10, Groundedness score: 1.0\n",
            "Index: 11, Groundedness score: 0.25\n",
            "Index: 12, Groundedness score: 0.0\n",
            "Index: 13, Groundedness score: 0.5\n",
            "Index: 14, Groundedness score: 1.0\n",
            "Index: 15, Groundedness score: 1.0\n",
            "Index: 16, Groundedness score: 1.0\n",
            "Index: 17, Groundedness score: 0.5\n",
            "Index: 18, Groundedness score: 0.5\n",
            "Index: 19, Groundedness score: 0.0\n",
            "Saved batch ending at index 19 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 20, Groundedness score: 0.75\n",
            "Index: 21, Groundedness score: 0.25\n",
            "Index: 22, Groundedness score: 0.5\n",
            "Index: 23, Groundedness score: 1.0\n",
            "Index: 24, Groundedness score: 1.0\n",
            "Index: 25, Groundedness score: 0.0\n",
            "Index: 26, Groundedness score: 1.0\n",
            "Index: 27, Groundedness score: 1.0\n",
            "Index: 28, Groundedness score: 1.0\n",
            "Index: 29, Groundedness score: 0.5\n",
            "Saved batch ending at index 29 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 30, Groundedness score: 0.5\n",
            "Index: 31, Groundedness score: 0.5\n",
            "Index: 32, Groundedness score: 0.5\n",
            "Index: 33, Groundedness score: 0.5\n",
            "Index: 34, Groundedness score: 1.0\n",
            "Index: 35, Groundedness score: 1.0\n",
            "Index: 36, Groundedness score: 1.0\n",
            "Index: 37, Groundedness score: 1.0\n",
            "Index: 38, Groundedness score: 1.0\n",
            "Index: 39, Groundedness score: 1.0\n",
            "Saved batch ending at index 39 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 40, Groundedness score: 1.0\n",
            "Index: 41, Groundedness score: 0.0\n",
            "Index: 42, Groundedness score: 1.0\n",
            "Index: 43, Groundedness score: 0.0\n",
            "Index: 44, Groundedness score: 1.0\n",
            "Index: 45, Groundedness score: 0.75\n",
            "Index: 46, Groundedness score: 0.75\n",
            "Index: 47, Groundedness score: 1.0\n",
            "Index: 48, Groundedness score: 0.5\n",
            "Index: 49, Groundedness score: 0.5\n",
            "Saved batch ending at index 49 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 50, Groundedness score: 1.0\n",
            "Index: 51, Groundedness score: 1.0\n",
            "Index: 52, Groundedness score: 0.5\n",
            "Index: 53, Groundedness score: 1.0\n",
            "Index: 54, Groundedness score: 0.5\n",
            "Index: 55, Groundedness score: 0.0\n",
            "Index: 56, Groundedness score: 1.0\n",
            "Index: 57, Groundedness score: 1.0\n",
            "Index: 58, Groundedness score: 0.25\n",
            "Index: 59, Groundedness score: 0.25\n",
            "Saved batch ending at index 59 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 60, Groundedness score: 0.5\n",
            "Index: 61, Groundedness score: 1.0\n",
            "Index: 62, Groundedness score: 0.5\n",
            "Index: 63, Groundedness score: 0.75\n",
            "Index: 64, Groundedness score: 1.0\n",
            "Index: 65, Groundedness score: 0.5\n",
            "Index: 66, Groundedness score: 1.0\n",
            "Index: 67, Groundedness score: 0.5\n",
            "Index: 68, Groundedness score: 1.0\n",
            "Index: 69, Groundedness score: 0.25\n",
            "Saved batch ending at index 69 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 70, Groundedness score: 1.0\n",
            "Index: 71, Groundedness score: 1.0\n",
            "Index: 72, Groundedness score: 0.0\n",
            "Index: 73, Groundedness score: 0.75\n",
            "Index: 74, Groundedness score: 0.25\n",
            "Index: 75, Groundedness score: 0.0\n",
            "Index: 76, Groundedness score: 1.0\n",
            "Index: 77, Groundedness score: 1.0\n",
            "Index: 78, Groundedness score: 1.0\n",
            "Index: 79, Groundedness score: 0.5\n",
            "Saved batch ending at index 79 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 80, Groundedness score: 0.5\n",
            "Index: 81, Groundedness score: 1.0\n",
            "Index: 82, Groundedness score: 0.25\n",
            "Index: 83, Groundedness score: 1.0\n",
            "Index: 84, Groundedness score: 0.0\n",
            "Index: 85, Groundedness score: 1.0\n",
            "Index: 86, Groundedness score: 1.0\n",
            "Index: 87, Groundedness score: 0.5\n",
            "Index: 88, Groundedness score: 1.0\n",
            "Index: 89, Groundedness score: 0.0\n",
            "Saved batch ending at index 89 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 90, Groundedness score: 0.5\n",
            "Index: 91, Groundedness score: 0.5\n",
            "Index: 92, Groundedness score: 0.0\n",
            "Index: 93, Groundedness score: 0.5\n",
            "Index: 94, Groundedness score: 1.0\n",
            "Index: 95, Groundedness score: 0.0\n",
            "Index: 96, Groundedness score: 0.5\n",
            "Index: 97, Groundedness score: 1.0\n",
            "Index: 98, Groundedness score: 0.5\n",
            "Index: 99, Groundedness score: 0.75\n",
            "Saved batch ending at index 99 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 100, Groundedness score: 0.0\n",
            "Index: 101, Groundedness score: 1.0\n",
            "Index: 102, Groundedness score: 0.25\n",
            "Index: 103, Groundedness score: 0.0\n",
            "Index: 104, Groundedness score: 0.0\n",
            "Index: 105, Groundedness score: 0.0\n",
            "Index: 106, Groundedness score: 0.5\n",
            "Index: 107, Groundedness score: 1.0\n",
            "Index: 108, Groundedness score: 1.0\n",
            "Index: 109, Groundedness score: 0.5\n",
            "Saved batch ending at index 109 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 110, Groundedness score: 0.5\n",
            "Index: 111, Groundedness score: 1.0\n",
            "Index: 112, Groundedness score: 0.5\n",
            "Index: 113, Groundedness score: 1.0\n",
            "Index: 114, Groundedness score: 0.5\n",
            "Index: 115, Groundedness score: 0.75\n",
            "Index: 116, Groundedness score: 0.25\n",
            "Index: 117, Groundedness score: 1.0\n",
            "Index: 118, Groundedness score: 0.5\n",
            "Index: 119, Groundedness score: 0.5\n",
            "Saved batch ending at index 119 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 120, Groundedness score: 1.0\n",
            "Index: 121, Groundedness score: 0.5\n",
            "Index: 122, Groundedness score: 0.0\n",
            "Index: 123, Groundedness score: 0.5\n",
            "Index: 124, Groundedness score: 1.0\n",
            "Index: 125, Groundedness score: 1.0\n",
            "Index: 126, Groundedness score: 1.0\n",
            "Index: 127, Groundedness score: 0.0\n",
            "Index: 128, Groundedness score: 0.25\n",
            "Index: 129, Groundedness score: 1.0\n",
            "Saved batch ending at index 129 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 130, Groundedness score: 0.0\n",
            "Index: 131, Groundedness score: 0.0\n",
            "Index: 132, Groundedness score: 1.0\n",
            "Index: 133, Groundedness score: 1.0\n",
            "Index: 134, Groundedness score: 1.0\n",
            "Index: 135, Groundedness score: 0.75\n",
            "Index: 136, Groundedness score: 1.0\n",
            "Index: 137, Groundedness score: 0.5\n",
            "Index: 138, Groundedness score: 0.5\n",
            "Index: 139, Groundedness score: 0.75\n",
            "Saved batch ending at index 139 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 140, Groundedness score: 0.5\n",
            "Index: 141, Groundedness score: 0.5\n",
            "Index: 142, Groundedness score: 0.5\n",
            "Index: 143, Groundedness score: 0.5\n",
            "Index: 144, Groundedness score: 1.0\n",
            "Index: 145, Groundedness score: 0.5\n",
            "Index: 146, Groundedness score: 1.0\n",
            "Index: 147, Groundedness score: 1.0\n",
            "Index: 148, Groundedness score: 0.5\n",
            "Index: 149, Groundedness score: 1.0\n",
            "Saved batch ending at index 149 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 150, Groundedness score: 1.0\n",
            "Index: 151, Groundedness score: 1.0\n",
            "Index: 152, Groundedness score: 0.0\n",
            "Index: 153, Groundedness score: 1.0\n",
            "Index: 154, Groundedness score: 0.0\n",
            "Index: 155, Groundedness score: 1.0\n",
            "Index: 156, Groundedness score: 0.5\n",
            "Index: 157, Groundedness score: 1.0\n",
            "Index: 158, Groundedness score: 0.5\n",
            "Index: 159, Groundedness score: 0.5\n",
            "Saved batch ending at index 159 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 160, Groundedness score: 1.0\n",
            "Index: 161, Groundedness score: 0.0\n",
            "Index: 162, Groundedness score: 1.0\n",
            "Index: 163, Groundedness score: 1.0\n",
            "Index: 164, Groundedness score: 1.0\n",
            "Index: 165, Groundedness score: 1.0\n",
            "Index: 166, Groundedness score: 0.5\n",
            "Index: 167, Groundedness score: 0.5\n",
            "Index: 168, Groundedness score: 1.0\n",
            "Index: 169, Groundedness score: 0.5\n",
            "Saved batch ending at index 169 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 170, Groundedness score: 0.75\n",
            "Index: 171, Groundedness score: 1.0\n",
            "Index: 172, Groundedness score: 0.0\n",
            "Index: 173, Groundedness score: 1.0\n",
            "Index: 174, Groundedness score: 0.5\n",
            "Index: 175, Groundedness score: 0.5\n",
            "Index: 176, Groundedness score: 0.0\n",
            "Index: 177, Groundedness score: 0.5\n",
            "Index: 178, Groundedness score: 1.0\n",
            "Index: 179, Groundedness score: 0.5\n",
            "Saved batch ending at index 179 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 180, Groundedness score: 0.75\n",
            "Index: 181, Groundedness score: 1.0\n",
            "Index: 182, Groundedness score: 1.0\n",
            "Index: 183, Groundedness score: 1.0\n",
            "Index: 184, Groundedness score: 0.0\n",
            "Index: 185, Groundedness score: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2209.77ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 186, Groundedness score: 0.0\n",
            "Index: 187, Groundedness score: 1.0\n",
            "Index: 188, Groundedness score: 1.0\n",
            "Index: 189, Groundedness score: 0.5\n",
            "Saved batch ending at index 189 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 190, Groundedness score: 1.0\n",
            "Index: 191, Groundedness score: 0.5\n",
            "Index: 192, Groundedness score: 0.75\n",
            "Index: 193, Groundedness score: 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1900.28ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 194, Groundedness score: 1.0\n",
            "Index: 195, Groundedness score: 1.0\n",
            "Index: 196, Groundedness score: 1.0\n",
            "Index: 197, Groundedness score: 0.5\n",
            "Index: 198, Groundedness score: 0.5\n",
            "Index: 199, Groundedness score: 1.0\n",
            "Saved batch ending at index 199 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 200, Groundedness score: 1.0\n",
            "Index: 201, Groundedness score: 0.0\n",
            "Index: 202, Groundedness score: 1.0\n",
            "Index: 203, Groundedness score: 1.0\n",
            "Index: 204, Groundedness score: 0.0\n",
            "Index: 205, Groundedness score: 0.5\n",
            "Index: 206, Groundedness score: 0.5\n",
            "Index: 207, Groundedness score: 0.25\n",
            "Index: 208, Groundedness score: 0.5\n",
            "Index: 209, Groundedness score: 0.5\n",
            "Saved batch ending at index 209 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 210, Groundedness score: 1.0\n",
            "Index: 211, Groundedness score: 1.0\n",
            "Index: 212, Groundedness score: 1.0\n",
            "Index: 213, Groundedness score: 0.5\n",
            "Index: 214, Groundedness score: 0.0\n",
            "Index: 215, Groundedness score: 0.25\n",
            "Index: 216, Groundedness score: 0.0\n",
            "Index: 217, Groundedness score: 1.0\n",
            "Index: 218, Groundedness score: 0.25\n",
            "Index: 219, Groundedness score: 0.75\n",
            "Saved batch ending at index 219 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 220, Groundedness score: 0.25\n",
            "Index: 221, Groundedness score: 1.0\n",
            "Index: 222, Groundedness score: 0.5\n",
            "Index: 223, Groundedness score: 0.5\n",
            "Index: 224, Groundedness score: 0.5\n",
            "Index: 225, Groundedness score: 1.0\n",
            "Index: 226, Groundedness score: 0.5\n",
            "Index: 227, Groundedness score: 0.0\n",
            "Index: 228, Groundedness score: 1.0\n",
            "Index: 229, Groundedness score: 0.0\n",
            "Saved batch ending at index 229 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 230, Groundedness score: 1.0\n",
            "Index: 231, Groundedness score: 1.0\n",
            "Index: 232, Groundedness score: 0.5\n",
            "Index: 233, Groundedness score: 0.5\n",
            "Index: 234, Groundedness score: 1.0\n",
            "Index: 235, Groundedness score: 1.0\n",
            "Index: 236, Groundedness score: 1.0\n",
            "Index: 237, Groundedness score: 1.0\n",
            "Index: 238, Groundedness score: 0.25\n",
            "Index: 239, Groundedness score: 0.75\n",
            "Saved batch ending at index 239 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 240, Groundedness score: 0.25\n",
            "Index: 241, Groundedness score: 1.0\n",
            "Index: 242, Groundedness score: 0.0\n",
            "Index: 243, Groundedness score: 0.5\n",
            "Index: 244, Groundedness score: 0.5\n",
            "Index: 245, Groundedness score: 1.0\n",
            "Index: 246, Groundedness score: 1.0\n",
            "Index: 247, Groundedness score: 0.5\n",
            "Index: 248, Groundedness score: 0.0\n",
            "Index: 249, Groundedness score: 1.0\n",
            "Saved batch ending at index 249 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 250, Groundedness score: 0.0\n",
            "Index: 251, Groundedness score: 0.25\n",
            "Index: 252, Groundedness score: 0.5\n",
            "Index: 253, Groundedness score: 0.25\n",
            "Index: 254, Groundedness score: 0.0\n",
            "Index: 255, Groundedness score: 1.0\n",
            "Index: 256, Groundedness score: 0.5\n",
            "Index: 257, Groundedness score: 0.25\n",
            "Index: 258, Groundedness score: 0.0\n",
            "Index: 259, Groundedness score: 1.0\n",
            "Saved batch ending at index 259 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 260, Groundedness score: 0.5\n",
            "Index: 261, Groundedness score: 0.25\n",
            "Index: 262, Groundedness score: 1.0\n",
            "Index: 263, Groundedness score: 0.5\n",
            "Index: 264, Groundedness score: 0.0\n",
            "Index: 265, Groundedness score: 0.5\n",
            "Index: 266, Groundedness score: 1.0\n",
            "Index: 267, Groundedness score: 1.0\n",
            "Index: 268, Groundedness score: 1.0\n",
            "Index: 269, Groundedness score: 0.0\n",
            "Saved batch ending at index 269 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 270, Groundedness score: 0.0\n",
            "Index: 271, Groundedness score: 0.0\n",
            "Index: 272, Groundedness score: 0.5\n",
            "Index: 273, Groundedness score: 0.5\n",
            "Index: 274, Groundedness score: 1.0\n",
            "Index: 275, Groundedness score: 1.0\n",
            "Index: 276, Groundedness score: 0.5\n",
            "Index: 277, Groundedness score: 1.0\n",
            "Index: 278, Groundedness score: 0.0\n",
            "Index: 279, Groundedness score: 1.0\n",
            "Saved batch ending at index 279 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 280, Groundedness score: 1.0\n",
            "Index: 281, Groundedness score: 0.5\n",
            "Index: 282, Groundedness score: 1.0\n",
            "Index: 283, Groundedness score: 1.0\n",
            "Index: 284, Groundedness score: 1.0\n",
            "Index: 285, Groundedness score: 0.5\n",
            "Index: 286, Groundedness score: 1.0\n",
            "Index: 287, Groundedness score: 0.5\n",
            "Index: 288, Groundedness score: 0.5\n",
            "Index: 289, Groundedness score: 1.0\n",
            "Saved batch ending at index 289 to eval_result/llama_3b_groundedness.csv\n",
            "Index: 290, Groundedness score: 0.5\n",
            "Index: 291, Groundedness score: 0.0\n",
            "Index: 292, Groundedness score: 0.0\n",
            "Index: 293, Groundedness score: 0.5\n",
            "Index: 294, Groundedness score: 0.0\n",
            "Index: 295, Groundedness score: 0.25\n",
            "Index: 296, Groundedness score: 0.5\n",
            "Index: 297, Groundedness score: 0.5\n",
            "Index: 298, Groundedness score: 0.5\n",
            "Index: 299, Groundedness score: 0.5\n",
            "Saved batch ending at index 299 to eval_result/llama_3b_groundedness.csv\n",
            "\n",
            "=== Groundedness Score Distribution ===\n",
            "Score 0.0: 47 responses\n",
            "Score 0.25: 21 responses\n",
            "Score 0.5: 93 responses\n",
            "Score 0.75: 14 responses\n",
            "Score 1.0: 125 responses\n",
            "\n",
            "Total Groundedness score (sum): 187.25\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from collections import Counter\n",
        "from time import sleep\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Load API key từ .env\n",
        "# -------------------------------\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Tạo Gemini 2.5 Flash instance\n",
        "# -------------------------------\n",
        "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Prompt functions (Judge 1 & Judge 2)\n",
        "# -------------------------------\n",
        "def response_groundedness_judge1_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"### Instruction\n",
        "You are a world class expert designed to evaluate the groundedness of an assertion.\n",
        "You will be provided with an assertion and a context.\n",
        "Your task is to determine if the assertion is supported by the context.\n",
        "Follow the instructions below:\n",
        "A. If the assertion or context is empty, say 0.\n",
        "B. If the assertion is not supported by the context, say 0.\n",
        "C. If the assertion is partially supported by the context, say 1.\n",
        "D. If the assertion is fully supported by the context, say 2.\n",
        "You must provide a rating of 0, 1, or 2, nothing else.\n",
        "\n",
        "### Context:\n",
        "<{context}>\n",
        "\n",
        "### Assertion:\n",
        "<{response}>\n",
        "\n",
        "Analyzing Context and Response, the Groundedness score is \"\"\"\n",
        "\n",
        "def response_groundedness_judge2_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"As a specialist in assessing the strength of connections between statements and their given contexts, I will evaluate the level of support an assertion receives from the provided context. Follow these guidelines:\n",
        "\n",
        "* If the assertion or context is empty or assertion is not supported, assign a score of 0.\n",
        "* If the assertion is partially supported, assign a score of 1.\n",
        "* If the assertion is fully supported, assign a score of 2.\n",
        "\n",
        "I will provide a rating of 0, 1, or 2, without any additional information.\n",
        "\n",
        "---\n",
        "**Context:**\n",
        "[{context}]\n",
        "\n",
        "**Assertion:**\n",
        "[{response}]\n",
        "\n",
        "Do not explain. Based on the provided context and response, the Groundedness score is:\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Helper function gọi Gemini Flash và chuẩn hóa\n",
        "# -------------------------------\n",
        "def call_gemini(prompt: str) -> float:\n",
        "    \"\"\"Gọi Gemini Flash, parse output thành float 0,1,2\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        text = response.text.strip()\n",
        "        score = int(text)\n",
        "        if score in [0, 1, 2]:\n",
        "            return score\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR call_gemini]: {e}\")\n",
        "    return None\n",
        "\n",
        "def compute_groundedness(response: str, context: str) -> float:\n",
        "    \"\"\"Tính điểm groundedness dựa trên 2 judge\"\"\"\n",
        "    score1 = call_gemini(response_groundedness_judge1_prompt(response, context))\n",
        "    score2 = call_gemini(response_groundedness_judge2_prompt(response, context))\n",
        "\n",
        "    normalized_scores = []\n",
        "    if score1 is not None:\n",
        "        normalized_scores.append(score1 / 2)\n",
        "    if score2 is not None:\n",
        "        normalized_scores.append(score2 / 2)\n",
        "\n",
        "    if not normalized_scores:\n",
        "        return 0.0\n",
        "    return sum(normalized_scores) / len(normalized_scores)\n",
        "\n",
        "# -------------------------------\n",
        "# 5️⃣ Load data từ CSV (toàn bộ, không sample)\n",
        "# -------------------------------\n",
        "csv_path = \"data/llama_3b_eval_data.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# -------------------------------\n",
        "# 6️⃣ Chuẩn bị output\n",
        "# -------------------------------\n",
        "os.makedirs(\"eval_result\", exist_ok=True)\n",
        "output_csv = \"eval_result/llama_3b_groundedness.csv\"\n",
        "\n",
        "# Nếu file đã tồn tại, xóa để ghi batch mới\n",
        "if os.path.exists(output_csv):\n",
        "    os.remove(output_csv)\n",
        "\n",
        "# -------------------------------\n",
        "# 7️⃣ Chạy evaluation theo batch 10 dòng\n",
        "# -------------------------------\n",
        "BATCH_SIZE = 10\n",
        "batch_results = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    response = str(row[\"response\"])\n",
        "    context = str(row[\"context\"])\n",
        "    score = compute_groundedness(response, context)\n",
        "    print(f\"Index: {idx}, Groundedness score: {score}\")\n",
        "\n",
        "    batch_results.append({\n",
        "        \"question\": row.get(\"question\", \"\"),\n",
        "        \"context\": context,\n",
        "        \"response\": response,\n",
        "        \"groundedness_score\": score\n",
        "    })\n",
        "\n",
        "    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n",
        "    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n",
        "        df_batch = pd.DataFrame(batch_results)\n",
        "        # Append nếu file đã tồn tại, else write mới\n",
        "        if os.path.exists(output_csv):\n",
        "            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n",
        "        else:\n",
        "            df_batch.to_csv(output_csv, index=False)\n",
        "        batch_results = []  # reset batch\n",
        "        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8️⃣ Thống kê tổng số điểm\n",
        "# -------------------------------\n",
        "df_final = pd.read_csv(output_csv)\n",
        "score_counts = Counter(df_final[\"groundedness_score\"])\n",
        "print(\"\\n=== Groundedness Score Distribution ===\")\n",
        "for score_level in sorted(score_counts.keys()):\n",
        "    print(f\"Score {score_level}: {score_counts[score_level]} responses\")\n",
        "\n",
        "total_score = df_final[\"groundedness_score\"].sum()\n",
        "print(f\"\\nTotal Groundedness score (sum): {total_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XQE0hyZS0ykh",
      "metadata": {
        "id": "XQE0hyZS0ykh"
      },
      "source": [
        "### Qwen2-7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C0YsoqVO1zuo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C0YsoqVO1zuo",
        "outputId": "1dc4c781-84bd-446a-9df3-1bed9d95e8c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: 0, Groundedness score: 1.0\n",
            "Index: 1, Groundedness score: 0.0\n",
            "Index: 2, Groundedness score: 1.0\n",
            "Index: 3, Groundedness score: 0.75\n",
            "Index: 4, Groundedness score: 1.0\n",
            "Index: 5, Groundedness score: 1.0\n",
            "Index: 6, Groundedness score: 1.0\n",
            "Index: 7, Groundedness score: 1.0\n",
            "Index: 8, Groundedness score: 1.0\n",
            "Index: 9, Groundedness score: 1.0\n",
            "Saved batch ending at index 9 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 10, Groundedness score: 0.5\n",
            "Index: 11, Groundedness score: 1.0\n",
            "Index: 12, Groundedness score: 0.0\n",
            "Index: 13, Groundedness score: 0.0\n",
            "Index: 14, Groundedness score: 1.0\n",
            "Index: 15, Groundedness score: 1.0\n",
            "Index: 16, Groundedness score: 1.0\n",
            "Index: 17, Groundedness score: 1.0\n",
            "Index: 18, Groundedness score: 1.0\n",
            "Index: 19, Groundedness score: 0.5\n",
            "Saved batch ending at index 19 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 20, Groundedness score: 0.5\n",
            "Index: 21, Groundedness score: 0.0\n",
            "Index: 22, Groundedness score: 1.0\n",
            "Index: 23, Groundedness score: 1.0\n",
            "Index: 24, Groundedness score: 1.0\n",
            "Index: 25, Groundedness score: 0.0\n",
            "Index: 26, Groundedness score: 1.0\n",
            "Index: 27, Groundedness score: 0.5\n",
            "Index: 28, Groundedness score: 1.0\n",
            "Index: 29, Groundedness score: 0.5\n",
            "Saved batch ending at index 29 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 30, Groundedness score: 0.0\n",
            "Index: 31, Groundedness score: 0.0\n",
            "Index: 32, Groundedness score: 1.0\n",
            "Index: 33, Groundedness score: 0.5\n",
            "Index: 34, Groundedness score: 1.0\n",
            "Index: 35, Groundedness score: 1.0\n",
            "Index: 36, Groundedness score: 1.0\n",
            "Index: 37, Groundedness score: 1.0\n",
            "Index: 38, Groundedness score: 1.0\n",
            "Index: 39, Groundedness score: 1.0\n",
            "Saved batch ending at index 39 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 40, Groundedness score: 1.0\n",
            "Index: 41, Groundedness score: 1.0\n",
            "Index: 42, Groundedness score: 1.0\n",
            "Index: 43, Groundedness score: 0.0\n",
            "Index: 44, Groundedness score: 0.75\n",
            "Index: 45, Groundedness score: 1.0\n",
            "Index: 46, Groundedness score: 1.0\n",
            "Index: 47, Groundedness score: 1.0\n",
            "Index: 48, Groundedness score: 0.0\n",
            "Index: 49, Groundedness score: 0.25\n",
            "Saved batch ending at index 49 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 50, Groundedness score: 1.0\n",
            "Index: 51, Groundedness score: 0.5\n",
            "Index: 52, Groundedness score: 1.0\n",
            "Index: 53, Groundedness score: 1.0\n",
            "Index: 54, Groundedness score: 1.0\n",
            "Index: 55, Groundedness score: 0.0\n",
            "Index: 56, Groundedness score: 1.0\n",
            "Index: 57, Groundedness score: 1.0\n",
            "Index: 58, Groundedness score: 0.0\n",
            "Index: 59, Groundedness score: 1.0\n",
            "Saved batch ending at index 59 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 60, Groundedness score: 1.0\n",
            "Index: 61, Groundedness score: 1.0\n",
            "Index: 62, Groundedness score: 1.0\n",
            "Index: 63, Groundedness score: 1.0\n",
            "Index: 64, Groundedness score: 0.75\n",
            "Index: 65, Groundedness score: 1.0\n",
            "Index: 66, Groundedness score: 1.0\n",
            "Index: 67, Groundedness score: 1.0\n",
            "Index: 68, Groundedness score: 1.0\n",
            "Index: 69, Groundedness score: 1.0\n",
            "Saved batch ending at index 69 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 70, Groundedness score: 1.0\n",
            "Index: 71, Groundedness score: 1.0\n",
            "Index: 72, Groundedness score: 1.0\n",
            "Index: 73, Groundedness score: 1.0\n",
            "Index: 74, Groundedness score: 1.0\n",
            "Index: 75, Groundedness score: 0.0\n",
            "Index: 76, Groundedness score: 0.5\n",
            "Index: 77, Groundedness score: 1.0\n",
            "Index: 78, Groundedness score: 1.0\n",
            "Index: 79, Groundedness score: 1.0\n",
            "Saved batch ending at index 79 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 80, Groundedness score: 1.0\n",
            "Index: 81, Groundedness score: 1.0\n",
            "Index: 82, Groundedness score: 0.0\n",
            "Index: 83, Groundedness score: 1.0\n",
            "Index: 84, Groundedness score: 0.25\n",
            "Index: 85, Groundedness score: 1.0\n",
            "Index: 86, Groundedness score: 1.0\n",
            "Index: 87, Groundedness score: 1.0\n",
            "Index: 88, Groundedness score: 0.5\n",
            "Index: 89, Groundedness score: 0.5\n",
            "Saved batch ending at index 89 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 90, Groundedness score: 1.0\n",
            "Index: 91, Groundedness score: 1.0\n",
            "Index: 92, Groundedness score: 0.0\n",
            "Index: 93, Groundedness score: 0.75\n",
            "Index: 94, Groundedness score: 1.0\n",
            "Index: 95, Groundedness score: 0.0\n",
            "Index: 96, Groundedness score: 1.0\n",
            "Index: 97, Groundedness score: 1.0\n",
            "Index: 98, Groundedness score: 1.0\n",
            "Index: 99, Groundedness score: 0.5\n",
            "Saved batch ending at index 99 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 100, Groundedness score: 0.5\n",
            "Index: 101, Groundedness score: 1.0\n",
            "Index: 102, Groundedness score: 0.25\n",
            "Index: 103, Groundedness score: 0.75\n",
            "Index: 104, Groundedness score: 0.0\n",
            "Index: 105, Groundedness score: 0.0\n",
            "Index: 106, Groundedness score: 1.0\n",
            "Index: 107, Groundedness score: 1.0\n",
            "Index: 108, Groundedness score: 1.0\n",
            "Index: 109, Groundedness score: 0.5\n",
            "Saved batch ending at index 109 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 110, Groundedness score: 1.0\n",
            "Index: 111, Groundedness score: 1.0\n",
            "Index: 112, Groundedness score: 1.0\n",
            "Index: 113, Groundedness score: 1.0\n",
            "Index: 114, Groundedness score: 1.0\n",
            "Index: 115, Groundedness score: 1.0\n",
            "Index: 116, Groundedness score: 1.0\n",
            "Index: 117, Groundedness score: 1.0\n",
            "Index: 118, Groundedness score: 1.0\n",
            "Index: 119, Groundedness score: 1.0\n",
            "Saved batch ending at index 119 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 120, Groundedness score: 1.0\n",
            "Index: 121, Groundedness score: 1.0\n",
            "Index: 122, Groundedness score: 0.75\n",
            "Index: 123, Groundedness score: 0.5\n",
            "Index: 124, Groundedness score: 1.0\n",
            "Index: 125, Groundedness score: 0.5\n",
            "Index: 126, Groundedness score: 1.0\n",
            "Index: 127, Groundedness score: 0.0\n",
            "Index: 128, Groundedness score: 0.0\n",
            "Index: 129, Groundedness score: 1.0\n",
            "Saved batch ending at index 129 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 130, Groundedness score: 0.0\n",
            "Index: 131, Groundedness score: 0.5\n",
            "Index: 132, Groundedness score: 1.0\n",
            "Index: 133, Groundedness score: 1.0\n",
            "Index: 134, Groundedness score: 1.0\n",
            "Index: 135, Groundedness score: 1.0\n",
            "Index: 136, Groundedness score: 1.0\n",
            "Index: 137, Groundedness score: 0.5\n",
            "Index: 138, Groundedness score: 1.0\n",
            "Index: 139, Groundedness score: 1.0\n",
            "Saved batch ending at index 139 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 140, Groundedness score: 0.5\n",
            "Index: 141, Groundedness score: 1.0\n",
            "Index: 142, Groundedness score: 0.0\n",
            "Index: 143, Groundedness score: 1.0\n",
            "Index: 144, Groundedness score: 1.0\n",
            "Index: 145, Groundedness score: 1.0\n",
            "Index: 146, Groundedness score: 1.0\n",
            "Index: 147, Groundedness score: 1.0\n",
            "Index: 148, Groundedness score: 1.0\n",
            "Index: 149, Groundedness score: 1.0\n",
            "Saved batch ending at index 149 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 150, Groundedness score: 1.0\n",
            "Index: 151, Groundedness score: 1.0\n",
            "Index: 152, Groundedness score: 0.25\n",
            "Index: 153, Groundedness score: 1.0\n",
            "Index: 154, Groundedness score: 0.0\n",
            "Index: 155, Groundedness score: 1.0\n",
            "Index: 156, Groundedness score: 1.0\n",
            "Index: 157, Groundedness score: 1.0\n",
            "Index: 158, Groundedness score: 1.0\n",
            "Index: 159, Groundedness score: 1.0\n",
            "Saved batch ending at index 159 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 160, Groundedness score: 1.0\n",
            "Index: 161, Groundedness score: 0.0\n",
            "Index: 162, Groundedness score: 1.0\n",
            "Index: 163, Groundedness score: 1.0\n",
            "Index: 164, Groundedness score: 1.0\n",
            "Index: 165, Groundedness score: 1.0\n",
            "Index: 166, Groundedness score: 1.0\n",
            "Index: 167, Groundedness score: 1.0\n",
            "Index: 168, Groundedness score: 1.0\n",
            "Index: 169, Groundedness score: 1.0\n",
            "Saved batch ending at index 169 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 170, Groundedness score: 1.0\n",
            "Index: 171, Groundedness score: 1.0\n",
            "Index: 172, Groundedness score: 0.0\n",
            "Index: 173, Groundedness score: 1.0\n",
            "Index: 174, Groundedness score: 1.0\n",
            "Index: 175, Groundedness score: 1.0\n",
            "Index: 176, Groundedness score: 0.75\n",
            "Index: 177, Groundedness score: 1.0\n",
            "Index: 178, Groundedness score: 1.0\n",
            "Index: 179, Groundedness score: 1.0\n",
            "Saved batch ending at index 179 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 180, Groundedness score: 1.0\n",
            "Index: 181, Groundedness score: 1.0\n",
            "Index: 182, Groundedness score: 0.5\n",
            "Index: 183, Groundedness score: 1.0\n",
            "Index: 184, Groundedness score: 1.0\n",
            "Index: 185, Groundedness score: 0.0\n",
            "Index: 186, Groundedness score: 0.25\n",
            "Index: 187, Groundedness score: 1.0\n",
            "Index: 188, Groundedness score: 0.5\n",
            "Index: 189, Groundedness score: 0.0\n",
            "Saved batch ending at index 189 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 190, Groundedness score: 0.0\n",
            "Index: 191, Groundedness score: 1.0\n",
            "Index: 192, Groundedness score: 1.0\n",
            "Index: 193, Groundedness score: 1.0\n",
            "Index: 194, Groundedness score: 0.25\n",
            "Index: 195, Groundedness score: 1.0\n",
            "Index: 196, Groundedness score: 1.0\n",
            "Index: 197, Groundedness score: 1.0\n",
            "Index: 198, Groundedness score: 1.0\n",
            "Index: 199, Groundedness score: 1.0\n",
            "Saved batch ending at index 199 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 200, Groundedness score: 1.0\n",
            "Index: 201, Groundedness score: 0.0\n",
            "Index: 202, Groundedness score: 1.0\n",
            "Index: 203, Groundedness score: 1.0\n",
            "Index: 204, Groundedness score: 1.0\n",
            "Index: 205, Groundedness score: 1.0\n",
            "Index: 206, Groundedness score: 1.0\n",
            "Index: 207, Groundedness score: 1.0\n",
            "Index: 208, Groundedness score: 0.75\n",
            "Index: 209, Groundedness score: 1.0\n",
            "Saved batch ending at index 209 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 210, Groundedness score: 1.0\n",
            "Index: 211, Groundedness score: 1.0\n",
            "Index: 212, Groundedness score: 1.0\n",
            "Index: 213, Groundedness score: 1.0\n",
            "Index: 214, Groundedness score: 0.0\n",
            "Index: 215, Groundedness score: 0.0\n",
            "Index: 216, Groundedness score: 0.0\n",
            "Index: 217, Groundedness score: 1.0\n",
            "Index: 218, Groundedness score: 0.0\n",
            "Index: 219, Groundedness score: 1.0\n",
            "Saved batch ending at index 219 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 220, Groundedness score: 0.0\n",
            "Index: 221, Groundedness score: 1.0\n",
            "Index: 222, Groundedness score: 1.0\n",
            "Index: 223, Groundedness score: 1.0\n",
            "Index: 224, Groundedness score: 1.0\n",
            "Index: 225, Groundedness score: 1.0\n",
            "Index: 226, Groundedness score: 1.0\n",
            "Index: 227, Groundedness score: 1.0\n",
            "Index: 228, Groundedness score: 1.0\n",
            "Index: 229, Groundedness score: 1.0\n",
            "Saved batch ending at index 229 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 230, Groundedness score: 1.0\n",
            "Index: 231, Groundedness score: 1.0\n",
            "Index: 232, Groundedness score: 1.0\n",
            "Index: 233, Groundedness score: 1.0\n",
            "Index: 234, Groundedness score: 1.0\n",
            "Index: 235, Groundedness score: 1.0\n",
            "Index: 236, Groundedness score: 1.0\n",
            "Index: 237, Groundedness score: 1.0\n",
            "Index: 238, Groundedness score: 1.0\n",
            "Index: 239, Groundedness score: 0.0\n",
            "Saved batch ending at index 239 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 240, Groundedness score: 1.0\n",
            "Index: 241, Groundedness score: 1.0\n",
            "Index: 242, Groundedness score: 0.5\n",
            "Index: 243, Groundedness score: 1.0\n",
            "Index: 244, Groundedness score: 0.0\n",
            "Index: 245, Groundedness score: 0.5\n",
            "Index: 246, Groundedness score: 1.0\n",
            "Index: 247, Groundedness score: 1.0\n",
            "Index: 248, Groundedness score: 0.0\n",
            "Index: 249, Groundedness score: 1.0\n",
            "Saved batch ending at index 249 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 250, Groundedness score: 0.0\n",
            "Index: 251, Groundedness score: 1.0\n",
            "Index: 252, Groundedness score: 1.0\n",
            "Index: 253, Groundedness score: 0.5\n",
            "Index: 254, Groundedness score: 0.0\n",
            "Index: 255, Groundedness score: 1.0\n",
            "Index: 256, Groundedness score: 1.0\n",
            "Index: 257, Groundedness score: 0.0\n",
            "Index: 258, Groundedness score: 0.0\n",
            "Index: 259, Groundedness score: 0.0\n",
            "Saved batch ending at index 259 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 260, Groundedness score: 1.0\n",
            "Index: 261, Groundedness score: 0.0\n",
            "Index: 262, Groundedness score: 0.5\n",
            "Index: 263, Groundedness score: 1.0\n",
            "Index: 264, Groundedness score: 1.0\n",
            "Index: 265, Groundedness score: 1.0\n",
            "Index: 266, Groundedness score: 1.0\n",
            "Index: 267, Groundedness score: 0.75\n",
            "Index: 268, Groundedness score: 1.0\n",
            "Index: 269, Groundedness score: 0.25\n",
            "Saved batch ending at index 269 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 270, Groundedness score: 0.0\n",
            "Index: 271, Groundedness score: 0.0\n",
            "Index: 272, Groundedness score: 1.0\n",
            "Index: 273, Groundedness score: 1.0\n",
            "Index: 274, Groundedness score: 1.0\n",
            "Index: 275, Groundedness score: 1.0\n",
            "Index: 276, Groundedness score: 1.0\n",
            "Index: 277, Groundedness score: 0.75\n",
            "Index: 278, Groundedness score: 0.0\n",
            "Index: 279, Groundedness score: 1.0\n",
            "Saved batch ending at index 279 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 280, Groundedness score: 1.0\n",
            "Index: 281, Groundedness score: 1.0\n",
            "Index: 282, Groundedness score: 1.0\n",
            "Index: 283, Groundedness score: 1.0\n",
            "Index: 284, Groundedness score: 1.0\n",
            "Index: 285, Groundedness score: 1.0\n",
            "Index: 286, Groundedness score: 1.0\n",
            "Index: 287, Groundedness score: 1.0\n",
            "Index: 288, Groundedness score: 0.75\n",
            "Index: 289, Groundedness score: 1.0\n",
            "Saved batch ending at index 289 to eval_result/qwen2_7b_groundedness.csv\n",
            "Index: 290, Groundedness score: 1.0\n",
            "Index: 291, Groundedness score: 0.0\n",
            "Index: 292, Groundedness score: 0.5\n",
            "Index: 293, Groundedness score: 1.0\n",
            "Index: 294, Groundedness score: 1.0\n",
            "Index: 295, Groundedness score: 1.0\n",
            "Index: 296, Groundedness score: 1.0\n",
            "Index: 297, Groundedness score: 1.0\n",
            "Index: 298, Groundedness score: 1.0\n",
            "Index: 299, Groundedness score: 1.0\n",
            "Saved batch ending at index 299 to eval_result/qwen2_7b_groundedness.csv\n",
            "\n",
            "=== Groundedness Score Distribution ===\n",
            "Score 0.0: 46 responses\n",
            "Score 0.25: 7 responses\n",
            "Score 0.5: 25 responses\n",
            "Score 0.75: 11 responses\n",
            "Score 1.0: 211 responses\n",
            "\n",
            "Total Groundedness score (sum): 233.5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from collections import Counter\n",
        "from time import sleep\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Load API key từ .env\n",
        "# -------------------------------\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Tạo Gemini 2.5 Flash instance\n",
        "# -------------------------------\n",
        "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Prompt functions (Judge 1 & Judge 2)\n",
        "# -------------------------------\n",
        "def response_groundedness_judge1_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"### Instruction\n",
        "You are a world class expert designed to evaluate the groundedness of an assertion.\n",
        "You will be provided with an assertion and a context.\n",
        "Your task is to determine if the assertion is supported by the context.\n",
        "Follow the instructions below:\n",
        "A. If the assertion or context is empty, say 0.\n",
        "B. If the assertion is not supported by the context, say 0.\n",
        "C. If the assertion is partially supported by the context, say 1.\n",
        "D. If the assertion is fully supported by the context, say 2.\n",
        "You must provide a rating of 0, 1, or 2, nothing else.\n",
        "\n",
        "### Context:\n",
        "<{context}>\n",
        "\n",
        "### Assertion:\n",
        "<{response}>\n",
        "\n",
        "Analyzing Context and Response, the Groundedness score is \"\"\"\n",
        "\n",
        "def response_groundedness_judge2_prompt(response: str, context: str) -> str:\n",
        "    return f\"\"\"As a specialist in assessing the strength of connections between statements and their given contexts, I will evaluate the level of support an assertion receives from the provided context. Follow these guidelines:\n",
        "\n",
        "* If the assertion or context is empty or assertion is not supported, assign a score of 0.\n",
        "* If the assertion is partially supported, assign a score of 1.\n",
        "* If the assertion is fully supported, assign a score of 2.\n",
        "\n",
        "I will provide a rating of 0, 1, or 2, without any additional information.\n",
        "\n",
        "---\n",
        "**Context:**\n",
        "[{context}]\n",
        "\n",
        "**Assertion:**\n",
        "[{response}]\n",
        "\n",
        "Do not explain. Based on the provided context and response, the Groundedness score is:\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Helper function gọi Gemini Flash và chuẩn hóa\n",
        "# -------------------------------\n",
        "def call_gemini(prompt: str) -> float:\n",
        "    \"\"\"Gọi Gemini Flash, parse output thành float 0,1,2\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        text = response.text.strip()\n",
        "        score = int(text)\n",
        "        if score in [0, 1, 2]:\n",
        "            return score\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR call_gemini]: {e}\")\n",
        "    return None\n",
        "\n",
        "def compute_groundedness(response: str, context: str) -> float:\n",
        "    \"\"\"Tính điểm groundedness dựa trên 2 judge\"\"\"\n",
        "    score1 = call_gemini(response_groundedness_judge1_prompt(response, context))\n",
        "    score2 = call_gemini(response_groundedness_judge2_prompt(response, context))\n",
        "\n",
        "    normalized_scores = []\n",
        "    if score1 is not None:\n",
        "        normalized_scores.append(score1 / 2)\n",
        "    if score2 is not None:\n",
        "        normalized_scores.append(score2 / 2)\n",
        "\n",
        "    if not normalized_scores:\n",
        "        return 0.0\n",
        "    return sum(normalized_scores) / len(normalized_scores)\n",
        "\n",
        "# -------------------------------\n",
        "# 5️⃣ Load data từ CSV (toàn bộ, không sample)\n",
        "# -------------------------------\n",
        "csv_path = \"data/qwen2_7b_eval_data.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# -------------------------------\n",
        "# 6️⃣ Chuẩn bị output\n",
        "# -------------------------------\n",
        "os.makedirs(\"eval_result\", exist_ok=True)\n",
        "output_csv = \"eval_result/qwen2_7b_groundedness.csv\"\n",
        "\n",
        "# Nếu file đã tồn tại, xóa để ghi batch mới\n",
        "if os.path.exists(output_csv):\n",
        "    os.remove(output_csv)\n",
        "\n",
        "# -------------------------------\n",
        "# 7️⃣ Chạy evaluation theo batch 10 dòng\n",
        "# -------------------------------\n",
        "BATCH_SIZE = 10\n",
        "batch_results = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    response = str(row[\"response\"])\n",
        "    context = str(row[\"context\"])\n",
        "    score = compute_groundedness(response, context)\n",
        "    print(f\"Index: {idx}, Groundedness score: {score}\")\n",
        "\n",
        "    batch_results.append({\n",
        "        \"question\": row.get(\"question\", \"\"),\n",
        "        \"context\": context,\n",
        "        \"response\": response,\n",
        "        \"groundedness_score\": score\n",
        "    })\n",
        "\n",
        "    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n",
        "    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n",
        "        df_batch = pd.DataFrame(batch_results)\n",
        "        # Append nếu file đã tồn tại, else write mới\n",
        "        if os.path.exists(output_csv):\n",
        "            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n",
        "        else:\n",
        "            df_batch.to_csv(output_csv, index=False)\n",
        "        batch_results = []  # reset batch\n",
        "        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8️⃣ Thống kê tổng số điểm\n",
        "# -------------------------------\n",
        "df_final = pd.read_csv(output_csv)\n",
        "score_counts = Counter(df_final[\"groundedness_score\"])\n",
        "print(\"\\n=== Groundedness Score Distribution ===\")\n",
        "for score_level in sorted(score_counts.keys()):\n",
        "    print(f\"Score {score_level}: {score_counts[score_level]} responses\")\n",
        "\n",
        "total_score = df_final[\"groundedness_score\"].sum()\n",
        "print(f\"\\nTotal Groundedness score (sum): {total_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bm_9e1xdKjWX",
      "metadata": {
        "id": "bm_9e1xdKjWX"
      },
      "source": [
        "### Groundedness Score Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "591798c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file: base_qwen2_3b_groundedness.csv ...\n",
            "Processing file: llama_3b_groundedness.csv ...\n",
            "Processing file: qwen2_3b_groundedness.csv ...\n",
            "Processing file: qwen2_7b_groundedness.csv ...\n",
            "\n",
            "Full summary saved to eval_result\\groundedness_summary_full.txt\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "# Thư mục chứa các file CSV\n",
        "data_dir = Path(\"eval_result\")\n",
        "\n",
        "# Lấy danh sách file CSV có tên chứa \"groundedness\"\n",
        "csv_files = sorted(data_dir.glob(\"*groundedness*.csv\"))\n",
        "\n",
        "# File để lưu toàn bộ output\n",
        "summary_file = data_dir / \"groundedness_summary_full.txt\"\n",
        "\n",
        "with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    if not csv_files:\n",
        "        print(\"Không tìm thấy file CSV nào có 'groundedness' trong tên trong thư mục:\", data_dir)\n",
        "        f.write(\"Không tìm thấy file CSV nào có 'groundedness' trong tên.\\n\")\n",
        "    else:\n",
        "        for csv_file in csv_files:\n",
        "            f.write(f\"\\n=== File: {csv_file.name} ===\\n\")\n",
        "            print(f\"Processing file: {csv_file.name} ...\")  # In tiến trình ngắn gọn\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file)\n",
        "            except Exception as e:\n",
        "                print(\"Lỗi khi đọc file:\", e)\n",
        "                f.write(f\"Lỗi khi đọc file: {e}\\n\")\n",
        "                continue\n",
        "\n",
        "            # Thống kê số lượng từng điểm groundedness_score\n",
        "            score_counts = Counter(df[\"groundedness_score\"])\n",
        "            if not score_counts:\n",
        "                f.write(\"File rỗng hoặc không có giá trị groundedness_score\\n\")\n",
        "                continue\n",
        "\n",
        "            f.write(\"=== Groundedness Score Distribution ===\\n\")\n",
        "            for score_level in sorted(score_counts.keys()):\n",
        "                f.write(f\"Score {score_level}: {score_counts[score_level]} responses\\n\")\n",
        "\n",
        "            # Tính tổng điểm\n",
        "            total_score = df[\"groundedness_score\"].sum()\n",
        "            f.write(f\"Total Groundedness score (sum): {total_score}\\n\")\n",
        "\n",
        "print(f\"\\nFull summary saved to {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f7EtN4twKmOI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7EtN4twKmOI",
        "outputId": "f532f164-ee77-48db-910e-48a004a20924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== File: base_qwen2_3b_groundedness.csv ===\n",
            "=== Groundedness Score Distribution ===\n",
            "Score 0.0: 61 responses\n",
            "Score 0.25: 30 responses\n",
            "Score 0.5: 57 responses\n",
            "Score 0.75: 31 responses\n",
            "Score 1.0: 121 responses\n",
            "Total Groundedness score (sum): 180.25\n",
            "\n",
            "=== File: llama_3b_groundedness.csv ===\n",
            "=== Groundedness Score Distribution ===\n",
            "Score 0.0: 47 responses\n",
            "Score 0.25: 21 responses\n",
            "Score 0.5: 93 responses\n",
            "Score 0.75: 14 responses\n",
            "Score 1.0: 125 responses\n",
            "Total Groundedness score (sum): 187.25\n",
            "\n",
            "=== File: qwen2_3b_groundedness.csv ===\n",
            "=== Groundedness Score Distribution ===\n",
            "Score 0.0: 37 responses\n",
            "Score 0.25: 12 responses\n",
            "Score 0.5: 83 responses\n",
            "Score 0.75: 30 responses\n",
            "Score 1.0: 138 responses\n",
            "Total Groundedness score (sum): 205.0\n",
            "\n",
            "=== File: qwen2_7b_groundedness.csv ===\n",
            "=== Groundedness Score Distribution ===\n",
            "Score 0.0: 46 responses\n",
            "Score 0.25: 7 responses\n",
            "Score 0.5: 25 responses\n",
            "Score 0.75: 11 responses\n",
            "Score 1.0: 211 responses\n",
            "Total Groundedness score (sum): 233.5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "# Thư mục chứa file CSV\n",
        "data_dir = Path(\"eval_result\")\n",
        "\n",
        "# Danh sách file muốn đọc từng cái\n",
        "file_list = [\n",
        "    \"base_qwen2_3b_groundedness.csv\",\n",
        "    \"llama_3b_groundedness.csv\",\n",
        "    \"qwen2_3b_groundedness.csv\",\n",
        "    \"qwen2_7b_groundedness.csv\"\n",
        "]\n",
        "\n",
        "for file_name in file_list:\n",
        "    csv_path = data_dir / file_name\n",
        "    if not csv_path.exists():\n",
        "        print(f\"File không tồn tại: {file_name}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n=== File: {file_name} ===\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
        "        # Đảm bảo cột groundedness_score là float, NaN -> 0\n",
        "        df[\"groundedness_score\"] = pd.to_numeric(df[\"groundedness_score\"], errors=\"coerce\").fillna(0)\n",
        "    except Exception as e:\n",
        "        print(\"Lỗi khi đọc file:\", e)\n",
        "        continue\n",
        "\n",
        "    # Thống kê số lượng từng score\n",
        "    score_counts = Counter(df[\"groundedness_score\"])\n",
        "    print(\"=== Groundedness Score Distribution ===\")\n",
        "    for score_level in sorted(score_counts.keys()):\n",
        "        print(f\"Score {score_level}: {score_counts[score_level]} responses\")\n",
        "\n",
        "    # Tổng điểm\n",
        "    total_score = df[\"groundedness_score\"].sum()\n",
        "    print(f\"Total Groundedness score (sum): {total_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S-2lFckBaDy8",
      "metadata": {
        "id": "S-2lFckBaDy8"
      },
      "source": [
        "# BertScore"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zv8a9W_GUWt8",
      "metadata": {
        "id": "zv8a9W_GUWt8"
      },
      "source": [
        "### Base qwen2-3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5Pn9D3Cfawwf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467,
          "referenced_widgets": [
            "1463a48503694647a8eaa51e2085c30e",
            "d774b2b026ae4647be08c27195810795",
            "25371e32a2e041baba8779aa2e08d89d",
            "87f306ea03354ebc8e15a535085e298e",
            "db090e3545a44dea8c7201098c3f086d",
            "952be771f51f45888b57ad600e061aa2",
            "6064e3bc966941c2b0e82bb4070bfc38",
            "05e7ad9f5cbf46fcadf32a797e553f2e",
            "e696a2df9be548108ca15970fbf9e1ff",
            "561956ed74ca4ade9f4b9bab4bbaccaa",
            "93846f875794433cbaac228c504f79ce",
            "30eef6f3b6944a589d2bec1a2e740ac1",
            "942627f951a54cf4b935ea58cf8df7bd",
            "d889a3a99da342ddb87fc30f52d893ec",
            "019ca64cab054a4baf6ce0e664dc7771",
            "fcfaa4a79bcb458998b534e4a78e8332",
            "a1cc85f02f874c6da5734ddfb91c2ddc",
            "7986a068cf7f4047a05b29607364946d",
            "ee14fce0a3304f48860574e1d102cbbf",
            "b0136b148f8f4905af78beae60eceab4",
            "c4afa5bd94c44662b2a87c67af992878",
            "77fa7622493345518cf8d6619c105333",
            "e7117f45332b46b3888437efd22546ff",
            "18c9cfd289e8497fba9a74deea668036",
            "57f133c6816c48629e7b90ab05022c53",
            "f85121d0f5514851b2d783ae628b21a5",
            "294fba1b8e574542a18f0ed1a2cecc54",
            "fdd0d9f4b8654a0bb1f17591e872bf81",
            "0f927bc19e544269984bb0a4ab34ae4b",
            "cf36c55f5f4e4bfc9a9d122574d5fdcd",
            "73d131fc8c69477d8fbeb807b4393ff6",
            "511f74ff2c37450f927f3fb0f313ff43",
            "bab5fb8694e2403e97c23035040905c3",
            "f47baa559deb4d4daf09765c61137253",
            "64699e7eb882414291f9d71badda1eef",
            "73f88a5bf02541beb0d564169e4858c2",
            "33bd387793b5426089c805e3518b4e19",
            "970833c6c4b14cc3a945efcd7c79231a",
            "5108ee605774494f9662cb1d9524b74e",
            "708f03df501b4ef2a23651e62eaff394",
            "2efd13ab441644a2bc07d4b924fa8f1b",
            "c8f45f5fa5f54277823ed957717618b2",
            "6c02ef874f87478dbbe2e970eb5e44bf",
            "321f080a058e4a58863fa16f30096854",
            "d634bf8fc71a404a89b92c5faa55d4a9",
            "2b3a0664144d4c4cb90b64c17e589693",
            "dd1786696b564e989a6136a6db3a7d1c",
            "cab8a1f9328e466d874e07a5e74780ec",
            "07f9d8ecc2a64f50ae14ba19f79a6dad",
            "72bf88c7ea884767aecedd4d5b37b857",
            "9ea4ac4816df49018fa1c8919799bede",
            "44af5dbcadf9416683cd7f12492281c3",
            "c685010ae80b435d98f260b5548e8e80",
            "dfd7ea73e3f3422d937100e7421fe2b4",
            "c0f3a0ce420a4f1fb9413d27a79cbfbc",
            "76e2d531a1b74123a18aec2197a1dade",
            "10c5ed81dd5d4a8e9c08095765cc5317",
            "26d7de360eb24798b025ba150ea82c6f",
            "2061cca575ef4c6986f8a29d3f63aa78",
            "1632a8138e274da194399eb843817e8d",
            "defd07e6a8ec4c3a918c61e90fc54af6",
            "b97d1d661f3545deb58b934ecbda97eb",
            "0c17157a36ae415f809a19d473879941",
            "070c1438579f47c1b4008cdb1fb71aca",
            "9a6ac1e062f94b82b5945080e41d5453",
            "af008636438741c396156c7db8e6d4e8",
            "fd7da4b9cdcf4adeb5e7d4816c3b4097",
            "fbae54db36194f76928626a4216f7c18",
            "bc6dc8eb71fa4be4bf772fb994a2f64c",
            "31e0d65e2f2a424ea8718d2e75840755",
            "71b04889d3704d0b8733c7f1573a8a01",
            "031dad1ec22841f796165c9670d32864",
            "6ba606be46504efd93873625c0dd4e74",
            "cfe00c657b264bcc9d7a636b8c5dc08b",
            "ac451a766dc34734b5493d7704db2fd4",
            "03a9b030e13249e29fa6ec3d23d3a931",
            "6c3050b1ba074376804d9dd2a0261874"
          ]
        },
        "id": "5Pn9D3Cfawwf",
        "outputId": "e957144a-fe5a-4263-b906-3926655c2d35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1463a48503694647a8eaa51e2085c30e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30eef6f3b6944a589d2bec1a2e740ac1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7117f45332b46b3888437efd22546ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f47baa559deb4d4daf09765c61137253",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d634bf8fc71a404a89b92c5faa55d4a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76e2d531a1b74123a18aec2197a1dade",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd7da4b9cdcf4adeb5e7d4816c3b4097",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 1820.89 seconds, 0.16 sentences/sec\n",
            "\n",
            "🎉 Done! Saved BERTScore to eval_result/base_qwen2_3b_bertscore.csv\n",
            "Average BERTScore F1 = 0.8845\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from bert_score import score\n",
        "\n",
        "# ===============================\n",
        "# 1. Load data\n",
        "# ===============================\n",
        "df_pred = pd.read_csv(\"data/base_qwen2_3b_eval_data.csv\")\n",
        "df_ref  = pd.read_csv(\"data/merged_all.csv\")\n",
        "\n",
        "# ===============================\n",
        "# 2. Lấy NUM_ROWS reference làm ground truth\n",
        "# ===============================\n",
        "NUM_ROWS = 300\n",
        "SEED = 42\n",
        "\n",
        "df_ref_sample = df_ref.sample(n=NUM_ROWS, random_state=SEED)\n",
        "df_pred_slice = df_pred.iloc[:NUM_ROWS]\n",
        "\n",
        "assert len(df_pred_slice) == len(df_ref_sample)\n",
        "\n",
        "predictions = df_pred_slice[\"response\"].astype(str).tolist()\n",
        "references  = df_ref_sample[\"response\"].astype(str).tolist()\n",
        "\n",
        "# ===============================\n",
        "# 4. Tính BERTScore cho tiếng Việt\n",
        "# ===============================\n",
        "P, R, F1 = score(\n",
        "    predictions,\n",
        "    references,\n",
        "    lang=\"vi\",                   # tokenizer cho tiếng Việt\n",
        "    model_type=\"xlm-roberta-large\",  # embedder tốt nhất cho tiếng Việt\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 5. Gộp vào DataFrame kết quả\n",
        "# ===============================\n",
        "df_result = df_pred_slice.copy()\n",
        "df_result[\"bert_precision\"] = P.tolist()\n",
        "df_result[\"bert_recall\"] = R.tolist()\n",
        "df_result[\"bert_f1\"] = F1.tolist()\n",
        "\n",
        "# ===============================\n",
        "# 6. Lưu file\n",
        "# ===============================\n",
        "output_csv = \"eval_result/base_qwen2_3b_bertscore.csv\"\n",
        "df_result.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"\\n🎉 Done! Saved BERTScore to {output_csv}\")\n",
        "print(f\"Average BERTScore F1 = {df_result['bert_f1'].mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7qgWeAJcTGY",
      "metadata": {
        "id": "e7qgWeAJcTGY"
      },
      "source": [
        "### Qwen2-3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Civ7sNnGRv55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467,
          "referenced_widgets": [
            "b8709d28191b469a8d5b8fab479bb364",
            "dc87772e5d7848b99ab1ebf75edd2612",
            "2c29805fd1094301a167189fde2cf244",
            "7928d7606596458181030d323ad3abce",
            "9417557d0ca04c048a9a2e14bad9a890",
            "fb4b9243ce5a4d8db6873ae8a5f90e54",
            "b04e5ceaf22741da84badf64393864f1",
            "75503fc761f14aecb52dbb740e519f85",
            "3fbe15b086a340c1887e6550f4cc31df",
            "7b3bedfb1d4f4224b2f8b69b55a689fd",
            "b8e405c0d1144608b94514dc9b70585d",
            "75467439ce9648a1b98b924dab837f9c",
            "b62a76fc88514fb7843d7f732a6e1a1b",
            "f90a5438241f4a498e08e7d35be195ef",
            "69a6ae7f34a24b1587e6ce7d384db1d0",
            "d0eafe4a35f54d0fbb3ef7cd8e87b7d5",
            "e84de7a4b20c45cf962c29b135b1aee9",
            "bea47811ae534b2aad7aeeea46cad61c",
            "f2e65cd458434424896badf9a4ddf448",
            "c1eb019363454812988a06ee1f009044",
            "5456f763206549a19d72b1d2e4dc3402",
            "c6c18efd956c434083a23c8a1bf92ad3",
            "881675cc267545d0b2de7b300e4c572d",
            "9e6f852a76dd428681b3e49564c1289b",
            "3b7e531d43e241cc9976d66b40f424f3",
            "e9f12c7e19eb4901a27c15028e90ee8b",
            "071edd89644542459fb919039f198360",
            "8109bce9787c42e2ac461a661bbce9e0",
            "b1c28cdf89bc45faa0ab13513aef81ce",
            "7a676307472a49e691dc4d1e85f6bdfb",
            "f7762a5aae36413694714d79b1e73442",
            "d65bf7d1c85944f8af37fa7e0c232419",
            "e938d5409f6443218ccf350fee5a3587",
            "4c31647eba1a4da6b3e7a569bb7666d1",
            "66ea5e2852f647cb8228ba76e9307d25",
            "361f4a5861734a20beaeb7fc3f7892c4",
            "f9e60c4a04864bb9bf270e75dc68b2ae",
            "8de663c91e4b4a498abc1dc80a6a78cd",
            "054caeb02b0a4b3aa7c96bc7943c3269",
            "b21c581a84674e088201502a30f91ea1",
            "091d27fdc26d4a6691ff2e1874bb2e91",
            "198995d834b34f89be4e9c453a091172",
            "9d63fc949c9443568caeec63b5606f38",
            "6d8496e6045d45d297c7550b5a86e949",
            "939dc75a99064db0aa2d164457a88cf7",
            "522041bc0081443f8862ff6648bcc68a",
            "ac01f4106398437986c28a5aebb9f6d2",
            "3b88998df4204d06b0af68c8c3a02e16",
            "a17a44146974428ba27dff77d74773b5",
            "cb62faa616294ba5880c1c83daac3ccd",
            "1914094976044017a9575713a0f591fa",
            "de2c1ca249fa448c9a9f7df9ec1ead78",
            "d087635be115419b899d9336db4e0f69",
            "93054a5c135b45178cfbaa3706015785",
            "f0b7e9079b5640aeba6b6c30696a97f7",
            "b72b54982dc14b519eca100f5de93aae",
            "f3a33c2382d24382b2f445243cf0bc93",
            "2070f65496814228b91505bca22a40e6",
            "83e140c096e04f3b9eb9b0fcefcd68d7",
            "6ac3e4b106884cdeb5d950e07a1604bc",
            "3b4bc11d246648669e1fd3be7327555c",
            "330cef4fc39149cebf675acbb0411e15",
            "6054fc43e8b146598c24c3ce30fdfd4a",
            "00ae88b453c0482aa042bacd09eaa251",
            "82fd749cfac548a4a4691dd82b9d7437",
            "d98923453b734a749d3dcb1a000409c2",
            "908dd1fa8ee44b3ebc0e0218e0ff764a",
            "673753e5955b48a5bb4d041a5d06e63a",
            "1154678a17a14e8390346c2659dca857",
            "2753dee5dd254ace852251a9ab8bc0d5",
            "6cddca2465c249a2b26a73dddddee383",
            "c7fb49789a01479e87d946d4e160f61f",
            "0e4574d3f3b64f2a984838a9dc869a94",
            "e7a6bba068f04708a8a17e2b6c1809ff",
            "64d46af1bef5423f80704f9e32bebb8c",
            "a163556d8efd4daea4fecc03bc40428d",
            "3f106e58e3954af1a142d9496de46d5a"
          ]
        },
        "id": "Civ7sNnGRv55",
        "outputId": "e7ce94cd-1a10-4100-b234-c3286c0eda2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8709d28191b469a8d5b8fab479bb364",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75467439ce9648a1b98b924dab837f9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "881675cc267545d0b2de7b300e4c572d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c31647eba1a4da6b3e7a569bb7666d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "939dc75a99064db0aa2d164457a88cf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b72b54982dc14b519eca100f5de93aae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "908dd1fa8ee44b3ebc0e0218e0ff764a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 8.37 seconds, 35.86 sentences/sec\n",
            "\n",
            "🎉 Done! Saved BERTScore to eval_result/qwen2_3b_bertscore.csv\n",
            "Average BERTScore F1 = 0.9323\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from bert_score import score\n",
        "\n",
        "# ===============================\n",
        "# 1. Load data\n",
        "# ===============================\n",
        "df_pred = pd.read_csv(\"data/qwen2-3b_eval_data.csv\")\n",
        "df_ref  = pd.read_csv(\"data/merged_all.csv\")\n",
        "\n",
        "# ===============================\n",
        "# 2. Lấy NUM_ROWS reference làm ground truth\n",
        "# ===============================\n",
        "NUM_ROWS = 300\n",
        "SEED = 42\n",
        "\n",
        "df_ref_sample = df_ref.sample(n=NUM_ROWS, random_state=SEED)\n",
        "df_pred_slice = df_pred.iloc[:NUM_ROWS]\n",
        "\n",
        "assert len(df_pred_slice) == len(df_ref_sample)\n",
        "\n",
        "predictions = df_pred_slice[\"response\"].astype(str).tolist()\n",
        "references  = df_ref_sample[\"response\"].astype(str).tolist()\n",
        "\n",
        "# ===============================\n",
        "# 4. Tính BERTScore cho tiếng Việt\n",
        "# ===============================\n",
        "P, R, F1 = score(\n",
        "    predictions,\n",
        "    references,\n",
        "    lang=\"vi\",                   # tokenizer cho tiếng Việt\n",
        "    model_type=\"xlm-roberta-large\",  # embedder tốt nhất cho tiếng Việt\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 5. Gộp vào DataFrame kết quả\n",
        "# ===============================\n",
        "df_result = df_pred_slice.copy()\n",
        "df_result[\"bert_precision\"] = P.tolist()\n",
        "df_result[\"bert_recall\"] = R.tolist()\n",
        "df_result[\"bert_f1\"] = F1.tolist()\n",
        "\n",
        "# ===============================\n",
        "# 6. Lưu file\n",
        "# ===============================\n",
        "output_csv = \"eval_result/qwen2_3b_bertscore.csv\"\n",
        "df_result.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"\\n🎉 Done! Saved BERTScore to {output_csv}\")\n",
        "print(f\"Average BERTScore F1 = {df_result['bert_f1'].mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h-Hb9zqxcVXd",
      "metadata": {
        "id": "h-Hb9zqxcVXd"
      },
      "source": [
        "### Llama 3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6I3xCeO3bRPC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203,
          "referenced_widgets": [
            "9568336b9ae74caab914f832741217a4",
            "837959fa83024865a41c03b1a93a2443",
            "56eb974edf9342bea0e94aa60c65a6f1",
            "9f5512648c1a45a693001e60d635bb28",
            "5ff69c5014ea4496b12a10ba6d83f971",
            "a8fb045db80f4289b04c72dcb5595e8a",
            "2211a39ad09f4e1b84d59edf670e47ff",
            "dd38441ec4da4f999dbfa9be01e64262",
            "12a920410811414fa19372eabb16eecd",
            "c13203382d6c49c5b565d147b2f8374f",
            "ccf875d9eec44e888ca54f3f7dd4da2a",
            "a016768197e841b092f7d43c10938690",
            "7cbb6d7eb1754cd9a12d06b99c4cf087",
            "50ef3f7e86b3490ab109b62e015262c6",
            "f3d1a444a0724fd4a8ff3ca998e5775d",
            "38a0e06219ca4098b718d6318a9915ba",
            "04303a92657e47dfb501707075fdcb8b",
            "3716bdde1a1343f3a955288bce3d5754",
            "55687145083f4fa1bf2d830f160fbef6",
            "c0ebe50012ab45539807af56d105811a",
            "5653be52a5554668bd4ebec680555a4a",
            "4e75906132354b2e80d50b071ce27ffb"
          ]
        },
        "id": "6I3xCeO3bRPC",
        "outputId": "a1b844f6-79b5-4252-8042-35bf5eefbce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9568336b9ae74caab914f832741217a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a016768197e841b092f7d43c10938690",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 20.13 seconds, 14.90 sentences/sec\n",
            "\n",
            "🎉 Done! Saved BERTScore to eval_result/llama_3b_bertscore.csv\n",
            "Average BERTScore F1 = 0.8973\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from bert_score import score\n",
        "\n",
        "# ===============================\n",
        "# 1. Load data\n",
        "# ===============================\n",
        "df_pred = pd.read_csv(\"data/llama_3b_eval_data.csv\")\n",
        "df_ref  = pd.read_csv(\"data/merged_all.csv\")\n",
        "\n",
        "# ===============================\n",
        "# 2. Lấy NUM_ROWS reference làm ground truth\n",
        "# ===============================\n",
        "NUM_ROWS = 300\n",
        "SEED = 42\n",
        "\n",
        "df_ref_sample = df_ref.sample(n=NUM_ROWS, random_state=SEED)\n",
        "df_pred_slice = df_pred.iloc[:NUM_ROWS]\n",
        "\n",
        "assert len(df_pred_slice) == len(df_ref_sample)\n",
        "\n",
        "predictions = df_pred_slice[\"response\"].astype(str).tolist()\n",
        "references  = df_ref_sample[\"response\"].astype(str).tolist()\n",
        "\n",
        "# ===============================\n",
        "# 4. Tính BERTScore cho tiếng Việt\n",
        "# ===============================\n",
        "P, R, F1 = score(\n",
        "    predictions,\n",
        "    references,\n",
        "    lang=\"vi\",                   # tokenizer cho tiếng Việt\n",
        "    model_type=\"xlm-roberta-large\",  # embedder tốt nhất cho tiếng Việt\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 5. Gộp vào DataFrame kết quả\n",
        "# ===============================\n",
        "df_result = df_pred_slice.copy()\n",
        "df_result[\"bert_precision\"] = P.tolist()\n",
        "df_result[\"bert_recall\"] = R.tolist()\n",
        "df_result[\"bert_f1\"] = F1.tolist()\n",
        "\n",
        "# ===============================\n",
        "# 6. Lưu file\n",
        "# ===============================\n",
        "output_csv = \"eval_result/llama_3b_bertscore.csv\"\n",
        "df_result.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"\\n🎉 Done! Saved BERTScore to {output_csv}\")\n",
        "print(f\"Average BERTScore F1 = {df_result['bert_f1'].mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83VLsIPUdHXq",
      "metadata": {
        "id": "83VLsIPUdHXq"
      },
      "source": [
        "### Qwen2 7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x15QkS9AdCoe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203,
          "referenced_widgets": [
            "df4d893372e74658af4b08748809f476",
            "c7d3d8d34e04437f8e899dc2f112f19e",
            "6f56ab5a26444c82958421803b861795",
            "722ed9a0447443d39d5c17e30b496288",
            "d472b588a6f2431a8cb6ea64e62200b6",
            "7b16dde3fc784c7baa0b97b9b8c35a74",
            "f8196072fea244b89690d2bed3f59e3c",
            "a7d3a1cce57c4caa822a1e5420b8a8e3",
            "dd0c87c8c53e43aeb3628729a0ab1ae2",
            "412ee84c3c804576bf93da3708bfdc0d",
            "18a7a38fcf2c4d85ad8f0abb011bb641",
            "71e871bebffa4340b00738c52f64d28a",
            "cdc37dba25ab4013a8a26538437c5d0b",
            "e79bd1e3d8b8483d8b995b4163cddf14",
            "7b1d0af7e60e41249bd20205c82f1e42",
            "a6e6b64c20d5430c925c4cb49ce3040b",
            "a785249d63424f559d4b294817ee0a88",
            "ef0b523d646e4c9b920076cdd80fd83f",
            "562528df95ea4332a8a072fac8dfb6c0",
            "b2d8968ffea9457c90a6e189a5785a3a",
            "34c769be5f4e47a989e1646d6da9e893",
            "ec19693e0b6d40a687713664d921e22a"
          ]
        },
        "id": "x15QkS9AdCoe",
        "outputId": "f1a036fe-cfc1-4914-8542-c9ecca775f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df4d893372e74658af4b08748809f476",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71e871bebffa4340b00738c52f64d28a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 8.57 seconds, 34.99 sentences/sec\n",
            "\n",
            "🎉 Done! Saved BERTScore to eval_result/qwen2_7b_bertscore.csv\n",
            "Average BERTScore F1 = 0.9462\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from bert_score import score\n",
        "\n",
        "# ===============================\n",
        "# 1. Load data\n",
        "# ===============================\n",
        "df_pred = pd.read_csv(\"data/qwen2_7b_eval_data.csv\")\n",
        "df_ref  = pd.read_csv(\"data/merged_all.csv\")\n",
        "\n",
        "# ===============================\n",
        "# 2. Lấy NUM_ROWS reference làm ground truth\n",
        "# ===============================\n",
        "NUM_ROWS = 300\n",
        "SEED = 42\n",
        "\n",
        "df_ref_sample = df_ref.sample(n=NUM_ROWS, random_state=SEED)\n",
        "df_pred_slice = df_pred.iloc[:NUM_ROWS]\n",
        "\n",
        "assert len(df_pred_slice) == len(df_ref_sample)\n",
        "\n",
        "predictions = df_pred_slice[\"response\"].astype(str).tolist()\n",
        "references  = df_ref_sample[\"response\"].astype(str).tolist()\n",
        "\n",
        "# ===============================\n",
        "# 4. Tính BERTScore cho tiếng Việt\n",
        "# ===============================\n",
        "P, R, F1 = score(\n",
        "    predictions,\n",
        "    references,\n",
        "    lang=\"vi\",                   # tokenizer cho tiếng Việt\n",
        "    model_type=\"xlm-roberta-large\",  # embedder tốt nhất cho tiếng Việt\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 5. Gộp vào DataFrame kết quả\n",
        "# ===============================\n",
        "df_result = df_pred_slice.copy()\n",
        "df_result[\"bert_precision\"] = P.tolist()\n",
        "df_result[\"bert_recall\"] = R.tolist()\n",
        "df_result[\"bert_f1\"] = F1.tolist()\n",
        "\n",
        "# ===============================\n",
        "# 6. Lưu file\n",
        "# ===============================\n",
        "output_csv = \"eval_result/qwen2_7b_bertscore.csv\"\n",
        "df_result.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"\\n🎉 Done! Saved BERTScore to {output_csv}\")\n",
        "print(f\"Average BERTScore F1 = {df_result['bert_f1'].mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b91a7cf",
      "metadata": {},
      "source": [
        "### Bertscore Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "NYCrJnBWdR6Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYCrJnBWdR6Q",
        "outputId": "461666a9-e967-434f-b4cf-05f692d826c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Mean BERTScore for base-qwen2-3b ===\n",
            "bert_precision : 0.8532\n",
            "bert_recall    : 0.9206\n",
            "bert_f1        : 0.8845\n",
            "\n",
            "=== Mean BERTScore for qwen2-3b ===\n",
            "bert_precision : 0.9159\n",
            "bert_recall    : 0.9503\n",
            "bert_f1        : 0.9323\n",
            "\n",
            "=== Mean BERTScore for llama-3b ===\n",
            "bert_precision : 0.8575\n",
            "bert_recall    : 0.9438\n",
            "bert_f1        : 0.8973\n",
            "\n",
            "=== Mean BERTScore for qwen2-7b ===\n",
            "bert_precision : 0.9350\n",
            "bert_recall    : 0.9588\n",
            "bert_f1        : 0.9462\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Danh sách file\n",
        "# -------------------------------\n",
        "files = {\n",
        "    \"base-qwen2-3b\": \"eval_result/base_qwen2_3b_bertscore.csv\",\n",
        "    \"qwen2-3b\": \"eval_result/qwen2_3b_bertscore.csv\",\n",
        "    \"llama-3b\": \"eval_result/llama_3b_bertscore.csv\",\n",
        "    \"qwen2-7b\": \"eval_result/qwen2_7b_bertscore.csv\"\n",
        "}\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Thống kê mean từng file\n",
        "# -------------------------------\n",
        "for name, path in files.items():\n",
        "    print(f\"\\n=== Mean BERTScore for {name} ===\")\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    cols = [\"bert_precision\", \"bert_recall\", \"bert_f1\"]\n",
        "\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            mean_val = df[col].mean()\n",
        "            print(f\"{col:15s}: {mean_val:.4f}\")\n",
        "        else:\n",
        "            print(f\"{col:15s}: column not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f-Z53cBiKraP",
      "metadata": {
        "id": "f-Z53cBiKraP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "K4hTE-oSoCEi",
        "MPpC57BbqaTt",
        "WBnjsESbFMeO",
        "awG38Pnc0vor",
        "XQE0hyZS0ykh",
        "h-Hb9zqxcVXd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00ae88b453c0482aa042bacd09eaa251": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "019ca64cab054a4baf6ce0e664dc7771": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4afa5bd94c44662b2a87c67af992878",
            "placeholder": "​",
            "style": "IPY_MODEL_77fa7622493345518cf8d6619c105333",
            "value": " 616/616 [00:00&lt;00:00, 18.2kB/s]"
          }
        },
        "031dad1ec22841f796165c9670d32864": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a9b030e13249e29fa6ec3d23d3a931": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04303a92657e47dfb501707075fdcb8b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "054caeb02b0a4b3aa7c96bc7943c3269": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05e7ad9f5cbf46fcadf32a797e553f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "070c1438579f47c1b4008cdb1fb71aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "071edd89644542459fb919039f198360": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f9d8ecc2a64f50ae14ba19f79a6dad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091d27fdc26d4a6691ff2e1874bb2e91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bef3b85ad5744d9bf8a1f7a1299f2fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c17157a36ae415f809a19d473879941": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e4574d3f3b64f2a984838a9dc869a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f927bc19e544269984bb0a4ab34ae4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10c5ed81dd5d4a8e9c08095765cc5317": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_defd07e6a8ec4c3a918c61e90fc54af6",
            "placeholder": "​",
            "style": "IPY_MODEL_b97d1d661f3545deb58b934ecbda97eb",
            "value": "100%"
          }
        },
        "1154678a17a14e8390346c2659dca857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7a6bba068f04708a8a17e2b6c1809ff",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64d46af1bef5423f80704f9e32bebb8c",
            "value": 5
          }
        },
        "12a920410811414fa19372eabb16eecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "137e63b93f644eca99ebf661a4ac72b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1463a48503694647a8eaa51e2085c30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d774b2b026ae4647be08c27195810795",
              "IPY_MODEL_25371e32a2e041baba8779aa2e08d89d",
              "IPY_MODEL_87f306ea03354ebc8e15a535085e298e"
            ],
            "layout": "IPY_MODEL_db090e3545a44dea8c7201098c3f086d"
          }
        },
        "15b3b2bd17ed43b889eacbe47cbfb00f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1632a8138e274da194399eb843817e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a7a38fcf2c4d85ad8f0abb011bb641": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18c9cfd289e8497fba9a74deea668036": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdd0d9f4b8654a0bb1f17591e872bf81",
            "placeholder": "​",
            "style": "IPY_MODEL_0f927bc19e544269984bb0a4ab34ae4b",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "1914094976044017a9575713a0f591fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "198995d834b34f89be4e9c453a091172": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b6399de21d449f9aad5df3c9af038f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2061cca575ef4c6986f8a29d3f63aa78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a6ac1e062f94b82b5945080e41d5453",
            "placeholder": "​",
            "style": "IPY_MODEL_af008636438741c396156c7db8e6d4e8",
            "value": " 10/10 [30:18&lt;00:00, 119.36s/it]"
          }
        },
        "2070f65496814228b91505bca22a40e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6054fc43e8b146598c24c3ce30fdfd4a",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00ae88b453c0482aa042bacd09eaa251",
            "value": 9
          }
        },
        "2211a39ad09f4e1b84d59edf670e47ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25371e32a2e041baba8779aa2e08d89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05e7ad9f5cbf46fcadf32a797e553f2e",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e696a2df9be548108ca15970fbf9e1ff",
            "value": 25
          }
        },
        "26d7de360eb24798b025ba150ea82c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c17157a36ae415f809a19d473879941",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_070c1438579f47c1b4008cdb1fb71aca",
            "value": 10
          }
        },
        "2753dee5dd254ace852251a9ab8bc0d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a163556d8efd4daea4fecc03bc40428d",
            "placeholder": "​",
            "style": "IPY_MODEL_3f106e58e3954af1a142d9496de46d5a",
            "value": " 5/5 [00:00&lt;00:00,  4.54it/s]"
          }
        },
        "294fba1b8e574542a18f0ed1a2cecc54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b3a0664144d4c4cb90b64c17e589693": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72bf88c7ea884767aecedd4d5b37b857",
            "placeholder": "​",
            "style": "IPY_MODEL_9ea4ac4816df49018fa1c8919799bede",
            "value": "model.safetensors: 100%"
          }
        },
        "2c29805fd1094301a167189fde2cf244": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75503fc761f14aecb52dbb740e519f85",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3fbe15b086a340c1887e6550f4cc31df",
            "value": 25
          }
        },
        "2efd13ab441644a2bc07d4b924fa8f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30eef6f3b6944a589d2bec1a2e740ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_942627f951a54cf4b935ea58cf8df7bd",
              "IPY_MODEL_d889a3a99da342ddb87fc30f52d893ec",
              "IPY_MODEL_019ca64cab054a4baf6ce0e664dc7771"
            ],
            "layout": "IPY_MODEL_fcfaa4a79bcb458998b534e4a78e8332"
          }
        },
        "31e0d65e2f2a424ea8718d2e75840755": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03a9b030e13249e29fa6ec3d23d3a931",
            "placeholder": "​",
            "style": "IPY_MODEL_6c3050b1ba074376804d9dd2a0261874",
            "value": " 5/5 [00:02&lt;00:00,  2.04it/s]"
          }
        },
        "321f080a058e4a58863fa16f30096854": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "330cef4fc39149cebf675acbb0411e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33bd387793b5426089c805e3518b4e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c02ef874f87478dbbe2e970eb5e44bf",
            "placeholder": "​",
            "style": "IPY_MODEL_321f080a058e4a58863fa16f30096854",
            "value": " 9.10M/9.10M [00:00&lt;00:00, 16.0MB/s]"
          }
        },
        "34c769be5f4e47a989e1646d6da9e893": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "361f4a5861734a20beaeb7fc3f7892c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_091d27fdc26d4a6691ff2e1874bb2e91",
            "max": 9096718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_198995d834b34f89be4e9c453a091172",
            "value": 9096718
          }
        },
        "3716bdde1a1343f3a955288bce3d5754": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38a0e06219ca4098b718d6318a9915ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b4bc11d246648669e1fd3be7327555c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b7e531d43e241cc9976d66b40f424f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a676307472a49e691dc4d1e85f6bdfb",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7762a5aae36413694714d79b1e73442",
            "value": 5069051
          }
        },
        "3b88998df4204d06b0af68c8c3a02e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93054a5c135b45178cfbaa3706015785",
            "placeholder": "​",
            "style": "IPY_MODEL_f0b7e9079b5640aeba6b6c30696a97f7",
            "value": " 2.24G/2.24G [00:21&lt;00:00, 150MB/s]"
          }
        },
        "3f106e58e3954af1a142d9496de46d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fbe15b086a340c1887e6550f4cc31df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "412ee84c3c804576bf93da3708bfdc0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44af5dbcadf9416683cd7f12492281c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "452a1efbb1d04c72921c82243dc77f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5809fba40e504616b7e8f99b754ef971",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bef3b85ad5744d9bf8a1f7a1299f2fc",
            "value": 2
          }
        },
        "4c31647eba1a4da6b3e7a569bb7666d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66ea5e2852f647cb8228ba76e9307d25",
              "IPY_MODEL_361f4a5861734a20beaeb7fc3f7892c4",
              "IPY_MODEL_f9e60c4a04864bb9bf270e75dc68b2ae"
            ],
            "layout": "IPY_MODEL_8de663c91e4b4a498abc1dc80a6a78cd"
          }
        },
        "4e75906132354b2e80d50b071ce27ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50ef3f7e86b3490ab109b62e015262c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55687145083f4fa1bf2d830f160fbef6",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0ebe50012ab45539807af56d105811a",
            "value": 5
          }
        },
        "5108ee605774494f9662cb1d9524b74e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "511f74ff2c37450f927f3fb0f313ff43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "522041bc0081443f8862ff6648bcc68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb62faa616294ba5880c1c83daac3ccd",
            "placeholder": "​",
            "style": "IPY_MODEL_1914094976044017a9575713a0f591fa",
            "value": "model.safetensors: 100%"
          }
        },
        "5456f763206549a19d72b1d2e4dc3402": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55687145083f4fa1bf2d830f160fbef6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "561956ed74ca4ade9f4b9bab4bbaccaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562528df95ea4332a8a072fac8dfb6c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5653be52a5554668bd4ebec680555a4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56eb974edf9342bea0e94aa60c65a6f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd38441ec4da4f999dbfa9be01e64262",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12a920410811414fa19372eabb16eecd",
            "value": 10
          }
        },
        "57f133c6816c48629e7b90ab05022c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf36c55f5f4e4bfc9a9d122574d5fdcd",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73d131fc8c69477d8fbeb807b4393ff6",
            "value": 5069051
          }
        },
        "5809fba40e504616b7e8f99b754ef971": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ff69c5014ea4496b12a10ba6d83f971": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6054fc43e8b146598c24c3ce30fdfd4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6064e3bc966941c2b0e82bb4070bfc38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64699e7eb882414291f9d71badda1eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5108ee605774494f9662cb1d9524b74e",
            "placeholder": "​",
            "style": "IPY_MODEL_708f03df501b4ef2a23651e62eaff394",
            "value": "tokenizer.json: 100%"
          }
        },
        "64d46af1bef5423f80704f9e32bebb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66ea5e2852f647cb8228ba76e9307d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_054caeb02b0a4b3aa7c96bc7943c3269",
            "placeholder": "​",
            "style": "IPY_MODEL_b21c581a84674e088201502a30f91ea1",
            "value": "tokenizer.json: 100%"
          }
        },
        "673753e5955b48a5bb4d041a5d06e63a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7fb49789a01479e87d946d4e160f61f",
            "placeholder": "​",
            "style": "IPY_MODEL_0e4574d3f3b64f2a984838a9dc869a94",
            "value": "100%"
          }
        },
        "69a6ae7f34a24b1587e6ce7d384db1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5456f763206549a19d72b1d2e4dc3402",
            "placeholder": "​",
            "style": "IPY_MODEL_c6c18efd956c434083a23c8a1bf92ad3",
            "value": " 616/616 [00:00&lt;00:00, 18.5kB/s]"
          }
        },
        "6ac3e4b106884cdeb5d950e07a1604bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba606be46504efd93873625c0dd4e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c02ef874f87478dbbe2e970eb5e44bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c3050b1ba074376804d9dd2a0261874": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cddca2465c249a2b26a73dddddee383": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8496e6045d45d297c7550b5a86e949": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f56ab5a26444c82958421803b861795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7d3a1cce57c4caa822a1e5420b8a8e3",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd0c87c8c53e43aeb3628729a0ab1ae2",
            "value": 9
          }
        },
        "708f03df501b4ef2a23651e62eaff394": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71b04889d3704d0b8733c7f1573a8a01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71e871bebffa4340b00738c52f64d28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdc37dba25ab4013a8a26538437c5d0b",
              "IPY_MODEL_e79bd1e3d8b8483d8b995b4163cddf14",
              "IPY_MODEL_7b1d0af7e60e41249bd20205c82f1e42"
            ],
            "layout": "IPY_MODEL_a6e6b64c20d5430c925c4cb49ce3040b"
          }
        },
        "722ed9a0447443d39d5c17e30b496288": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_412ee84c3c804576bf93da3708bfdc0d",
            "placeholder": "​",
            "style": "IPY_MODEL_18a7a38fcf2c4d85ad8f0abb011bb641",
            "value": " 9/9 [00:08&lt;00:00,  2.14it/s]"
          }
        },
        "72bf88c7ea884767aecedd4d5b37b857": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73bc661b9af24612b3a042930596b5b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d131fc8c69477d8fbeb807b4393ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73f88a5bf02541beb0d564169e4858c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2efd13ab441644a2bc07d4b924fa8f1b",
            "max": 9096718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8f45f5fa5f54277823ed957717618b2",
            "value": 9096718
          }
        },
        "75467439ce9648a1b98b924dab837f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b62a76fc88514fb7843d7f732a6e1a1b",
              "IPY_MODEL_f90a5438241f4a498e08e7d35be195ef",
              "IPY_MODEL_69a6ae7f34a24b1587e6ce7d384db1d0"
            ],
            "layout": "IPY_MODEL_d0eafe4a35f54d0fbb3ef7cd8e87b7d5"
          }
        },
        "75503fc761f14aecb52dbb740e519f85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76e2d531a1b74123a18aec2197a1dade": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10c5ed81dd5d4a8e9c08095765cc5317",
              "IPY_MODEL_26d7de360eb24798b025ba150ea82c6f",
              "IPY_MODEL_2061cca575ef4c6986f8a29d3f63aa78"
            ],
            "layout": "IPY_MODEL_1632a8138e274da194399eb843817e8d"
          }
        },
        "77fa7622493345518cf8d6619c105333": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7928d7606596458181030d323ad3abce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b3bedfb1d4f4224b2f8b69b55a689fd",
            "placeholder": "​",
            "style": "IPY_MODEL_b8e405c0d1144608b94514dc9b70585d",
            "value": " 25.0/25.0 [00:00&lt;00:00, 470B/s]"
          }
        },
        "7986a068cf7f4047a05b29607364946d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a676307472a49e691dc4d1e85f6bdfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b16dde3fc784c7baa0b97b9b8c35a74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b1d0af7e60e41249bd20205c82f1e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34c769be5f4e47a989e1646d6da9e893",
            "placeholder": "​",
            "style": "IPY_MODEL_ec19693e0b6d40a687713664d921e22a",
            "value": " 5/5 [00:00&lt;00:00, 32.51it/s]"
          }
        },
        "7b3bedfb1d4f4224b2f8b69b55a689fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cbb6d7eb1754cd9a12d06b99c4cf087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04303a92657e47dfb501707075fdcb8b",
            "placeholder": "​",
            "style": "IPY_MODEL_3716bdde1a1343f3a955288bce3d5754",
            "value": "100%"
          }
        },
        "7f813c407dfb4ee89dfe79ec99384da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73bc661b9af24612b3a042930596b5b4",
            "placeholder": "​",
            "style": "IPY_MODEL_1b6399de21d449f9aad5df3c9af038f8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8109bce9787c42e2ac461a661bbce9e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82fd749cfac548a4a4691dd82b9d7437": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "837959fa83024865a41c03b1a93a2443": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8fb045db80f4289b04c72dcb5595e8a",
            "placeholder": "​",
            "style": "IPY_MODEL_2211a39ad09f4e1b84d59edf670e47ff",
            "value": "100%"
          }
        },
        "83e140c096e04f3b9eb9b0fcefcd68d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82fd749cfac548a4a4691dd82b9d7437",
            "placeholder": "​",
            "style": "IPY_MODEL_d98923453b734a749d3dcb1a000409c2",
            "value": " 9/9 [00:08&lt;00:00,  1.86it/s]"
          }
        },
        "87f306ea03354ebc8e15a535085e298e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_561956ed74ca4ade9f4b9bab4bbaccaa",
            "placeholder": "​",
            "style": "IPY_MODEL_93846f875794433cbaac228c504f79ce",
            "value": " 25.0/25.0 [00:00&lt;00:00, 644B/s]"
          }
        },
        "881675cc267545d0b2de7b300e4c572d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e6f852a76dd428681b3e49564c1289b",
              "IPY_MODEL_3b7e531d43e241cc9976d66b40f424f3",
              "IPY_MODEL_e9f12c7e19eb4901a27c15028e90ee8b"
            ],
            "layout": "IPY_MODEL_071edd89644542459fb919039f198360"
          }
        },
        "8de663c91e4b4a498abc1dc80a6a78cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "908dd1fa8ee44b3ebc0e0218e0ff764a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_673753e5955b48a5bb4d041a5d06e63a",
              "IPY_MODEL_1154678a17a14e8390346c2659dca857",
              "IPY_MODEL_2753dee5dd254ace852251a9ab8bc0d5"
            ],
            "layout": "IPY_MODEL_6cddca2465c249a2b26a73dddddee383"
          }
        },
        "93054a5c135b45178cfbaa3706015785": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93846f875794433cbaac228c504f79ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "939dc75a99064db0aa2d164457a88cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_522041bc0081443f8862ff6648bcc68a",
              "IPY_MODEL_ac01f4106398437986c28a5aebb9f6d2",
              "IPY_MODEL_3b88998df4204d06b0af68c8c3a02e16"
            ],
            "layout": "IPY_MODEL_a17a44146974428ba27dff77d74773b5"
          }
        },
        "9417557d0ca04c048a9a2e14bad9a890": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "942627f951a54cf4b935ea58cf8df7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1cc85f02f874c6da5734ddfb91c2ddc",
            "placeholder": "​",
            "style": "IPY_MODEL_7986a068cf7f4047a05b29607364946d",
            "value": "config.json: 100%"
          }
        },
        "952be771f51f45888b57ad600e061aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9568336b9ae74caab914f832741217a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_837959fa83024865a41c03b1a93a2443",
              "IPY_MODEL_56eb974edf9342bea0e94aa60c65a6f1",
              "IPY_MODEL_9f5512648c1a45a693001e60d635bb28"
            ],
            "layout": "IPY_MODEL_5ff69c5014ea4496b12a10ba6d83f971"
          }
        },
        "95844ba924e6411b848700ba2d509926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f813c407dfb4ee89dfe79ec99384da4",
              "IPY_MODEL_452a1efbb1d04c72921c82243dc77f83",
              "IPY_MODEL_af8f8b1513184ab7aba07d347cfdb706"
            ],
            "layout": "IPY_MODEL_137e63b93f644eca99ebf661a4ac72b1"
          }
        },
        "970833c6c4b14cc3a945efcd7c79231a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a6ac1e062f94b82b5945080e41d5453": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d63fc949c9443568caeec63b5606f38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6f852a76dd428681b3e49564c1289b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8109bce9787c42e2ac461a661bbce9e0",
            "placeholder": "​",
            "style": "IPY_MODEL_b1c28cdf89bc45faa0ab13513aef81ce",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "9ea4ac4816df49018fa1c8919799bede": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f5512648c1a45a693001e60d635bb28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c13203382d6c49c5b565d147b2f8374f",
            "placeholder": "​",
            "style": "IPY_MODEL_ccf875d9eec44e888ca54f3f7dd4da2a",
            "value": " 10/10 [00:19&lt;00:00,  1.01it/s]"
          }
        },
        "a016768197e841b092f7d43c10938690": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7cbb6d7eb1754cd9a12d06b99c4cf087",
              "IPY_MODEL_50ef3f7e86b3490ab109b62e015262c6",
              "IPY_MODEL_f3d1a444a0724fd4a8ff3ca998e5775d"
            ],
            "layout": "IPY_MODEL_38a0e06219ca4098b718d6318a9915ba"
          }
        },
        "a163556d8efd4daea4fecc03bc40428d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17a44146974428ba27dff77d74773b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1cc85f02f874c6da5734ddfb91c2ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e6b64c20d5430c925c4cb49ce3040b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a785249d63424f559d4b294817ee0a88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7d3a1cce57c4caa822a1e5420b8a8e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8fb045db80f4289b04c72dcb5595e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac01f4106398437986c28a5aebb9f6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de2c1ca249fa448c9a9f7df9ec1ead78",
            "max": 2244817354,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d087635be115419b899d9336db4e0f69",
            "value": 2244817354
          }
        },
        "ac451a766dc34734b5493d7704db2fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af008636438741c396156c7db8e6d4e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af18bed4a48349b6b77bdfce7f1589fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af8f8b1513184ab7aba07d347cfdb706": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15b3b2bd17ed43b889eacbe47cbfb00f",
            "placeholder": "​",
            "style": "IPY_MODEL_af18bed4a48349b6b77bdfce7f1589fa",
            "value": " 2/2 [00:36&lt;00:00, 17.13s/it]"
          }
        },
        "b0136b148f8f4905af78beae60eceab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b04e5ceaf22741da84badf64393864f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1c28cdf89bc45faa0ab13513aef81ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b21c581a84674e088201502a30f91ea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2d8968ffea9457c90a6e189a5785a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b62a76fc88514fb7843d7f732a6e1a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e84de7a4b20c45cf962c29b135b1aee9",
            "placeholder": "​",
            "style": "IPY_MODEL_bea47811ae534b2aad7aeeea46cad61c",
            "value": "config.json: 100%"
          }
        },
        "b72b54982dc14b519eca100f5de93aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3a33c2382d24382b2f445243cf0bc93",
              "IPY_MODEL_2070f65496814228b91505bca22a40e6",
              "IPY_MODEL_83e140c096e04f3b9eb9b0fcefcd68d7"
            ],
            "layout": "IPY_MODEL_6ac3e4b106884cdeb5d950e07a1604bc"
          }
        },
        "b8709d28191b469a8d5b8fab479bb364": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc87772e5d7848b99ab1ebf75edd2612",
              "IPY_MODEL_2c29805fd1094301a167189fde2cf244",
              "IPY_MODEL_7928d7606596458181030d323ad3abce"
            ],
            "layout": "IPY_MODEL_9417557d0ca04c048a9a2e14bad9a890"
          }
        },
        "b8e405c0d1144608b94514dc9b70585d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b97d1d661f3545deb58b934ecbda97eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bab5fb8694e2403e97c23035040905c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc6dc8eb71fa4be4bf772fb994a2f64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfe00c657b264bcc9d7a636b8c5dc08b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac451a766dc34734b5493d7704db2fd4",
            "value": 5
          }
        },
        "bea47811ae534b2aad7aeeea46cad61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0ebe50012ab45539807af56d105811a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0f3a0ce420a4f1fb9413d27a79cbfbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c13203382d6c49c5b565d147b2f8374f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1eb019363454812988a06ee1f009044": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4afa5bd94c44662b2a87c67af992878": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c685010ae80b435d98f260b5548e8e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6c18efd956c434083a23c8a1bf92ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7d3d8d34e04437f8e899dc2f112f19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b16dde3fc784c7baa0b97b9b8c35a74",
            "placeholder": "​",
            "style": "IPY_MODEL_f8196072fea244b89690d2bed3f59e3c",
            "value": "100%"
          }
        },
        "c7fb49789a01479e87d946d4e160f61f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8f45f5fa5f54277823ed957717618b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cab8a1f9328e466d874e07a5e74780ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfd7ea73e3f3422d937100e7421fe2b4",
            "placeholder": "​",
            "style": "IPY_MODEL_c0f3a0ce420a4f1fb9413d27a79cbfbc",
            "value": " 2.24G/2.24G [00:37&lt;00:00, 170MB/s]"
          }
        },
        "cb62faa616294ba5880c1c83daac3ccd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccf875d9eec44e888ca54f3f7dd4da2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdc37dba25ab4013a8a26538437c5d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a785249d63424f559d4b294817ee0a88",
            "placeholder": "​",
            "style": "IPY_MODEL_ef0b523d646e4c9b920076cdd80fd83f",
            "value": "100%"
          }
        },
        "cf36c55f5f4e4bfc9a9d122574d5fdcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfe00c657b264bcc9d7a636b8c5dc08b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d087635be115419b899d9336db4e0f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0eafe4a35f54d0fbb3ef7cd8e87b7d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d472b588a6f2431a8cb6ea64e62200b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d634bf8fc71a404a89b92c5faa55d4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b3a0664144d4c4cb90b64c17e589693",
              "IPY_MODEL_dd1786696b564e989a6136a6db3a7d1c",
              "IPY_MODEL_cab8a1f9328e466d874e07a5e74780ec"
            ],
            "layout": "IPY_MODEL_07f9d8ecc2a64f50ae14ba19f79a6dad"
          }
        },
        "d65bf7d1c85944f8af37fa7e0c232419": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d774b2b026ae4647be08c27195810795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_952be771f51f45888b57ad600e061aa2",
            "placeholder": "​",
            "style": "IPY_MODEL_6064e3bc966941c2b0e82bb4070bfc38",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d889a3a99da342ddb87fc30f52d893ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee14fce0a3304f48860574e1d102cbbf",
            "max": 616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0136b148f8f4905af78beae60eceab4",
            "value": 616
          }
        },
        "d98923453b734a749d3dcb1a000409c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db090e3545a44dea8c7201098c3f086d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc87772e5d7848b99ab1ebf75edd2612": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb4b9243ce5a4d8db6873ae8a5f90e54",
            "placeholder": "​",
            "style": "IPY_MODEL_b04e5ceaf22741da84badf64393864f1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "dd0c87c8c53e43aeb3628729a0ab1ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd1786696b564e989a6136a6db3a7d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44af5dbcadf9416683cd7f12492281c3",
            "max": 2244817354,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c685010ae80b435d98f260b5548e8e80",
            "value": 2244817354
          }
        },
        "dd38441ec4da4f999dbfa9be01e64262": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de2c1ca249fa448c9a9f7df9ec1ead78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "defd07e6a8ec4c3a918c61e90fc54af6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df4d893372e74658af4b08748809f476": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7d3d8d34e04437f8e899dc2f112f19e",
              "IPY_MODEL_6f56ab5a26444c82958421803b861795",
              "IPY_MODEL_722ed9a0447443d39d5c17e30b496288"
            ],
            "layout": "IPY_MODEL_d472b588a6f2431a8cb6ea64e62200b6"
          }
        },
        "dfd7ea73e3f3422d937100e7421fe2b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e696a2df9be548108ca15970fbf9e1ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7117f45332b46b3888437efd22546ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18c9cfd289e8497fba9a74deea668036",
              "IPY_MODEL_57f133c6816c48629e7b90ab05022c53",
              "IPY_MODEL_f85121d0f5514851b2d783ae628b21a5"
            ],
            "layout": "IPY_MODEL_294fba1b8e574542a18f0ed1a2cecc54"
          }
        },
        "e79bd1e3d8b8483d8b995b4163cddf14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_562528df95ea4332a8a072fac8dfb6c0",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2d8968ffea9457c90a6e189a5785a3a",
            "value": 5
          }
        },
        "e7a6bba068f04708a8a17e2b6c1809ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e84de7a4b20c45cf962c29b135b1aee9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e938d5409f6443218ccf350fee5a3587": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9f12c7e19eb4901a27c15028e90ee8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d65bf7d1c85944f8af37fa7e0c232419",
            "placeholder": "​",
            "style": "IPY_MODEL_e938d5409f6443218ccf350fee5a3587",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 13.5MB/s]"
          }
        },
        "ec19693e0b6d40a687713664d921e22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee14fce0a3304f48860574e1d102cbbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef0b523d646e4c9b920076cdd80fd83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0b7e9079b5640aeba6b6c30696a97f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2e65cd458434424896badf9a4ddf448": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a33c2382d24382b2f445243cf0bc93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b4bc11d246648669e1fd3be7327555c",
            "placeholder": "​",
            "style": "IPY_MODEL_330cef4fc39149cebf675acbb0411e15",
            "value": "100%"
          }
        },
        "f3d1a444a0724fd4a8ff3ca998e5775d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5653be52a5554668bd4ebec680555a4a",
            "placeholder": "​",
            "style": "IPY_MODEL_4e75906132354b2e80d50b071ce27ffb",
            "value": " 5/5 [00:00&lt;00:00, 23.00it/s]"
          }
        },
        "f47baa559deb4d4daf09765c61137253": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64699e7eb882414291f9d71badda1eef",
              "IPY_MODEL_73f88a5bf02541beb0d564169e4858c2",
              "IPY_MODEL_33bd387793b5426089c805e3518b4e19"
            ],
            "layout": "IPY_MODEL_970833c6c4b14cc3a945efcd7c79231a"
          }
        },
        "f7762a5aae36413694714d79b1e73442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8196072fea244b89690d2bed3f59e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f85121d0f5514851b2d783ae628b21a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_511f74ff2c37450f927f3fb0f313ff43",
            "placeholder": "​",
            "style": "IPY_MODEL_bab5fb8694e2403e97c23035040905c3",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 13.0MB/s]"
          }
        },
        "f90a5438241f4a498e08e7d35be195ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e65cd458434424896badf9a4ddf448",
            "max": 616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1eb019363454812988a06ee1f009044",
            "value": 616
          }
        },
        "f9e60c4a04864bb9bf270e75dc68b2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d63fc949c9443568caeec63b5606f38",
            "placeholder": "​",
            "style": "IPY_MODEL_6d8496e6045d45d297c7550b5a86e949",
            "value": " 9.10M/9.10M [00:00&lt;00:00, 20.8MB/s]"
          }
        },
        "fb4b9243ce5a4d8db6873ae8a5f90e54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbae54db36194f76928626a4216f7c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_031dad1ec22841f796165c9670d32864",
            "placeholder": "​",
            "style": "IPY_MODEL_6ba606be46504efd93873625c0dd4e74",
            "value": "100%"
          }
        },
        "fcfaa4a79bcb458998b534e4a78e8332": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd7da4b9cdcf4adeb5e7d4816c3b4097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbae54db36194f76928626a4216f7c18",
              "IPY_MODEL_bc6dc8eb71fa4be4bf772fb994a2f64c",
              "IPY_MODEL_31e0d65e2f2a424ea8718d2e75840755"
            ],
            "layout": "IPY_MODEL_71b04889d3704d0b8733c7f1573a8a01"
          }
        },
        "fdd0d9f4b8654a0bb1f17591e872bf81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
