{"cells":[{"cell_type":"code","execution_count":1,"id":"4df638a0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26808,"status":"ok","timestamp":1764646226867,"user":{"displayName":"Dailymate","userId":"09240786015871031354"},"user_tz":-420},"id":"4df638a0","outputId":"f2819387-99f2-44f9-ef34-8939c877a298"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"id":"yAB_C6g2VSTn","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":848,"status":"ok","timestamp":1764646227719,"user":{"displayName":"Dailymate","userId":"09240786015871031354"},"user_tz":-420},"id":"yAB_C6g2VSTn","outputId":"f7d7eda5-0c6f-4f37-a654-11d68dafddd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/chatbot\n","app_gradio.ipynb  eval_rag_faithfulness_gemini_custom.csv  models\n","data\t\t  eval_result\t\t\t\t   rag_evaluation.ipynb\n","data_generating   faithfulness_results.jsonl\t\t   src\n","data_viz.ipynb\t  model_evaluation.ipynb\t\t   train_qwen2_3b.ipynb\n"]}],"source":["\n","import os\n","import sys\n","\n","# 1️⃣ Chuyển working directory tới root project (nơi có thư mục src)\n","%cd /content/drive/MyDrive/chatbot\n","\n","# 2️⃣ Thêm folder root vào sys.path để Python tìm src\n","sys.path.append(os.getcwd())\n","\n","# 3️⃣ Kiểm tra\n","!ls\n","# Nên thấy thư mục src, app_gradio.py, notebooks, v.v.\n"]},{"cell_type":"code","execution_count":3,"id":"iTKvwrjTtSwA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64024,"status":"ok","timestamp":1764635063838,"user":{"displayName":"Dailymate","userId":"09240786015871031354"},"user_tz":-420},"id":"iTKvwrjTtSwA","outputId":"9b1d8ab8-331c-433a-9b08-df8111e79922"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m2.3/2.5 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/146.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m745.9/745.9 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.9/280.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","langchain-google-genai 0.0.1 requires google-generativeai<0.4.0,>=0.3.1, but you have google-generativeai 0.8.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","langchain-google-genai 0.0.1 requires google-generativeai<0.4.0,>=0.3.1, but you have google-generativeai 0.8.5 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\n","opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install langchain langchain-community langchain-google-genai google-generativeai transformers torch sentence-transformers pinecone gradio peft bitsandbytes accelerate pymongo --quiet\n","!pip install pinecone --upgrade --quiet\n","!pip install -U bitsandbytes --quiet\n","!pip install rank-bm25 unidecode -q\n","!pip install rouge-score bert-score sacrebleu -q\n","# !pip install ragas\n","# !pip install deepeval\n","!pip install --upgrade google-generativeai -q\n","!python -m pip install protobuf==4.25.3 -q"]},{"cell_type":"markdown","id":"K4hTE-oSoCEi","metadata":{"id":"K4hTE-oSoCEi"},"source":["# Make Response"]},{"cell_type":"markdown","id":"OyLr_L05qV5r","metadata":{"id":"OyLr_L05qV5r"},"source":["### Qweb2-3b"]},{"cell_type":"code","execution_count":null,"id":"Fk3hsxXHoBq1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223,"referenced_widgets":["5d4ea5df5b604643a5d5f86f98253b81"]},"executionInfo":{"elapsed":2846179,"status":"ok","timestamp":1764591181896,"user":{"displayName":"Dailymate","userId":"09240786015871031354"},"user_tz":-420},"id":"Fk3hsxXHoBq1","outputId":"0ed443a3-4910-4356-8dd3-475f289f60aa"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1041: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d4ea5df5b604643a5d5f86f98253b81","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n","  warnings.warn(\n","Device set to use cuda:0\n","  3%|▎         | 10/300 [01:33<45:10,  9.35s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","100%|██████████| 300/300 [47:09<00:00,  9.43s/it]"]},{"name":"stdout","output_type":"stream","text":["Saved 300 rows to data/qwen2-3b_eval_data.csv\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import pandas as pd\n","import random\n","from tqdm import tqdm\n","import sys\n","import os\n","\n","\n","from src.logic_module import build_pipeline, bm25_retriever, embedding_model, custom_template, build_context\n","\n","# -------------------- Config --------------------\n","MODEL_KEY = \"qwen2-3b\"\n","INPUT_CSV = \"data/merged_all.csv\"\n","OUTPUT_CSV = f\"data/{MODEL_KEY}_eval_data.csv\"\n","NUM_ROWS = 300       # số dòng muốn chạy\n","SEED = 42\n","MAX_TOKENS = 512\n","\n","# -------------------- Load data --------------------\n","df = pd.read_csv(INPUT_CSV)\n","random.seed(SEED)\n","sampled_rows = df.sample(n=min(NUM_ROWS, len(df)), random_state=SEED)\n","\n","# -------------------- Load model pipeline --------------------\n","# Chạy CPU safe (nếu GPU không đủ VRAM)\n","try:\n","    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n","except Exception as e:\n","    print(\"Error loading pipeline, fallback to CPU\")\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable GPU\n","    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n","\n","# -------------------- Run inference --------------------\n","results = []\n","retrieval_cache = []  # cache phải là list để build_context append được\n","\n","for _, row in tqdm(sampled_rows.iterrows(), total=len(sampled_rows)):\n","    # Giả sử có cột 'text' hoặc 'question'\n","    user_input = row.get(\"text\") or row.get(\"question\") or \"\"\n","    if not user_input.strip():\n","        continue\n","\n","    # Build context\n","    context, refs, retrieval_cache, _, _ = build_context(\n","        user_input,\n","        retrieval_cache=retrieval_cache,\n","        bm25_retriever=bm25_retriever,\n","        embedding_model=embedding_model\n","    )\n","\n","    # Build full prompt\n","    full_prompt = custom_template.format(\n","        context=context,\n","        history=\"Không có hội thoại trước.\",\n","        input=user_input\n","    )\n","\n","    # Generate response\n","    try:\n","        ans_full = gen_pipe(full_prompt)[0][\"generated_text\"]\n","        split_token = \"### Trả lời:\"\n","        ans = ans_full.split(split_token, 1)[1].strip() if split_token in ans_full else ans_full.strip()\n","    except Exception as e:\n","        ans = f\"[ERROR generating response: {e}]\"\n","\n","    results.append({\n","        \"input\": user_input,\n","        \"context\": context,\n","        \"response\": ans\n","    })\n","\n","# -------------------- Save output --------------------\n","pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n","print(f\"Saved {len(results)} rows to {OUTPUT_CSV}\")\n"]},{"cell_type":"markdown","id":"MPpC57BbqaTt","metadata":{"id":"MPpC57BbqaTt"},"source":["### LLama-3b"]},{"cell_type":"code","execution_count":null,"id":"c1T-lwmIqY1Z","metadata":{"id":"c1T-lwmIqY1Z"},"outputs":[],"source":["import pandas as pd\n","import random\n","from tqdm import tqdm\n","import sys\n","import os\n","\n","\n","from src.logic_module import build_pipeline, bm25_retriever, embedding_model, custom_template, build_context\n","\n","# -------------------- Config --------------------\n","MODEL_KEY = \"llama-3b\"\n","INPUT_CSV = \"data/merged_all.csv\"\n","OUTPUT_CSV = f\"data/{MODEL_KEY}_eval_data.csv\"\n","NUM_ROWS = 300       # số dòng muốn chạy\n","SEED = 42\n","MAX_TOKENS = 128     # giảm để tránh OOM trên GPU/CPU\n","\n","# -------------------- Load data --------------------\n","df = pd.read_csv(INPUT_CSV)\n","random.seed(SEED)\n","sampled_rows = df.sample(n=min(NUM_ROWS, len(df)), random_state=SEED)\n","\n","# -------------------- Load model pipeline --------------------\n","# Chạy CPU safe (nếu GPU không đủ VRAM)\n","try:\n","    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n","except Exception as e:\n","    print(\"Error loading pipeline, fallback to CPU\")\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable GPU\n","    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n","\n","# -------------------- Run inference --------------------\n","results = []\n","retrieval_cache = []  # cache phải là list để build_context append được\n","\n","for _, row in tqdm(sampled_rows.iterrows(), total=len(sampled_rows)):\n","    # Giả sử có cột 'text' hoặc 'question'\n","    user_input = row.get(\"text\") or row.get(\"question\") or \"\"\n","    if not user_input.strip():\n","        continue\n","\n","    # Build context\n","    context, refs, retrieval_cache, _, _ = build_context(\n","        user_input,\n","        retrieval_cache=retrieval_cache,\n","        bm25_retriever=bm25_retriever,\n","        embedding_model=embedding_model\n","    )\n","\n","    # Build full prompt\n","    full_prompt = custom_template.format(\n","        context=context,\n","        history=\"Không có hội thoại trước.\",\n","        input=user_input\n","    )\n","\n","    # Generate response\n","    try:\n","        ans_full = gen_pipe(full_prompt)[0][\"generated_text\"]\n","        split_token = \"### Trả lời:\"\n","        ans = ans_full.split(split_token, 1)[1].strip() if split_token in ans_full else ans_full.strip()\n","    except Exception as e:\n","        ans = f\"[ERROR generating response: {e}]\"\n","\n","    results.append({\n","        \"input\": user_input,\n","        \"context\": context,\n","        \"response\": ans\n","    })\n","\n","# -------------------- Save output --------------------\n","pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n","print(f\"Saved {len(results)} rows to {OUTPUT_CSV}\")\n"]},{"cell_type":"markdown","id":"WPVcyXxU3-VR","metadata":{"id":"WPVcyXxU3-VR"},"source":["### Qwen2-7b"]},{"cell_type":"code","execution_count":null,"id":"AsooHbZaqd5u","metadata":{"id":"AsooHbZaqd5u"},"outputs":[],"source":["import pandas as pd\n","import random\n","from tqdm import tqdm\n","import sys\n","import os\n","\n","\n","from src.logic_module import build_pipeline, bm25_retriever, embedding_model, custom_template, build_context\n","\n","# -------------------- Config --------------------\n","MODEL_KEY = \"qwen2-7b\"\n","INPUT_CSV = \"data/merged_all.csv\"\n","OUTPUT_CSV = f\"data/{MODEL_KEY}_eval_data.csv\"\n","NUM_ROWS = 300       # số dòng muốn chạy\n","SEED = 42\n","MAX_TOKENS = 128     # giảm để tránh OOM trên GPU/CPU\n","\n","# -------------------- Load data --------------------\n","df = pd.read_csv(INPUT_CSV)\n","random.seed(SEED)\n","sampled_rows = df.sample(n=min(NUM_ROWS, len(df)), random_state=SEED)\n","\n","# -------------------- Load model pipeline --------------------\n","# Chạy CPU safe (nếu GPU không đủ VRAM)\n","try:\n","    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n","except Exception as e:\n","    print(\"Error loading pipeline, fallback to CPU\")\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable GPU\n","    gen_pipe, tokenizer = build_pipeline(model_key=MODEL_KEY, max_new_tokens=MAX_TOKENS)\n","\n","# -------------------- Run inference --------------------\n","results = []\n","retrieval_cache = []  # cache phải là list để build_context append được\n","\n","for _, row in tqdm(sampled_rows.iterrows(), total=len(sampled_rows)):\n","    # Giả sử có cột 'text' hoặc 'question'\n","    user_input = row.get(\"text\") or row.get(\"question\") or \"\"\n","    if not user_input.strip():\n","        continue\n","\n","    # Build context\n","    context, refs, retrieval_cache, _, _ = build_context(\n","        user_input,\n","        retrieval_cache=retrieval_cache,\n","        bm25_retriever=bm25_retriever,\n","        embedding_model=embedding_model\n","    )\n","\n","    # Build full prompt\n","    full_prompt = custom_template.format(\n","        context=context,\n","        history=\"Không có hội thoại trước.\",\n","        input=user_input\n","    )\n","\n","    # Generate response\n","    try:\n","        ans_full = gen_pipe(full_prompt)[0][\"generated_text\"]\n","        split_token = \"### Trả lời:\"\n","        ans = ans_full.split(split_token, 1)[1].strip() if split_token in ans_full else ans_full.strip()\n","    except Exception as e:\n","        ans = f\"[ERROR generating response: {e}]\"\n","\n","    results.append({\n","        \"input\": user_input,\n","        \"context\": context,\n","        \"response\": ans\n","    })\n","\n","# -------------------- Save output --------------------\n","pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n","print(f\"Saved {len(results)} rows to {OUTPUT_CSV}\")\n"]},{"cell_type":"markdown","id":"r9a9jvX8xWbw","metadata":{"id":"r9a9jvX8xWbw"},"source":["#Response Groundedness"]},{"cell_type":"markdown","id":"WBnjsESbFMeO","metadata":{"id":"WBnjsESbFMeO"},"source":["### Qwen2-3b"]},{"cell_type":"code","source":["import os\n","from dotenv import load_dotenv\n","import pandas as pd\n","import google.generativeai as genai\n","from collections import Counter\n","from time import sleep\n","\n","# -------------------------------\n","# 1️⃣ Load API key từ .env\n","# -------------------------------\n","load_dotenv()\n","api_key = os.getenv(\"GEMINI_API_KEY\")\n","genai.configure(api_key=api_key)\n","\n","# -------------------------------\n","# 2️⃣ Tạo Gemini 2.5 Flash instance\n","# -------------------------------\n","model = genai.GenerativeModel(\"gemini-2.5-pro\")\n","\n","# -------------------------------\n","# 3️⃣ Prompt functions (Judge 1 & Judge 2)\n","# -------------------------------\n","def response_groundedness_judge1_prompt(response: str, context: str) -> str:\n","    return f\"\"\"### Instruction\n","You are a world class expert designed to evaluate the groundedness of an assertion.\n","You will be provided with an assertion and a context.\n","Your task is to determine if the assertion is supported by the context.\n","Follow the instructions below:\n","A. If the assertion or context is empty, say 0.\n","B. If the assertion is not supported by the context, say 0.\n","C. If the assertion is partially supported by the context, say 1.\n","D. If the assertion is fully supported by the context, say 2.\n","You must provide a rating of 0, 1, or 2, nothing else.\n","\n","### Context:\n","<{context}>\n","\n","### Assertion:\n","<{response}>\n","\n","Analyzing Context and Response, the Groundedness score is \"\"\"\n","\n","def response_groundedness_judge2_prompt(response: str, context: str) -> str:\n","    return f\"\"\"As a specialist in assessing the strength of connections between statements and their given contexts, I will evaluate the level of support an assertion receives from the provided context. Follow these guidelines:\n","\n","* If the assertion or context is empty or assertion is not supported, assign a score of 0.\n","* If the assertion is partially supported, assign a score of 1.\n","* If the assertion is fully supported, assign a score of 2.\n","\n","I will provide a rating of 0, 1, or 2, without any additional information.\n","\n","---\n","**Context:**\n","[{context}]\n","\n","**Assertion:**\n","[{response}]\n","\n","Do not explain. Based on the provided context and response, the Groundedness score is:\"\"\"\n","\n","# -------------------------------\n","# 4️⃣ Helper function gọi Gemini Flash và chuẩn hóa\n","# -------------------------------\n","def call_gemini(prompt: str) -> float:\n","    \"\"\"Gọi Gemini Flash, parse output thành float 0,1,2\"\"\"\n","    try:\n","        response = model.generate_content(prompt)\n","        text = response.text.strip()\n","        score = int(text)\n","        if score in [0, 1, 2]:\n","            return score\n","    except Exception as e:\n","        print(f\"[ERROR call_gemini]: {e}\")\n","    return None\n","\n","def compute_groundedness(response: str, context: str) -> float:\n","    \"\"\"Tính điểm groundedness dựa trên 2 judge\"\"\"\n","    score1 = call_gemini(response_groundedness_judge1_prompt(response, context))\n","    score2 = call_gemini(response_groundedness_judge2_prompt(response, context))\n","\n","    normalized_scores = []\n","    if score1 is not None:\n","        normalized_scores.append(score1 / 2)\n","    if score2 is not None:\n","        normalized_scores.append(score2 / 2)\n","\n","    if not normalized_scores:\n","        return 0.0\n","    return sum(normalized_scores) / len(normalized_scores)\n","\n","# -------------------------------\n","# 5️⃣ Load data từ CSV (toàn bộ, không sample)\n","# -------------------------------\n","csv_path = \"data/qwen2-3b_eval_data.csv\"\n","df = pd.read_csv(csv_path)\n","\n","# -------------------------------\n","# 6️⃣ Chuẩn bị output\n","# -------------------------------\n","os.makedirs(\"eval_result\", exist_ok=True)\n","output_csv = \"eval_result/qwen2_3b_groundedness.csv\"\n","\n","# Nếu file đã tồn tại, xóa để ghi batch mới\n","if os.path.exists(output_csv):\n","    os.remove(output_csv)\n","\n","# -------------------------------\n","# 7️⃣ Chạy evaluation theo batch 10 dòng\n","# -------------------------------\n","BATCH_SIZE = 10\n","batch_results = []\n","\n","for idx, row in df.iterrows():\n","    response = str(row[\"response\"])\n","    context = str(row[\"context\"])\n","    score = compute_groundedness(response, context)\n","    print(f\"Index: {idx}, Groundedness score: {score}\")\n","\n","    batch_results.append({\n","        \"question\": row.get(\"question\", \"\"),\n","        \"context\": context,\n","        \"response\": response,\n","        \"groundedness_score\": score\n","    })\n","\n","    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n","    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n","        df_batch = pd.DataFrame(batch_results)\n","        # Append nếu file đã tồn tại, else write mới\n","        if os.path.exists(output_csv):\n","            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n","        else:\n","            df_batch.to_csv(output_csv, index=False)\n","        batch_results = []  # reset batch\n","        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n","\n","# -------------------------------\n","# 8️⃣ Thống kê tổng số điểm\n","# -------------------------------\n","df_final = pd.read_csv(output_csv)\n","score_counts = Counter(df_final[\"groundedness_score\"])\n","print(\"\\n=== Groundedness Score Distribution ===\")\n","for score_level in sorted(score_counts.keys()):\n","    print(f\"Score {score_level}: {score_counts[score_level]} responses\")\n","\n","total_score = df_final[\"groundedness_score\"].sum()\n","print(f\"\\nTotal Groundedness score (sum): {total_score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wplUCHBu02OH","executionInfo":{"status":"ok","timestamp":1764615527324,"user_tz":-420,"elapsed":7646034,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"385cacd3-752e-464c-a40b-db64f90b74b1"},"id":"wplUCHBu02OH","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Index: 0, Groundedness score: 1.0\n","Index: 1, Groundedness score: 0.5\n","Index: 2, Groundedness score: 1.0\n","Index: 3, Groundedness score: 1.0\n","Index: 4, Groundedness score: 0.5\n","Index: 5, Groundedness score: 1.0\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2258.44ms\n"]},{"output_type":"stream","name":"stdout","text":["Index: 6, Groundedness score: 1.0\n","Index: 7, Groundedness score: 1.0\n","Index: 8, Groundedness score: 1.0\n","Index: 9, Groundedness score: 1.0\n","Saved batch ending at index 9 to eval_result/qwen2_3b_groundedness.csv\n","Index: 10, Groundedness score: 0.5\n","Index: 11, Groundedness score: 1.0\n","Index: 12, Groundedness score: 0.5\n","Index: 13, Groundedness score: 0.5\n","Index: 14, Groundedness score: 1.0\n","Index: 15, Groundedness score: 0.75\n","Index: 16, Groundedness score: 0.25\n","Index: 17, Groundedness score: 1.0\n","Index: 18, Groundedness score: 1.0\n","Index: 19, Groundedness score: 0.0\n","Saved batch ending at index 19 to eval_result/qwen2_3b_groundedness.csv\n","Index: 20, Groundedness score: 1.0\n","Index: 21, Groundedness score: 0.5\n","Index: 22, Groundedness score: 0.0\n","Index: 23, Groundedness score: 1.0\n","Index: 24, Groundedness score: 0.5\n","Index: 25, Groundedness score: 0.5\n","Index: 26, Groundedness score: 1.0\n","Index: 27, Groundedness score: 1.0\n","Index: 28, Groundedness score: 0.5\n","Index: 29, Groundedness score: 1.0\n","Saved batch ending at index 29 to eval_result/qwen2_3b_groundedness.csv\n","Index: 30, Groundedness score: 0.5\n","Index: 31, Groundedness score: 0.5\n","Index: 32, Groundedness score: 1.0\n","Index: 33, Groundedness score: 0.5\n","Index: 34, Groundedness score: 1.0\n","Index: 35, Groundedness score: 1.0\n","Index: 36, Groundedness score: 1.0\n","Index: 37, Groundedness score: 1.0\n","Index: 38, Groundedness score: 0.5\n","Index: 39, Groundedness score: 0.75\n","Saved batch ending at index 39 to eval_result/qwen2_3b_groundedness.csv\n","Index: 40, Groundedness score: 1.0\n","Index: 41, Groundedness score: 1.0\n","Index: 42, Groundedness score: 0.75\n","Index: 43, Groundedness score: 0.0\n","Index: 44, Groundedness score: 0.5\n","Index: 45, Groundedness score: 0.5\n","Index: 46, Groundedness score: 1.0\n","Index: 47, Groundedness score: 1.0\n","Index: 48, Groundedness score: 0.0\n","Index: 49, Groundedness score: 0.5\n","Saved batch ending at index 49 to eval_result/qwen2_3b_groundedness.csv\n","Index: 50, Groundedness score: 1.0\n","Index: 51, Groundedness score: 0.5\n","Index: 52, Groundedness score: 0.5\n","Index: 53, Groundedness score: 1.0\n","Index: 54, Groundedness score: 1.0\n","Index: 55, Groundedness score: 0.0\n","Index: 56, Groundedness score: 0.75\n","Index: 57, Groundedness score: 0.5\n","Index: 58, Groundedness score: 0.0\n","Index: 59, Groundedness score: 0.0\n","Saved batch ending at index 59 to eval_result/qwen2_3b_groundedness.csv\n","Index: 60, Groundedness score: 1.0\n","Index: 61, Groundedness score: 0.75\n","Index: 62, Groundedness score: 1.0\n","Index: 63, Groundedness score: 1.0\n","Index: 64, Groundedness score: 0.75\n","Index: 65, Groundedness score: 1.0\n","Index: 66, Groundedness score: 0.75\n","Index: 67, Groundedness score: 0.25\n","Index: 68, Groundedness score: 0.75\n","Index: 69, Groundedness score: 0.75\n","Saved batch ending at index 69 to eval_result/qwen2_3b_groundedness.csv\n","Index: 70, Groundedness score: 0.75\n","Index: 71, Groundedness score: 0.75\n","Index: 72, Groundedness score: 1.0\n","Index: 73, Groundedness score: 1.0\n","Index: 74, Groundedness score: 0.5\n","Index: 75, Groundedness score: 0.5\n","Index: 76, Groundedness score: 0.5\n","Index: 77, Groundedness score: 1.0\n","Index: 78, Groundedness score: 1.0\n","Index: 79, Groundedness score: 0.75\n","Saved batch ending at index 79 to eval_result/qwen2_3b_groundedness.csv\n","Index: 80, Groundedness score: 0.5\n","Index: 81, Groundedness score: 1.0\n","Index: 82, Groundedness score: 0.0\n","Index: 83, Groundedness score: 0.5\n","Index: 84, Groundedness score: 1.0\n","Index: 85, Groundedness score: 1.0\n","Index: 86, Groundedness score: 1.0\n","Index: 87, Groundedness score: 0.5\n","Index: 88, Groundedness score: 0.5\n","Index: 89, Groundedness score: 0.0\n","Saved batch ending at index 89 to eval_result/qwen2_3b_groundedness.csv\n","Index: 90, Groundedness score: 0.5\n","Index: 91, Groundedness score: 0.5\n","Index: 92, Groundedness score: 0.0\n","Index: 93, Groundedness score: 0.25\n","Index: 94, Groundedness score: 0.5\n","Index: 95, Groundedness score: 0.0\n","Index: 96, Groundedness score: 0.5\n","Index: 97, Groundedness score: 1.0\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2665.16ms\n"]},{"output_type":"stream","name":"stdout","text":["Index: 98, Groundedness score: 1.0\n","Index: 99, Groundedness score: 1.0\n","Saved batch ending at index 99 to eval_result/qwen2_3b_groundedness.csv\n","Index: 100, Groundedness score: 0.5\n","Index: 101, Groundedness score: 1.0\n","Index: 102, Groundedness score: 0.5\n","Index: 103, Groundedness score: 1.0\n","Index: 104, Groundedness score: 0.0\n","Index: 105, Groundedness score: 0.0\n","Index: 106, Groundedness score: 1.0\n","Index: 107, Groundedness score: 1.0\n","Index: 108, Groundedness score: 1.0\n","Index: 109, Groundedness score: 1.0\n","Saved batch ending at index 109 to eval_result/qwen2_3b_groundedness.csv\n","Index: 110, Groundedness score: 0.5\n","Index: 111, Groundedness score: 1.0\n","Index: 112, Groundedness score: 0.5\n","Index: 113, Groundedness score: 1.0\n","Index: 114, Groundedness score: 0.75\n","Index: 115, Groundedness score: 0.25\n","Index: 116, Groundedness score: 1.0\n","Index: 117, Groundedness score: 0.5\n","Index: 118, Groundedness score: 1.0\n","Index: 119, Groundedness score: 1.0\n","Saved batch ending at index 119 to eval_result/qwen2_3b_groundedness.csv\n","Index: 120, Groundedness score: 1.0\n","Index: 121, Groundedness score: 0.5\n","Index: 122, Groundedness score: 0.0\n","Index: 123, Groundedness score: 1.0\n","Index: 124, Groundedness score: 1.0\n","Index: 125, Groundedness score: 1.0\n","Index: 126, Groundedness score: 0.75\n","Index: 127, Groundedness score: 0.0\n","Index: 128, Groundedness score: 0.0\n","Index: 129, Groundedness score: 1.0\n","Saved batch ending at index 129 to eval_result/qwen2_3b_groundedness.csv\n","Index: 130, Groundedness score: 0.0\n","Index: 131, Groundedness score: 0.5\n","Index: 132, Groundedness score: 1.0\n","Index: 133, Groundedness score: 1.0\n","Index: 134, Groundedness score: 1.0\n","Index: 135, Groundedness score: 0.5\n","Index: 136, Groundedness score: 1.0\n","Index: 137, Groundedness score: 1.0\n","Index: 138, Groundedness score: 1.0\n","Index: 139, Groundedness score: 0.5\n","Saved batch ending at index 139 to eval_result/qwen2_3b_groundedness.csv\n","Index: 140, Groundedness score: 0.75\n","Index: 141, Groundedness score: 0.5\n","Index: 142, Groundedness score: 0.5\n","Index: 143, Groundedness score: 1.0\n","Index: 144, Groundedness score: 1.0\n","Index: 145, Groundedness score: 0.5\n","Index: 146, Groundedness score: 1.0\n","Index: 147, Groundedness score: 1.0\n","Index: 148, Groundedness score: 1.0\n","Index: 149, Groundedness score: 1.0\n","Saved batch ending at index 149 to eval_result/qwen2_3b_groundedness.csv\n","Index: 150, Groundedness score: 0.5\n","Index: 151, Groundedness score: 0.0\n","Index: 152, Groundedness score: 1.0\n","Index: 153, Groundedness score: 0.5\n","Index: 154, Groundedness score: 0.0\n","Index: 155, Groundedness score: 0.5\n","Index: 156, Groundedness score: 1.0\n","Index: 157, Groundedness score: 1.0\n","Index: 158, Groundedness score: 0.5\n","Index: 159, Groundedness score: 1.0\n","Saved batch ending at index 159 to eval_result/qwen2_3b_groundedness.csv\n","Index: 160, Groundedness score: 0.5\n","Index: 161, Groundedness score: 1.0\n","Index: 162, Groundedness score: 0.5\n","Index: 163, Groundedness score: 0.5\n","Index: 164, Groundedness score: 0.75\n","Index: 165, Groundedness score: 0.5\n","Index: 166, Groundedness score: 1.0\n","Index: 167, Groundedness score: 1.0\n","Index: 168, Groundedness score: 1.0\n","Index: 169, Groundedness score: 1.0\n","Saved batch ending at index 169 to eval_result/qwen2_3b_groundedness.csv\n","Index: 170, Groundedness score: 0.5\n","Index: 171, Groundedness score: 1.0\n","Index: 172, Groundedness score: 0.0\n","Index: 173, Groundedness score: 1.0\n","Index: 174, Groundedness score: 1.0\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1628.87ms\n","ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3630.94ms\n"]},{"output_type":"stream","name":"stdout","text":["[ERROR call_gemini]: HTTPConnectionPool(host='localhost', port=33031): Read timed out. (read timeout=600.0)\n","Index: 175, Groundedness score: 1.0\n","Index: 176, Groundedness score: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1951.70ms\n"]},{"output_type":"stream","name":"stdout","text":["Index: 177, Groundedness score: 0.5\n","Index: 178, Groundedness score: 1.0\n","Index: 179, Groundedness score: 1.0\n","Saved batch ending at index 179 to eval_result/qwen2_3b_groundedness.csv\n","Index: 180, Groundedness score: 0.5\n","Index: 181, Groundedness score: 0.75\n","Index: 182, Groundedness score: 0.5\n","Index: 183, Groundedness score: 1.0\n","Index: 184, Groundedness score: 0.75\n","Index: 185, Groundedness score: 0.0\n","Index: 186, Groundedness score: 0.0\n","Index: 187, Groundedness score: 0.5\n","Index: 188, Groundedness score: 0.75\n","Index: 189, Groundedness score: 0.75\n","Saved batch ending at index 189 to eval_result/qwen2_3b_groundedness.csv\n","Index: 190, Groundedness score: 1.0\n","Index: 191, Groundedness score: 1.0\n","Index: 192, Groundedness score: 0.5\n","Index: 193, Groundedness score: 0.5\n","Index: 194, Groundedness score: 0.25\n","Index: 195, Groundedness score: 1.0\n","Index: 196, Groundedness score: 0.75\n","Index: 197, Groundedness score: 0.5\n","Index: 198, Groundedness score: 1.0\n","Index: 199, Groundedness score: 0.5\n","Saved batch ending at index 199 to eval_result/qwen2_3b_groundedness.csv\n","Index: 200, Groundedness score: 1.0\n","Index: 201, Groundedness score: 0.0\n","Index: 202, Groundedness score: 0.5\n","Index: 203, Groundedness score: 0.5\n","Index: 204, Groundedness score: 1.0\n","Index: 205, Groundedness score: 0.75\n","Index: 206, Groundedness score: 1.0\n","Index: 207, Groundedness score: 0.0\n","Index: 208, Groundedness score: 1.0\n","Index: 209, Groundedness score: 0.25\n","Saved batch ending at index 209 to eval_result/qwen2_3b_groundedness.csv\n","Index: 210, Groundedness score: 0.5\n","Index: 211, Groundedness score: 0.5\n","Index: 212, Groundedness score: 1.0\n","Index: 213, Groundedness score: 1.0\n","Index: 214, Groundedness score: 0.0\n","Index: 215, Groundedness score: 0.25\n","Index: 216, Groundedness score: 0.0\n","Index: 217, Groundedness score: 0.5\n","Index: 218, Groundedness score: 0.0\n","Index: 219, Groundedness score: 0.75\n","Saved batch ending at index 219 to eval_result/qwen2_3b_groundedness.csv\n","Index: 220, Groundedness score: 0.25\n","Index: 221, Groundedness score: 0.25\n","Index: 222, Groundedness score: 1.0\n","Index: 223, Groundedness score: 1.0\n","Index: 224, Groundedness score: 1.0\n","Index: 225, Groundedness score: 1.0\n","Index: 226, Groundedness score: 0.5\n","Index: 227, Groundedness score: 1.0\n","Index: 228, Groundedness score: 1.0\n","Index: 229, Groundedness score: 0.5\n","Saved batch ending at index 229 to eval_result/qwen2_3b_groundedness.csv\n","Index: 230, Groundedness score: 0.75\n","Index: 231, Groundedness score: 1.0\n","Index: 232, Groundedness score: 0.5\n","Index: 233, Groundedness score: 1.0\n","Index: 234, Groundedness score: 1.0\n","Index: 235, Groundedness score: 1.0\n","Index: 236, Groundedness score: 1.0\n","Index: 237, Groundedness score: 0.75\n","Index: 238, Groundedness score: 0.5\n","Index: 239, Groundedness score: 0.5\n","Saved batch ending at index 239 to eval_result/qwen2_3b_groundedness.csv\n","Index: 240, Groundedness score: 0.25\n","Index: 241, Groundedness score: 1.0\n","Index: 242, Groundedness score: 0.5\n","Index: 243, Groundedness score: 1.0\n","Index: 244, Groundedness score: 0.75\n","Index: 245, Groundedness score: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 4895.03ms\n","ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3371.11ms\n"]},{"output_type":"stream","name":"stdout","text":["Index: 246, Groundedness score: 1.0\n","Index: 247, Groundedness score: 1.0\n","Index: 248, Groundedness score: 0.0\n","Index: 249, Groundedness score: 1.0\n","Saved batch ending at index 249 to eval_result/qwen2_3b_groundedness.csv\n","Index: 250, Groundedness score: 0.5\n","Index: 251, Groundedness score: 1.0\n","Index: 252, Groundedness score: 0.5\n","Index: 253, Groundedness score: 0.75\n","Index: 254, Groundedness score: 0.0\n","Index: 255, Groundedness score: 1.0\n","Index: 256, Groundedness score: 0.5\n","Index: 257, Groundedness score: 0.0\n","Index: 258, Groundedness score: 0.5\n","Index: 259, Groundedness score: 0.0\n","Saved batch ending at index 259 to eval_result/qwen2_3b_groundedness.csv\n","Index: 260, Groundedness score: 1.0\n","Index: 261, Groundedness score: 1.0\n","Index: 262, Groundedness score: 1.0\n","Index: 263, Groundedness score: 1.0\n","Index: 264, Groundedness score: 0.0\n","Index: 265, Groundedness score: 0.5\n","Index: 266, Groundedness score: 1.0\n","Index: 267, Groundedness score: 0.5\n","Index: 268, Groundedness score: 1.0\n","Index: 269, Groundedness score: 0.25\n","Saved batch ending at index 269 to eval_result/qwen2_3b_groundedness.csv\n","Index: 270, Groundedness score: 0.5\n","Index: 271, Groundedness score: 1.0\n","Index: 272, Groundedness score: 1.0\n","Index: 273, Groundedness score: 1.0\n","Index: 274, Groundedness score: 1.0\n","Index: 275, Groundedness score: 1.0\n","Index: 276, Groundedness score: 1.0\n","Index: 277, Groundedness score: 0.75\n","Index: 278, Groundedness score: 0.25\n","Index: 279, Groundedness score: 1.0\n","Saved batch ending at index 279 to eval_result/qwen2_3b_groundedness.csv\n","Index: 280, Groundedness score: 0.75\n","Index: 281, Groundedness score: 0.5\n","Index: 282, Groundedness score: 1.0\n","Index: 283, Groundedness score: 0.0\n","Index: 284, Groundedness score: 1.0\n","Index: 285, Groundedness score: 1.0\n","Index: 286, Groundedness score: 1.0\n","Index: 287, Groundedness score: 1.0\n","Index: 288, Groundedness score: 0.75\n","Index: 289, Groundedness score: 1.0\n","Saved batch ending at index 289 to eval_result/qwen2_3b_groundedness.csv\n","Index: 290, Groundedness score: 0.5\n","Index: 291, Groundedness score: 0.5\n","Index: 292, Groundedness score: 0.0\n","Index: 293, Groundedness score: 1.0\n","Index: 294, Groundedness score: 0.0\n","Index: 295, Groundedness score: 0.5\n","Index: 296, Groundedness score: 1.0\n","Index: 297, Groundedness score: 1.0\n","Index: 298, Groundedness score: 0.5\n","Index: 299, Groundedness score: 0.5\n","Saved batch ending at index 299 to eval_result/qwen2_3b_groundedness.csv\n","\n","=== Groundedness Score Distribution ===\n","Score 0.0: 37 responses\n","Score 0.25: 12 responses\n","Score 0.5: 83 responses\n","Score 0.75: 30 responses\n","Score 1.0: 138 responses\n","\n","Total Groundedness score (sum): 205.0\n"]}]},{"cell_type":"markdown","source":["### LLama 3b"],"metadata":{"id":"awG38Pnc0vor"},"id":"awG38Pnc0vor"},{"cell_type":"code","source":["import os\n","from dotenv import load_dotenv\n","import pandas as pd\n","import google.generativeai as genai\n","from collections import Counter\n","from time import sleep\n","\n","# -------------------------------\n","# 1️⃣ Load API key từ .env\n","# -------------------------------\n","load_dotenv()\n","api_key = os.getenv(\"GEMINI_API_KEY\")\n","genai.configure(api_key=api_key)\n","\n","# -------------------------------\n","# 2️⃣ Tạo Gemini 2.5 Flash instance\n","# -------------------------------\n","model = genai.GenerativeModel(\"gemini-2.5-pro\")\n","\n","# -------------------------------\n","# 3️⃣ Prompt functions (Judge 1 & Judge 2)\n","# -------------------------------\n","def response_groundedness_judge1_prompt(response: str, context: str) -> str:\n","    return f\"\"\"### Instruction\n","You are a world class expert designed to evaluate the groundedness of an assertion.\n","You will be provided with an assertion and a context.\n","Your task is to determine if the assertion is supported by the context.\n","Follow the instructions below:\n","A. If the assertion or context is empty, say 0.\n","B. If the assertion is not supported by the context, say 0.\n","C. If the assertion is partially supported by the context, say 1.\n","D. If the assertion is fully supported by the context, say 2.\n","You must provide a rating of 0, 1, or 2, nothing else.\n","\n","### Context:\n","<{context}>\n","\n","### Assertion:\n","<{response}>\n","\n","Analyzing Context and Response, the Groundedness score is \"\"\"\n","\n","def response_groundedness_judge2_prompt(response: str, context: str) -> str:\n","    return f\"\"\"As a specialist in assessing the strength of connections between statements and their given contexts, I will evaluate the level of support an assertion receives from the provided context. Follow these guidelines:\n","\n","* If the assertion or context is empty or assertion is not supported, assign a score of 0.\n","* If the assertion is partially supported, assign a score of 1.\n","* If the assertion is fully supported, assign a score of 2.\n","\n","I will provide a rating of 0, 1, or 2, without any additional information.\n","\n","---\n","**Context:**\n","[{context}]\n","\n","**Assertion:**\n","[{response}]\n","\n","Do not explain. Based on the provided context and response, the Groundedness score is:\"\"\"\n","\n","# -------------------------------\n","# 4️⃣ Helper function gọi Gemini Flash và chuẩn hóa\n","# -------------------------------\n","def call_gemini(prompt: str) -> float:\n","    \"\"\"Gọi Gemini Flash, parse output thành float 0,1,2\"\"\"\n","    try:\n","        response = model.generate_content(prompt)\n","        text = response.text.strip()\n","        score = int(text)\n","        if score in [0, 1, 2]:\n","            return score\n","    except Exception as e:\n","        print(f\"[ERROR call_gemini]: {e}\")\n","    return None\n","\n","def compute_groundedness(response: str, context: str) -> float:\n","    \"\"\"Tính điểm groundedness dựa trên 2 judge\"\"\"\n","    score1 = call_gemini(response_groundedness_judge1_prompt(response, context))\n","    score2 = call_gemini(response_groundedness_judge2_prompt(response, context))\n","\n","    normalized_scores = []\n","    if score1 is not None:\n","        normalized_scores.append(score1 / 2)\n","    if score2 is not None:\n","        normalized_scores.append(score2 / 2)\n","\n","    if not normalized_scores:\n","        return 0.0\n","    return sum(normalized_scores) / len(normalized_scores)\n","\n","# -------------------------------\n","# 5️⃣ Load data từ CSV (toàn bộ, không sample)\n","# -------------------------------\n","csv_path = \"data/llama_3b_eval_data.csv\"\n","df = pd.read_csv(csv_path)\n","\n","# -------------------------------\n","# 6️⃣ Chuẩn bị output\n","# -------------------------------\n","os.makedirs(\"eval_result\", exist_ok=True)\n","output_csv = \"eval_result/llama_3b_groundedness.csv\"\n","\n","# Nếu file đã tồn tại, xóa để ghi batch mới\n","if os.path.exists(output_csv):\n","    os.remove(output_csv)\n","\n","# -------------------------------\n","# 7️⃣ Chạy evaluation theo batch 10 dòng\n","# -------------------------------\n","BATCH_SIZE = 10\n","batch_results = []\n","\n","for idx, row in df.iterrows():\n","    response = str(row[\"response\"])\n","    context = str(row[\"context\"])\n","    score = compute_groundedness(response, context)\n","    print(f\"Index: {idx}, Groundedness score: {score}\")\n","\n","    batch_results.append({\n","        \"question\": row.get(\"question\", \"\"),\n","        \"context\": context,\n","        \"response\": response,\n","        \"groundedness_score\": score\n","    })\n","\n","    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n","    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n","        df_batch = pd.DataFrame(batch_results)\n","        # Append nếu file đã tồn tại, else write mới\n","        if os.path.exists(output_csv):\n","            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n","        else:\n","            df_batch.to_csv(output_csv, index=False)\n","        batch_results = []  # reset batch\n","        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n","\n","# -------------------------------\n","# 8️⃣ Thống kê tổng số điểm\n","# -------------------------------\n","df_final = pd.read_csv(output_csv)\n","score_counts = Counter(df_final[\"groundedness_score\"])\n","print(\"\\n=== Groundedness Score Distribution ===\")\n","for score_level in sorted(score_counts.keys()):\n","    print(f\"Score {score_level}: {score_counts[score_level]} responses\")\n","\n","total_score = df_final[\"groundedness_score\"].sum()\n","print(f\"\\nTotal Groundedness score (sum): {total_score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3ZBxiQUx1lxz","executionInfo":{"status":"ok","timestamp":1764622510225,"user_tz":-420,"elapsed":6969634,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"4c39e332-c250-4863-a260-034178bb14d8"},"id":"3ZBxiQUx1lxz","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Index: 0, Groundedness score: 1.0\n","Index: 1, Groundedness score: 1.0\n","Index: 2, Groundedness score: 0.5\n","Index: 3, Groundedness score: 1.0\n","Index: 4, Groundedness score: 0.5\n","Index: 5, Groundedness score: 0.5\n","Index: 6, Groundedness score: 1.0\n","Index: 7, Groundedness score: 1.0\n","Index: 8, Groundedness score: 0.5\n","Index: 9, Groundedness score: 1.0\n","Saved batch ending at index 9 to eval_result/llama_3b_groundedness.csv\n","Index: 10, Groundedness score: 1.0\n","Index: 11, Groundedness score: 0.25\n","Index: 12, Groundedness score: 0.0\n","Index: 13, Groundedness score: 0.5\n","Index: 14, Groundedness score: 1.0\n","Index: 15, Groundedness score: 1.0\n","Index: 16, Groundedness score: 1.0\n","Index: 17, Groundedness score: 0.5\n","Index: 18, Groundedness score: 0.5\n","Index: 19, Groundedness score: 0.0\n","Saved batch ending at index 19 to eval_result/llama_3b_groundedness.csv\n","Index: 20, Groundedness score: 0.75\n","Index: 21, Groundedness score: 0.25\n","Index: 22, Groundedness score: 0.5\n","Index: 23, Groundedness score: 1.0\n","Index: 24, Groundedness score: 1.0\n","Index: 25, Groundedness score: 0.0\n","Index: 26, Groundedness score: 1.0\n","Index: 27, Groundedness score: 1.0\n","Index: 28, Groundedness score: 1.0\n","Index: 29, Groundedness score: 0.5\n","Saved batch ending at index 29 to eval_result/llama_3b_groundedness.csv\n","Index: 30, Groundedness score: 0.5\n","Index: 31, Groundedness score: 0.5\n","Index: 32, Groundedness score: 0.5\n","Index: 33, Groundedness score: 0.5\n","Index: 34, Groundedness score: 1.0\n","Index: 35, Groundedness score: 1.0\n","Index: 36, Groundedness score: 1.0\n","Index: 37, Groundedness score: 1.0\n","Index: 38, Groundedness score: 1.0\n","Index: 39, Groundedness score: 1.0\n","Saved batch ending at index 39 to eval_result/llama_3b_groundedness.csv\n","Index: 40, Groundedness score: 1.0\n","Index: 41, Groundedness score: 0.0\n","Index: 42, Groundedness score: 1.0\n","Index: 43, Groundedness score: 0.0\n","Index: 44, Groundedness score: 1.0\n","Index: 45, Groundedness score: 0.75\n","Index: 46, Groundedness score: 0.75\n","Index: 47, Groundedness score: 1.0\n","Index: 48, Groundedness score: 0.5\n","Index: 49, Groundedness score: 0.5\n","Saved batch ending at index 49 to eval_result/llama_3b_groundedness.csv\n","Index: 50, Groundedness score: 1.0\n","Index: 51, Groundedness score: 1.0\n","Index: 52, Groundedness score: 0.5\n","Index: 53, Groundedness score: 1.0\n","Index: 54, Groundedness score: 0.5\n","Index: 55, Groundedness score: 0.0\n","Index: 56, Groundedness score: 1.0\n","Index: 57, Groundedness score: 1.0\n","Index: 58, Groundedness score: 0.25\n","Index: 59, Groundedness score: 0.25\n","Saved batch ending at index 59 to eval_result/llama_3b_groundedness.csv\n","Index: 60, Groundedness score: 0.5\n","Index: 61, Groundedness score: 1.0\n","Index: 62, Groundedness score: 0.5\n","Index: 63, Groundedness score: 0.75\n","Index: 64, Groundedness score: 1.0\n","Index: 65, Groundedness score: 0.5\n","Index: 66, Groundedness score: 1.0\n","Index: 67, Groundedness score: 0.5\n","Index: 68, Groundedness score: 1.0\n","Index: 69, Groundedness score: 0.25\n","Saved batch ending at index 69 to eval_result/llama_3b_groundedness.csv\n","Index: 70, Groundedness score: 1.0\n","Index: 71, Groundedness score: 1.0\n","Index: 72, Groundedness score: 0.0\n","Index: 73, Groundedness score: 0.75\n","Index: 74, Groundedness score: 0.25\n","Index: 75, Groundedness score: 0.0\n","Index: 76, Groundedness score: 1.0\n","Index: 77, Groundedness score: 1.0\n","Index: 78, Groundedness score: 1.0\n","Index: 79, Groundedness score: 0.5\n","Saved batch ending at index 79 to eval_result/llama_3b_groundedness.csv\n","Index: 80, Groundedness score: 0.5\n","Index: 81, Groundedness score: 1.0\n","Index: 82, Groundedness score: 0.25\n","Index: 83, Groundedness score: 1.0\n","Index: 84, Groundedness score: 0.0\n","Index: 85, Groundedness score: 1.0\n","Index: 86, Groundedness score: 1.0\n","Index: 87, Groundedness score: 0.5\n","Index: 88, Groundedness score: 1.0\n","Index: 89, Groundedness score: 0.0\n","Saved batch ending at index 89 to eval_result/llama_3b_groundedness.csv\n","Index: 90, Groundedness score: 0.5\n","Index: 91, Groundedness score: 0.5\n","Index: 92, Groundedness score: 0.0\n","Index: 93, Groundedness score: 0.5\n","Index: 94, Groundedness score: 1.0\n","Index: 95, Groundedness score: 0.0\n","Index: 96, Groundedness score: 0.5\n","Index: 97, Groundedness score: 1.0\n","Index: 98, Groundedness score: 0.5\n","Index: 99, Groundedness score: 0.75\n","Saved batch ending at index 99 to eval_result/llama_3b_groundedness.csv\n","Index: 100, Groundedness score: 0.0\n","Index: 101, Groundedness score: 1.0\n","Index: 102, Groundedness score: 0.25\n","Index: 103, Groundedness score: 0.0\n","Index: 104, Groundedness score: 0.0\n","Index: 105, Groundedness score: 0.0\n","Index: 106, Groundedness score: 0.5\n","Index: 107, Groundedness score: 1.0\n","Index: 108, Groundedness score: 1.0\n","Index: 109, Groundedness score: 0.5\n","Saved batch ending at index 109 to eval_result/llama_3b_groundedness.csv\n","Index: 110, Groundedness score: 0.5\n","Index: 111, Groundedness score: 1.0\n","Index: 112, Groundedness score: 0.5\n","Index: 113, Groundedness score: 1.0\n","Index: 114, Groundedness score: 0.5\n","Index: 115, Groundedness score: 0.75\n","Index: 116, Groundedness score: 0.25\n","Index: 117, Groundedness score: 1.0\n","Index: 118, Groundedness score: 0.5\n","Index: 119, Groundedness score: 0.5\n","Saved batch ending at index 119 to eval_result/llama_3b_groundedness.csv\n","Index: 120, Groundedness score: 1.0\n","Index: 121, Groundedness score: 0.5\n","Index: 122, Groundedness score: 0.0\n","Index: 123, Groundedness score: 0.5\n","Index: 124, Groundedness score: 1.0\n","Index: 125, Groundedness score: 1.0\n","Index: 126, Groundedness score: 1.0\n","Index: 127, Groundedness score: 0.0\n","Index: 128, Groundedness score: 0.25\n","Index: 129, Groundedness score: 1.0\n","Saved batch ending at index 129 to eval_result/llama_3b_groundedness.csv\n","Index: 130, Groundedness score: 0.0\n","Index: 131, Groundedness score: 0.0\n","Index: 132, Groundedness score: 1.0\n","Index: 133, Groundedness score: 1.0\n","Index: 134, Groundedness score: 1.0\n","Index: 135, Groundedness score: 0.75\n","Index: 136, Groundedness score: 1.0\n","Index: 137, Groundedness score: 0.5\n","Index: 138, Groundedness score: 0.5\n","Index: 139, Groundedness score: 0.75\n","Saved batch ending at index 139 to eval_result/llama_3b_groundedness.csv\n","Index: 140, Groundedness score: 0.5\n","Index: 141, Groundedness score: 0.5\n","Index: 142, Groundedness score: 0.5\n","Index: 143, Groundedness score: 0.5\n","Index: 144, Groundedness score: 1.0\n","Index: 145, Groundedness score: 0.5\n","Index: 146, Groundedness score: 1.0\n","Index: 147, Groundedness score: 1.0\n","Index: 148, Groundedness score: 0.5\n","Index: 149, Groundedness score: 1.0\n","Saved batch ending at index 149 to eval_result/llama_3b_groundedness.csv\n","Index: 150, Groundedness score: 1.0\n","Index: 151, Groundedness score: 1.0\n","Index: 152, Groundedness score: 0.0\n","Index: 153, Groundedness score: 1.0\n","Index: 154, Groundedness score: 0.0\n","Index: 155, Groundedness score: 1.0\n","Index: 156, Groundedness score: 0.5\n","Index: 157, Groundedness score: 1.0\n","Index: 158, Groundedness score: 0.5\n","Index: 159, Groundedness score: 0.5\n","Saved batch ending at index 159 to eval_result/llama_3b_groundedness.csv\n","Index: 160, Groundedness score: 1.0\n","Index: 161, Groundedness score: 0.0\n","Index: 162, Groundedness score: 1.0\n","Index: 163, Groundedness score: 1.0\n","Index: 164, Groundedness score: 1.0\n","Index: 165, Groundedness score: 1.0\n","Index: 166, Groundedness score: 0.5\n","Index: 167, Groundedness score: 0.5\n","Index: 168, Groundedness score: 1.0\n","Index: 169, Groundedness score: 0.5\n","Saved batch ending at index 169 to eval_result/llama_3b_groundedness.csv\n","Index: 170, Groundedness score: 0.75\n","Index: 171, Groundedness score: 1.0\n","Index: 172, Groundedness score: 0.0\n","Index: 173, Groundedness score: 1.0\n","Index: 174, Groundedness score: 0.5\n","Index: 175, Groundedness score: 0.5\n","Index: 176, Groundedness score: 0.0\n","Index: 177, Groundedness score: 0.5\n","Index: 178, Groundedness score: 1.0\n","Index: 179, Groundedness score: 0.5\n","Saved batch ending at index 179 to eval_result/llama_3b_groundedness.csv\n","Index: 180, Groundedness score: 0.75\n","Index: 181, Groundedness score: 1.0\n","Index: 182, Groundedness score: 1.0\n","Index: 183, Groundedness score: 1.0\n","Index: 184, Groundedness score: 0.0\n","Index: 185, Groundedness score: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2209.77ms\n"]},{"output_type":"stream","name":"stdout","text":["Index: 186, Groundedness score: 0.0\n","Index: 187, Groundedness score: 1.0\n","Index: 188, Groundedness score: 1.0\n","Index: 189, Groundedness score: 0.5\n","Saved batch ending at index 189 to eval_result/llama_3b_groundedness.csv\n","Index: 190, Groundedness score: 1.0\n","Index: 191, Groundedness score: 0.5\n","Index: 192, Groundedness score: 0.75\n","Index: 193, Groundedness score: 0.5\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1900.28ms\n"]},{"output_type":"stream","name":"stdout","text":["Index: 194, Groundedness score: 1.0\n","Index: 195, Groundedness score: 1.0\n","Index: 196, Groundedness score: 1.0\n","Index: 197, Groundedness score: 0.5\n","Index: 198, Groundedness score: 0.5\n","Index: 199, Groundedness score: 1.0\n","Saved batch ending at index 199 to eval_result/llama_3b_groundedness.csv\n","Index: 200, Groundedness score: 1.0\n","Index: 201, Groundedness score: 0.0\n","Index: 202, Groundedness score: 1.0\n","Index: 203, Groundedness score: 1.0\n","Index: 204, Groundedness score: 0.0\n","Index: 205, Groundedness score: 0.5\n","Index: 206, Groundedness score: 0.5\n","Index: 207, Groundedness score: 0.25\n","Index: 208, Groundedness score: 0.5\n","Index: 209, Groundedness score: 0.5\n","Saved batch ending at index 209 to eval_result/llama_3b_groundedness.csv\n","Index: 210, Groundedness score: 1.0\n","Index: 211, Groundedness score: 1.0\n","Index: 212, Groundedness score: 1.0\n","Index: 213, Groundedness score: 0.5\n","Index: 214, Groundedness score: 0.0\n","Index: 215, Groundedness score: 0.25\n","Index: 216, Groundedness score: 0.0\n","Index: 217, Groundedness score: 1.0\n","Index: 218, Groundedness score: 0.25\n","Index: 219, Groundedness score: 0.75\n","Saved batch ending at index 219 to eval_result/llama_3b_groundedness.csv\n","Index: 220, Groundedness score: 0.25\n","Index: 221, Groundedness score: 1.0\n","Index: 222, Groundedness score: 0.5\n","Index: 223, Groundedness score: 0.5\n","Index: 224, Groundedness score: 0.5\n","Index: 225, Groundedness score: 1.0\n","Index: 226, Groundedness score: 0.5\n","Index: 227, Groundedness score: 0.0\n","Index: 228, Groundedness score: 1.0\n","Index: 229, Groundedness score: 0.0\n","Saved batch ending at index 229 to eval_result/llama_3b_groundedness.csv\n","Index: 230, Groundedness score: 1.0\n","Index: 231, Groundedness score: 1.0\n","Index: 232, Groundedness score: 0.5\n","Index: 233, Groundedness score: 0.5\n","Index: 234, Groundedness score: 1.0\n","Index: 235, Groundedness score: 1.0\n","Index: 236, Groundedness score: 1.0\n","Index: 237, Groundedness score: 1.0\n","Index: 238, Groundedness score: 0.25\n","Index: 239, Groundedness score: 0.75\n","Saved batch ending at index 239 to eval_result/llama_3b_groundedness.csv\n","Index: 240, Groundedness score: 0.25\n","Index: 241, Groundedness score: 1.0\n","Index: 242, Groundedness score: 0.0\n","Index: 243, Groundedness score: 0.5\n","Index: 244, Groundedness score: 0.5\n","Index: 245, Groundedness score: 1.0\n","Index: 246, Groundedness score: 1.0\n","Index: 247, Groundedness score: 0.5\n","Index: 248, Groundedness score: 0.0\n","Index: 249, Groundedness score: 1.0\n","Saved batch ending at index 249 to eval_result/llama_3b_groundedness.csv\n","Index: 250, Groundedness score: 0.0\n","Index: 251, Groundedness score: 0.25\n","Index: 252, Groundedness score: 0.5\n","Index: 253, Groundedness score: 0.25\n","Index: 254, Groundedness score: 0.0\n","Index: 255, Groundedness score: 1.0\n","Index: 256, Groundedness score: 0.5\n","Index: 257, Groundedness score: 0.25\n","Index: 258, Groundedness score: 0.0\n","Index: 259, Groundedness score: 1.0\n","Saved batch ending at index 259 to eval_result/llama_3b_groundedness.csv\n","Index: 260, Groundedness score: 0.5\n","Index: 261, Groundedness score: 0.25\n","Index: 262, Groundedness score: 1.0\n","Index: 263, Groundedness score: 0.5\n","Index: 264, Groundedness score: 0.0\n","Index: 265, Groundedness score: 0.5\n","Index: 266, Groundedness score: 1.0\n","Index: 267, Groundedness score: 1.0\n","Index: 268, Groundedness score: 1.0\n","Index: 269, Groundedness score: 0.0\n","Saved batch ending at index 269 to eval_result/llama_3b_groundedness.csv\n","Index: 270, Groundedness score: 0.0\n","Index: 271, Groundedness score: 0.0\n","Index: 272, Groundedness score: 0.5\n","Index: 273, Groundedness score: 0.5\n","Index: 274, Groundedness score: 1.0\n","Index: 275, Groundedness score: 1.0\n","Index: 276, Groundedness score: 0.5\n","Index: 277, Groundedness score: 1.0\n","Index: 278, Groundedness score: 0.0\n","Index: 279, Groundedness score: 1.0\n","Saved batch ending at index 279 to eval_result/llama_3b_groundedness.csv\n","Index: 280, Groundedness score: 1.0\n","Index: 281, Groundedness score: 0.5\n","Index: 282, Groundedness score: 1.0\n","Index: 283, Groundedness score: 1.0\n","Index: 284, Groundedness score: 1.0\n","Index: 285, Groundedness score: 0.5\n","Index: 286, Groundedness score: 1.0\n","Index: 287, Groundedness score: 0.5\n","Index: 288, Groundedness score: 0.5\n","Index: 289, Groundedness score: 1.0\n","Saved batch ending at index 289 to eval_result/llama_3b_groundedness.csv\n","Index: 290, Groundedness score: 0.5\n","Index: 291, Groundedness score: 0.0\n","Index: 292, Groundedness score: 0.0\n","Index: 293, Groundedness score: 0.5\n","Index: 294, Groundedness score: 0.0\n","Index: 295, Groundedness score: 0.25\n","Index: 296, Groundedness score: 0.5\n","Index: 297, Groundedness score: 0.5\n","Index: 298, Groundedness score: 0.5\n","Index: 299, Groundedness score: 0.5\n","Saved batch ending at index 299 to eval_result/llama_3b_groundedness.csv\n","\n","=== Groundedness Score Distribution ===\n","Score 0.0: 47 responses\n","Score 0.25: 21 responses\n","Score 0.5: 93 responses\n","Score 0.75: 14 responses\n","Score 1.0: 125 responses\n","\n","Total Groundedness score (sum): 187.25\n"]}]},{"cell_type":"markdown","source":["### Qwen2-7b"],"metadata":{"id":"XQE0hyZS0ykh"},"id":"XQE0hyZS0ykh"},{"cell_type":"code","source":["import os\n","from dotenv import load_dotenv\n","import pandas as pd\n","import google.generativeai as genai\n","from collections import Counter\n","from time import sleep\n","\n","# -------------------------------\n","# 1️⃣ Load API key từ .env\n","# -------------------------------\n","load_dotenv()\n","api_key = os.getenv(\"GEMINI_API_KEY\")\n","genai.configure(api_key=api_key)\n","\n","# -------------------------------\n","# 2️⃣ Tạo Gemini 2.5 Flash instance\n","# -------------------------------\n","model = genai.GenerativeModel(\"gemini-2.5-pro\")\n","\n","# -------------------------------\n","# 3️⃣ Prompt functions (Judge 1 & Judge 2)\n","# -------------------------------\n","def response_groundedness_judge1_prompt(response: str, context: str) -> str:\n","    return f\"\"\"### Instruction\n","You are a world class expert designed to evaluate the groundedness of an assertion.\n","You will be provided with an assertion and a context.\n","Your task is to determine if the assertion is supported by the context.\n","Follow the instructions below:\n","A. If the assertion or context is empty, say 0.\n","B. If the assertion is not supported by the context, say 0.\n","C. If the assertion is partially supported by the context, say 1.\n","D. If the assertion is fully supported by the context, say 2.\n","You must provide a rating of 0, 1, or 2, nothing else.\n","\n","### Context:\n","<{context}>\n","\n","### Assertion:\n","<{response}>\n","\n","Analyzing Context and Response, the Groundedness score is \"\"\"\n","\n","def response_groundedness_judge2_prompt(response: str, context: str) -> str:\n","    return f\"\"\"As a specialist in assessing the strength of connections between statements and their given contexts, I will evaluate the level of support an assertion receives from the provided context. Follow these guidelines:\n","\n","* If the assertion or context is empty or assertion is not supported, assign a score of 0.\n","* If the assertion is partially supported, assign a score of 1.\n","* If the assertion is fully supported, assign a score of 2.\n","\n","I will provide a rating of 0, 1, or 2, without any additional information.\n","\n","---\n","**Context:**\n","[{context}]\n","\n","**Assertion:**\n","[{response}]\n","\n","Do not explain. Based on the provided context and response, the Groundedness score is:\"\"\"\n","\n","# -------------------------------\n","# 4️⃣ Helper function gọi Gemini Flash và chuẩn hóa\n","# -------------------------------\n","def call_gemini(prompt: str) -> float:\n","    \"\"\"Gọi Gemini Flash, parse output thành float 0,1,2\"\"\"\n","    try:\n","        response = model.generate_content(prompt)\n","        text = response.text.strip()\n","        score = int(text)\n","        if score in [0, 1, 2]:\n","            return score\n","    except Exception as e:\n","        print(f\"[ERROR call_gemini]: {e}\")\n","    return None\n","\n","def compute_groundedness(response: str, context: str) -> float:\n","    \"\"\"Tính điểm groundedness dựa trên 2 judge\"\"\"\n","    score1 = call_gemini(response_groundedness_judge1_prompt(response, context))\n","    score2 = call_gemini(response_groundedness_judge2_prompt(response, context))\n","\n","    normalized_scores = []\n","    if score1 is not None:\n","        normalized_scores.append(score1 / 2)\n","    if score2 is not None:\n","        normalized_scores.append(score2 / 2)\n","\n","    if not normalized_scores:\n","        return 0.0\n","    return sum(normalized_scores) / len(normalized_scores)\n","\n","# -------------------------------\n","# 5️⃣ Load data từ CSV (toàn bộ, không sample)\n","# -------------------------------\n","csv_path = \"data/qwen2_7b_eval_data.csv\"\n","df = pd.read_csv(csv_path)\n","\n","# -------------------------------\n","# 6️⃣ Chuẩn bị output\n","# -------------------------------\n","os.makedirs(\"eval_result\", exist_ok=True)\n","output_csv = \"eval_result/qwen2_7b_groundedness.csv\"\n","\n","# Nếu file đã tồn tại, xóa để ghi batch mới\n","if os.path.exists(output_csv):\n","    os.remove(output_csv)\n","\n","# -------------------------------\n","# 7️⃣ Chạy evaluation theo batch 10 dòng\n","# -------------------------------\n","BATCH_SIZE = 10\n","batch_results = []\n","\n","for idx, row in df.iterrows():\n","    response = str(row[\"response\"])\n","    context = str(row[\"context\"])\n","    score = compute_groundedness(response, context)\n","    print(f\"Index: {idx}, Groundedness score: {score}\")\n","\n","    batch_results.append({\n","        \"question\": row.get(\"question\", \"\"),\n","        \"context\": context,\n","        \"response\": response,\n","        \"groundedness_score\": score\n","    })\n","\n","    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n","    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n","        df_batch = pd.DataFrame(batch_results)\n","        # Append nếu file đã tồn tại, else write mới\n","        if os.path.exists(output_csv):\n","            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n","        else:\n","            df_batch.to_csv(output_csv, index=False)\n","        batch_results = []  # reset batch\n","        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n","\n","# -------------------------------\n","# 8️⃣ Thống kê tổng số điểm\n","# -------------------------------\n","df_final = pd.read_csv(output_csv)\n","score_counts = Counter(df_final[\"groundedness_score\"])\n","print(\"\\n=== Groundedness Score Distribution ===\")\n","for score_level in sorted(score_counts.keys()):\n","    print(f\"Score {score_level}: {score_counts[score_level]} responses\")\n","\n","total_score = df_final[\"groundedness_score\"].sum()\n","print(f\"\\nTotal Groundedness score (sum): {total_score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"C0YsoqVO1zuo","executionInfo":{"status":"ok","timestamp":1764628496872,"user_tz":-420,"elapsed":5986636,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"1dc4c781-84bd-446a-9df3-1bed9d95e8c1"},"id":"C0YsoqVO1zuo","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Index: 0, Groundedness score: 1.0\n","Index: 1, Groundedness score: 0.0\n","Index: 2, Groundedness score: 1.0\n","Index: 3, Groundedness score: 0.75\n","Index: 4, Groundedness score: 1.0\n","Index: 5, Groundedness score: 1.0\n","Index: 6, Groundedness score: 1.0\n","Index: 7, Groundedness score: 1.0\n","Index: 8, Groundedness score: 1.0\n","Index: 9, Groundedness score: 1.0\n","Saved batch ending at index 9 to eval_result/qwen2_7b_groundedness.csv\n","Index: 10, Groundedness score: 0.5\n","Index: 11, Groundedness score: 1.0\n","Index: 12, Groundedness score: 0.0\n","Index: 13, Groundedness score: 0.0\n","Index: 14, Groundedness score: 1.0\n","Index: 15, Groundedness score: 1.0\n","Index: 16, Groundedness score: 1.0\n","Index: 17, Groundedness score: 1.0\n","Index: 18, Groundedness score: 1.0\n","Index: 19, Groundedness score: 0.5\n","Saved batch ending at index 19 to eval_result/qwen2_7b_groundedness.csv\n","Index: 20, Groundedness score: 0.5\n","Index: 21, Groundedness score: 0.0\n","Index: 22, Groundedness score: 1.0\n","Index: 23, Groundedness score: 1.0\n","Index: 24, Groundedness score: 1.0\n","Index: 25, Groundedness score: 0.0\n","Index: 26, Groundedness score: 1.0\n","Index: 27, Groundedness score: 0.5\n","Index: 28, Groundedness score: 1.0\n","Index: 29, Groundedness score: 0.5\n","Saved batch ending at index 29 to eval_result/qwen2_7b_groundedness.csv\n","Index: 30, Groundedness score: 0.0\n","Index: 31, Groundedness score: 0.0\n","Index: 32, Groundedness score: 1.0\n","Index: 33, Groundedness score: 0.5\n","Index: 34, Groundedness score: 1.0\n","Index: 35, Groundedness score: 1.0\n","Index: 36, Groundedness score: 1.0\n","Index: 37, Groundedness score: 1.0\n","Index: 38, Groundedness score: 1.0\n","Index: 39, Groundedness score: 1.0\n","Saved batch ending at index 39 to eval_result/qwen2_7b_groundedness.csv\n","Index: 40, Groundedness score: 1.0\n","Index: 41, Groundedness score: 1.0\n","Index: 42, Groundedness score: 1.0\n","Index: 43, Groundedness score: 0.0\n","Index: 44, Groundedness score: 0.75\n","Index: 45, Groundedness score: 1.0\n","Index: 46, Groundedness score: 1.0\n","Index: 47, Groundedness score: 1.0\n","Index: 48, Groundedness score: 0.0\n","Index: 49, Groundedness score: 0.25\n","Saved batch ending at index 49 to eval_result/qwen2_7b_groundedness.csv\n","Index: 50, Groundedness score: 1.0\n","Index: 51, Groundedness score: 0.5\n","Index: 52, Groundedness score: 1.0\n","Index: 53, Groundedness score: 1.0\n","Index: 54, Groundedness score: 1.0\n","Index: 55, Groundedness score: 0.0\n","Index: 56, Groundedness score: 1.0\n","Index: 57, Groundedness score: 1.0\n","Index: 58, Groundedness score: 0.0\n","Index: 59, Groundedness score: 1.0\n","Saved batch ending at index 59 to eval_result/qwen2_7b_groundedness.csv\n","Index: 60, Groundedness score: 1.0\n","Index: 61, Groundedness score: 1.0\n","Index: 62, Groundedness score: 1.0\n","Index: 63, Groundedness score: 1.0\n","Index: 64, Groundedness score: 0.75\n","Index: 65, Groundedness score: 1.0\n","Index: 66, Groundedness score: 1.0\n","Index: 67, Groundedness score: 1.0\n","Index: 68, Groundedness score: 1.0\n","Index: 69, Groundedness score: 1.0\n","Saved batch ending at index 69 to eval_result/qwen2_7b_groundedness.csv\n","Index: 70, Groundedness score: 1.0\n","Index: 71, Groundedness score: 1.0\n","Index: 72, Groundedness score: 1.0\n","Index: 73, Groundedness score: 1.0\n","Index: 74, Groundedness score: 1.0\n","Index: 75, Groundedness score: 0.0\n","Index: 76, Groundedness score: 0.5\n","Index: 77, Groundedness score: 1.0\n","Index: 78, Groundedness score: 1.0\n","Index: 79, Groundedness score: 1.0\n","Saved batch ending at index 79 to eval_result/qwen2_7b_groundedness.csv\n","Index: 80, Groundedness score: 1.0\n","Index: 81, Groundedness score: 1.0\n","Index: 82, Groundedness score: 0.0\n","Index: 83, Groundedness score: 1.0\n","Index: 84, Groundedness score: 0.25\n","Index: 85, Groundedness score: 1.0\n","Index: 86, Groundedness score: 1.0\n","Index: 87, Groundedness score: 1.0\n","Index: 88, Groundedness score: 0.5\n","Index: 89, Groundedness score: 0.5\n","Saved batch ending at index 89 to eval_result/qwen2_7b_groundedness.csv\n","Index: 90, Groundedness score: 1.0\n","Index: 91, Groundedness score: 1.0\n","Index: 92, Groundedness score: 0.0\n","Index: 93, Groundedness score: 0.75\n","Index: 94, Groundedness score: 1.0\n","Index: 95, Groundedness score: 0.0\n","Index: 96, Groundedness score: 1.0\n","Index: 97, Groundedness score: 1.0\n","Index: 98, Groundedness score: 1.0\n","Index: 99, Groundedness score: 0.5\n","Saved batch ending at index 99 to eval_result/qwen2_7b_groundedness.csv\n","Index: 100, Groundedness score: 0.5\n","Index: 101, Groundedness score: 1.0\n","Index: 102, Groundedness score: 0.25\n","Index: 103, Groundedness score: 0.75\n","Index: 104, Groundedness score: 0.0\n","Index: 105, Groundedness score: 0.0\n","Index: 106, Groundedness score: 1.0\n","Index: 107, Groundedness score: 1.0\n","Index: 108, Groundedness score: 1.0\n","Index: 109, Groundedness score: 0.5\n","Saved batch ending at index 109 to eval_result/qwen2_7b_groundedness.csv\n","Index: 110, Groundedness score: 1.0\n","Index: 111, Groundedness score: 1.0\n","Index: 112, Groundedness score: 1.0\n","Index: 113, Groundedness score: 1.0\n","Index: 114, Groundedness score: 1.0\n","Index: 115, Groundedness score: 1.0\n","Index: 116, Groundedness score: 1.0\n","Index: 117, Groundedness score: 1.0\n","Index: 118, Groundedness score: 1.0\n","Index: 119, Groundedness score: 1.0\n","Saved batch ending at index 119 to eval_result/qwen2_7b_groundedness.csv\n","Index: 120, Groundedness score: 1.0\n","Index: 121, Groundedness score: 1.0\n","Index: 122, Groundedness score: 0.75\n","Index: 123, Groundedness score: 0.5\n","Index: 124, Groundedness score: 1.0\n","Index: 125, Groundedness score: 0.5\n","Index: 126, Groundedness score: 1.0\n","Index: 127, Groundedness score: 0.0\n","Index: 128, Groundedness score: 0.0\n","Index: 129, Groundedness score: 1.0\n","Saved batch ending at index 129 to eval_result/qwen2_7b_groundedness.csv\n","Index: 130, Groundedness score: 0.0\n","Index: 131, Groundedness score: 0.5\n","Index: 132, Groundedness score: 1.0\n","Index: 133, Groundedness score: 1.0\n","Index: 134, Groundedness score: 1.0\n","Index: 135, Groundedness score: 1.0\n","Index: 136, Groundedness score: 1.0\n","Index: 137, Groundedness score: 0.5\n","Index: 138, Groundedness score: 1.0\n","Index: 139, Groundedness score: 1.0\n","Saved batch ending at index 139 to eval_result/qwen2_7b_groundedness.csv\n","Index: 140, Groundedness score: 0.5\n","Index: 141, Groundedness score: 1.0\n","Index: 142, Groundedness score: 0.0\n","Index: 143, Groundedness score: 1.0\n","Index: 144, Groundedness score: 1.0\n","Index: 145, Groundedness score: 1.0\n","Index: 146, Groundedness score: 1.0\n","Index: 147, Groundedness score: 1.0\n","Index: 148, Groundedness score: 1.0\n","Index: 149, Groundedness score: 1.0\n","Saved batch ending at index 149 to eval_result/qwen2_7b_groundedness.csv\n","Index: 150, Groundedness score: 1.0\n","Index: 151, Groundedness score: 1.0\n","Index: 152, Groundedness score: 0.25\n","Index: 153, Groundedness score: 1.0\n","Index: 154, Groundedness score: 0.0\n","Index: 155, Groundedness score: 1.0\n","Index: 156, Groundedness score: 1.0\n","Index: 157, Groundedness score: 1.0\n","Index: 158, Groundedness score: 1.0\n","Index: 159, Groundedness score: 1.0\n","Saved batch ending at index 159 to eval_result/qwen2_7b_groundedness.csv\n","Index: 160, Groundedness score: 1.0\n","Index: 161, Groundedness score: 0.0\n","Index: 162, Groundedness score: 1.0\n","Index: 163, Groundedness score: 1.0\n","Index: 164, Groundedness score: 1.0\n","Index: 165, Groundedness score: 1.0\n","Index: 166, Groundedness score: 1.0\n","Index: 167, Groundedness score: 1.0\n","Index: 168, Groundedness score: 1.0\n","Index: 169, Groundedness score: 1.0\n","Saved batch ending at index 169 to eval_result/qwen2_7b_groundedness.csv\n","Index: 170, Groundedness score: 1.0\n","Index: 171, Groundedness score: 1.0\n","Index: 172, Groundedness score: 0.0\n","Index: 173, Groundedness score: 1.0\n","Index: 174, Groundedness score: 1.0\n","Index: 175, Groundedness score: 1.0\n","Index: 176, Groundedness score: 0.75\n","Index: 177, Groundedness score: 1.0\n","Index: 178, Groundedness score: 1.0\n","Index: 179, Groundedness score: 1.0\n","Saved batch ending at index 179 to eval_result/qwen2_7b_groundedness.csv\n","Index: 180, Groundedness score: 1.0\n","Index: 181, Groundedness score: 1.0\n","Index: 182, Groundedness score: 0.5\n","Index: 183, Groundedness score: 1.0\n","Index: 184, Groundedness score: 1.0\n","Index: 185, Groundedness score: 0.0\n","Index: 186, Groundedness score: 0.25\n","Index: 187, Groundedness score: 1.0\n","Index: 188, Groundedness score: 0.5\n","Index: 189, Groundedness score: 0.0\n","Saved batch ending at index 189 to eval_result/qwen2_7b_groundedness.csv\n","Index: 190, Groundedness score: 0.0\n","Index: 191, Groundedness score: 1.0\n","Index: 192, Groundedness score: 1.0\n","Index: 193, Groundedness score: 1.0\n","Index: 194, Groundedness score: 0.25\n","Index: 195, Groundedness score: 1.0\n","Index: 196, Groundedness score: 1.0\n","Index: 197, Groundedness score: 1.0\n","Index: 198, Groundedness score: 1.0\n","Index: 199, Groundedness score: 1.0\n","Saved batch ending at index 199 to eval_result/qwen2_7b_groundedness.csv\n","Index: 200, Groundedness score: 1.0\n","Index: 201, Groundedness score: 0.0\n","Index: 202, Groundedness score: 1.0\n","Index: 203, Groundedness score: 1.0\n","Index: 204, Groundedness score: 1.0\n","Index: 205, Groundedness score: 1.0\n","Index: 206, Groundedness score: 1.0\n","Index: 207, Groundedness score: 1.0\n","Index: 208, Groundedness score: 0.75\n","Index: 209, Groundedness score: 1.0\n","Saved batch ending at index 209 to eval_result/qwen2_7b_groundedness.csv\n","Index: 210, Groundedness score: 1.0\n","Index: 211, Groundedness score: 1.0\n","Index: 212, Groundedness score: 1.0\n","Index: 213, Groundedness score: 1.0\n","Index: 214, Groundedness score: 0.0\n","Index: 215, Groundedness score: 0.0\n","Index: 216, Groundedness score: 0.0\n","Index: 217, Groundedness score: 1.0\n","Index: 218, Groundedness score: 0.0\n","Index: 219, Groundedness score: 1.0\n","Saved batch ending at index 219 to eval_result/qwen2_7b_groundedness.csv\n","Index: 220, Groundedness score: 0.0\n","Index: 221, Groundedness score: 1.0\n","Index: 222, Groundedness score: 1.0\n","Index: 223, Groundedness score: 1.0\n","Index: 224, Groundedness score: 1.0\n","Index: 225, Groundedness score: 1.0\n","Index: 226, Groundedness score: 1.0\n","Index: 227, Groundedness score: 1.0\n","Index: 228, Groundedness score: 1.0\n","Index: 229, Groundedness score: 1.0\n","Saved batch ending at index 229 to eval_result/qwen2_7b_groundedness.csv\n","Index: 230, Groundedness score: 1.0\n","Index: 231, Groundedness score: 1.0\n","Index: 232, Groundedness score: 1.0\n","Index: 233, Groundedness score: 1.0\n","Index: 234, Groundedness score: 1.0\n","Index: 235, Groundedness score: 1.0\n","Index: 236, Groundedness score: 1.0\n","Index: 237, Groundedness score: 1.0\n","Index: 238, Groundedness score: 1.0\n","Index: 239, Groundedness score: 0.0\n","Saved batch ending at index 239 to eval_result/qwen2_7b_groundedness.csv\n","Index: 240, Groundedness score: 1.0\n","Index: 241, Groundedness score: 1.0\n","Index: 242, Groundedness score: 0.5\n","Index: 243, Groundedness score: 1.0\n","Index: 244, Groundedness score: 0.0\n","Index: 245, Groundedness score: 0.5\n","Index: 246, Groundedness score: 1.0\n","Index: 247, Groundedness score: 1.0\n","Index: 248, Groundedness score: 0.0\n","Index: 249, Groundedness score: 1.0\n","Saved batch ending at index 249 to eval_result/qwen2_7b_groundedness.csv\n","Index: 250, Groundedness score: 0.0\n","Index: 251, Groundedness score: 1.0\n","Index: 252, Groundedness score: 1.0\n","Index: 253, Groundedness score: 0.5\n","Index: 254, Groundedness score: 0.0\n","Index: 255, Groundedness score: 1.0\n","Index: 256, Groundedness score: 1.0\n","Index: 257, Groundedness score: 0.0\n","Index: 258, Groundedness score: 0.0\n","Index: 259, Groundedness score: 0.0\n","Saved batch ending at index 259 to eval_result/qwen2_7b_groundedness.csv\n","Index: 260, Groundedness score: 1.0\n","Index: 261, Groundedness score: 0.0\n","Index: 262, Groundedness score: 0.5\n","Index: 263, Groundedness score: 1.0\n","Index: 264, Groundedness score: 1.0\n","Index: 265, Groundedness score: 1.0\n","Index: 266, Groundedness score: 1.0\n","Index: 267, Groundedness score: 0.75\n","Index: 268, Groundedness score: 1.0\n","Index: 269, Groundedness score: 0.25\n","Saved batch ending at index 269 to eval_result/qwen2_7b_groundedness.csv\n","Index: 270, Groundedness score: 0.0\n","Index: 271, Groundedness score: 0.0\n","Index: 272, Groundedness score: 1.0\n","Index: 273, Groundedness score: 1.0\n","Index: 274, Groundedness score: 1.0\n","Index: 275, Groundedness score: 1.0\n","Index: 276, Groundedness score: 1.0\n","Index: 277, Groundedness score: 0.75\n","Index: 278, Groundedness score: 0.0\n","Index: 279, Groundedness score: 1.0\n","Saved batch ending at index 279 to eval_result/qwen2_7b_groundedness.csv\n","Index: 280, Groundedness score: 1.0\n","Index: 281, Groundedness score: 1.0\n","Index: 282, Groundedness score: 1.0\n","Index: 283, Groundedness score: 1.0\n","Index: 284, Groundedness score: 1.0\n","Index: 285, Groundedness score: 1.0\n","Index: 286, Groundedness score: 1.0\n","Index: 287, Groundedness score: 1.0\n","Index: 288, Groundedness score: 0.75\n","Index: 289, Groundedness score: 1.0\n","Saved batch ending at index 289 to eval_result/qwen2_7b_groundedness.csv\n","Index: 290, Groundedness score: 1.0\n","Index: 291, Groundedness score: 0.0\n","Index: 292, Groundedness score: 0.5\n","Index: 293, Groundedness score: 1.0\n","Index: 294, Groundedness score: 1.0\n","Index: 295, Groundedness score: 1.0\n","Index: 296, Groundedness score: 1.0\n","Index: 297, Groundedness score: 1.0\n","Index: 298, Groundedness score: 1.0\n","Index: 299, Groundedness score: 1.0\n","Saved batch ending at index 299 to eval_result/qwen2_7b_groundedness.csv\n","\n","=== Groundedness Score Distribution ===\n","Score 0.0: 46 responses\n","Score 0.25: 7 responses\n","Score 0.5: 25 responses\n","Score 0.75: 11 responses\n","Score 1.0: 211 responses\n","\n","Total Groundedness score (sum): 233.5\n"]}]},{"cell_type":"markdown","id":"NCxTdjXCzSN2","metadata":{"id":"NCxTdjXCzSN2"},"source":["# Faithfulness score"]},{"cell_type":"markdown","source":["### Qwen2-3b"],"metadata":{"id":"MG9O7-UyKAv3"},"id":"MG9O7-UyKAv3"},{"cell_type":"code","source":["# eval_faithfulness_flash.py\n","import os\n","from dotenv import load_dotenv\n","import pandas as pd\n","import google.generativeai as genai\n","import json\n","from tqdm import tqdm\n","import typing as t\n","\n","# -------------------------------\n","# 1️⃣ Load API key từ .env\n","# -------------------------------\n","load_dotenv()\n","api_key = os.getenv(\"GEMINI_API_KEY\")\n","genai.configure(api_key=api_key)\n","\n","# -------------------------------\n","# 2️⃣ Tạo Gemini 2.5 Flash instance\n","# -------------------------------\n","model = genai.GenerativeModel(\"gemini-2.5-flash\")\n","\n","# -------------------------------\n","# 3️⃣ Prompt functions (Statement Generator & NLI Judge)\n","# -------------------------------\n","def statement_generator_prompt(question: str, answer: str) -> str:\n","    \"\"\"\n","    V1-identical statement generator - matches PydanticPrompt.to_string() exactly.\n","\n","    Args:\n","        question: The question being answered\n","        answer: The answer text to break down into statements\n","\n","    Returns:\n","        V1-identical prompt string for the LLM\n","    \"\"\"\n","    # Format inputs exactly like V1's model_dump_json(indent=4, exclude_none=True)\n","    safe_question = json.dumps(question)\n","    safe_answer = json.dumps(answer)\n","\n","    return f\"\"\"Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\n","Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n","{{\"properties\": {{\"statements\": {{\"description\": \"The generated statements\", \"items\": {{\"type\": \"string\"}}, \"title\": \"Statements\", \"type\": \"array\"}}}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n","\n","--------EXAMPLES-----------\n","Example 1\n","Input: {{\n","    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n","    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n","}}\n","Output: {{\n","    \"statements\": [\n","        \"Albert Einstein was a German-born theoretical physicist.\",\n","        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\n","        \"Albert Einstein was best known for developing the theory of relativity.\",\n","        \"Albert Einstein made important contributions to the development of the theory of quantum mechanics.\"\n","    ]\n","}}\n","-----------------------------\n","\n","Now perform the same with the following input\n","input: {{\n","    \"question\": {safe_question},\n","    \"answer\": {safe_answer}\n","}}\n","Output: \"\"\"\n","\n","def nli_statement_prompt(context: str, statements: t.List[str]) -> str:\n","    \"\"\"\n","    V1-identical NLI statement evaluation - matches PydanticPrompt.to_string() exactly.\n","\n","    Args:\n","        context: The context to evaluate statements against\n","        statements: The statements to judge for faithfulness\n","\n","    Returns:\n","        V1-identical prompt string for the LLM\n","    \"\"\"\n","    # Format inputs exactly like V1's model_dump_json(indent=4, exclude_none=True)\n","    safe_context = json.dumps(context)\n","    safe_statements = json.dumps(statements, indent=4).replace(\"\\n\", \"\\n    \")\n","\n","    return f\"\"\"Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n","Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n","{{\"$defs\": {{\"StatementFaithfulnessAnswer\": {{\"properties\": {{\"statement\": {{\"description\": \"the original statement, word-by-word\", \"title\": \"Statement\", \"type\": \"string\"}}, \"reason\": {{\"description\": \"the reason of the verdict\", \"title\": \"Reason\", \"type\": \"string\"}}, \"verdict\": {{\"description\": \"the verdict(0/1) of the faithfulness.\", \"title\": \"Verdict\", \"type\": \"integer\"}}}}, \"required\": [\"statement\", \"reason\", \"verdict\"], \"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\"}}}}, \"properties\": {{\"statements\": {{\"items\": {{\"$ref\": \"#/$defs/StatementFaithfulnessAnswer\"}}, \"title\": \"Statements\", \"type\": \"array\"}}}}, \"required\": [\"statements\"], \"title\": \"NLIStatementOutput\", \"type\": \"object\"}}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n","\n","--------EXAMPLES-----------\n","Example 1\n","Input: {{\n","    \"context\": \"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\",\n","    \"statements\": [\n","        \"John is majoring in Biology.\",\n","        \"John is taking a course on Artificial Intelligence.\",\n","        \"John is a dedicated student.\",\n","        \"John has a part-time job.\"\n","    ]\n","}}\n","Output: {{\n","    \"statements\": [\n","        {{\n","            \"statement\": \"John is majoring in Biology.\",\n","            \"reason\": \"John's major is explicitly stated as Computer Science, not Biology.\",\n","            \"verdict\": 0\n","        }},\n","        {{\n","            \"statement\": \"John is taking a course on Artificial Intelligence.\",\n","            \"reason\": \"The context mentions courses in Data Structures, Algorithms, and Database Management, but does not mention Artificial Intelligence.\",\n","            \"verdict\": 0\n","        }},\n","        {{\n","            \"statement\": \"John is a dedicated student.\",\n","            \"reason\": \"The context states that John is a diligent student who spends a significant amount of time studying and completing assignments.\",\n","            \"verdict\": 1\n","        }},\n","        {{\n","            \"statement\": \"John has a part-time job.\",\n","            \"reason\": \"There is no information in the context about John having a part-time job.\",\n","            \"verdict\": 0\n","        }}\n","    ]\n","}}\n","-----------------------------\n","\n","Now perform the same with the following input\n","input: {{\n","    \"context\": {safe_context},\n","    \"statements\": {safe_statements}\n","}}\n","Output: \"\"\"\n","# -------------------------------\n","# 4️⃣ Helper functions gọi Gemini Flash\n","# -------------------------------\n","def generate_statements(question: str, answer: str) -> list:\n","    prompt = statement_generator_prompt(question, answer)\n","    resp = model.generate_content(prompt)\n","    try:\n","        data = json.loads(resp.text.strip())\n","        return data.get(\"statements\", [])\n","    except:\n","        return []\n","\n","def judge_statements(context: str, statements: list) -> list:\n","    if not statements:\n","        return []\n","    prompt = nli_statement_prompt(context, statements)\n","    resp = model.generate_content(prompt)\n","    try:\n","        data = json.loads(resp.text.strip())\n","        verdicts = [s.get(\"verdict\", 0) for s in data.get(\"statements\", [])]\n","        return verdicts\n","    except:\n","        return [0] * len(statements)\n","\n","def compute_faithfulness(question: str, response: str, context: str) -> float:\n","    statements = generate_statements(question, response)\n","    verdicts = judge_statements(context, statements)\n","    if not verdicts:\n","        return 0.0\n","    return sum(verdicts) / len(verdicts)\n","\n","# -------------------------------\n","# 5️⃣ Load data CSV (1000 dòng ngẫu nhiên)\n","# -------------------------------\n","csv_path = \"data/merged_all.csv\"\n","df = pd.read_csv(csv_path)\n","df_sample = df.sample(n=10, random_state=42)\n","\n","# -------------------------------\n","# 6️⃣ Chạy evaluation\n","# -------------------------------\n","faithfulness_scores = []\n","\n","for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n","    question = str(row.get(\"response\", \"\"))\n","    response = str(row.get(\"response\", \"\"))\n","    context = str(row.get(\"final_text\", \"\"))\n","    score = compute_faithfulness(question, response, context)\n","    faithfulness_scores.append(score)\n","\n","# -------------------------------\n","# 7️⃣ Thống kê tổng số điểm và phân bố\n","# -------------------------------\n","total_score = sum(faithfulness_scores)\n","print(f\"Total Faithfulness score: {total_score}\")\n","\n","# Optional: phân bố score 0–1\n","import numpy as np\n","score_bins = [0, 0.25, 0.5, 0.75, 1.0]\n","hist, _ = np.histogram(faithfulness_scores, bins=score_bins)\n","for i, bin_edge in enumerate(score_bins[:-1]):\n","    print(f\"Score range {bin_edge}-{score_bins[i+1]}: {hist[i]} responses\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"eVQlUrYKSTzl","executionInfo":{"status":"error","timestamp":1764632424837,"user_tz":-420,"elapsed":6228,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"a2fe049b-8a0d-4f79-c21b-4c8f1aa5aa79"},"id":"eVQlUrYKSTzl","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/10 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4016515065.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_faithfulness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mfaithfulness_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4016515065.py\u001b[0m in \u001b[0;36mcompute_faithfulness\u001b[0;34m(question, response, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_faithfulness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mstatements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_statements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mverdicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjudge_statements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverdicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4016515065.py\u001b[0m in \u001b[0;36mgenerate_statements\u001b[0;34m(question, answer)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_statements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement_generator_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             response = GenerativeServiceRestTransport._GenerateContent._get_response(\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(host, metadata, query_params, session, timeout, transcoded_request, body)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             response = getattr(session, method)(\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LOGGER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    536\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    645\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":3,"id":"yBV7kNIYzbdC","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"elapsed":92084,"status":"error","timestamp":1764634401401,"user":{"displayName":"Dailymate","userId":"09240786015871031354"},"user_tz":-420},"id":"yBV7kNIYzbdC","outputId":"0eeca4be-cf6c-4959-834d-1b3364bbb673"},"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ Resuming from index 61 ...\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/239 [00:09<36:34,  9.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Index: 61, Faithfulness score: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 2/239 [00:13<24:59,  6.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Index: 62, Faithfulness score: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|▏         | 3/239 [00:30<43:49, 11.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Index: 63, Faithfulness score: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 4/239 [00:40<41:46, 10.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Index: 64, Faithfulness score: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 5/239 [00:59<53:48, 13.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Index: 65, Faithfulness score: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 6/239 [01:28<1:13:47, 19.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Index: 66, Faithfulness score: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 6/239 [01:31<59:26, 15.31s/it]  \n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-64659256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_faithfulness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Index: {idx}, Faithfulness score: {score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-64659256.py\u001b[0m in \u001b[0;36mcompute_faithfulness\u001b[0;34m(question, response, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_faithfulness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mstatements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_statements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mverdicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjudge_statements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverdicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-64659256.py\u001b[0m in \u001b[0;36mgenerate_statements\u001b[0;34m(question, answer)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_statements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement_generator_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             response = GenerativeServiceRestTransport._GenerateContent._get_response(\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(host, metadata, query_params, session, timeout, transcoded_request, body)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             response = getattr(session, method)(\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LOGGER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    536\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    645\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# eval_faithfulness_flash_batch.py\n","import os\n","from dotenv import load_dotenv\n","import pandas as pd\n","import google.generativeai as genai\n","import json\n","from tqdm import tqdm\n","import typing as t\n","\n","# -------------------------------\n","# 1️⃣ Load API key từ .env\n","# -------------------------------\n","load_dotenv()\n","api_key = os.getenv(\"GEMINI_API_KEY\")\n","genai.configure(api_key=api_key)\n","\n","# -------------------------------\n","# 2️⃣ Tạo Gemini 2.5 Flash instance\n","# -------------------------------\n","model = genai.GenerativeModel(\"gemini-2.5-flash\")\n","\n","# -------------------------------\n","# 3️⃣ Prompt functions (Statement Generator & NLI Judge)\n","# -------------------------------\n","def statement_generator_prompt(question: str, answer: str) -> str:\n","    \"\"\"\n","    V1-identical statement generator - matches PydanticPrompt.to_string() exactly.\n","\n","    Args:\n","        question: The question being answered\n","        answer: The answer text to break down into statements\n","\n","    Returns:\n","        V1-identical prompt string for the LLM\n","    \"\"\"\n","    # Format inputs exactly like V1's model_dump_json(indent=4, exclude_none=True)\n","    safe_question = json.dumps(question)\n","    safe_answer = json.dumps(answer)\n","\n","    return f\"\"\"Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\n","Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n","{{\"properties\": {{\"statements\": {{\"description\": \"The generated statements\", \"items\": {{\"type\": \"string\"}}, \"title\": \"Statements\", \"type\": \"array\"}}}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n","\n","--------EXAMPLES-----------\n","Example 1\n","Input: {{\n","    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n","    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n","}}\n","Output: {{\n","    \"statements\": [\n","        \"Albert Einstein was a German-born theoretical physicist.\",\n","        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\n","        \"Albert Einstein was best known for developing the theory of relativity.\",\n","        \"Albert Einstein made important contributions to the development of the theory of quantum mechanics.\"\n","    ]\n","}}\n","-----------------------------\n","\n","Now perform the same with the following input\n","input: {{\n","    \"question\": {safe_question},\n","    \"answer\": {safe_answer}\n","}}\n","Output: \"\"\"\n","\n","def nli_statement_prompt(context: str, statements: t.List[str]) -> str:\n","    \"\"\"\n","    V1-identical NLI statement evaluation - matches PydanticPrompt.to_string() exactly.\n","\n","    Args:\n","        context: The context to evaluate statements against\n","        statements: The statements to judge for faithfulness\n","\n","    Returns:\n","        V1-identical prompt string for the LLM\n","    \"\"\"\n","    # Format inputs exactly like V1's model_dump_json(indent=4, exclude_none=True)\n","    safe_context = json.dumps(context)\n","    safe_statements = json.dumps(statements, indent=4).replace(\"\\n\", \"\\n    \")\n","\n","    return f\"\"\"Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n","Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n","{{\"$defs\": {{\"StatementFaithfulnessAnswer\": {{\"properties\": {{\"statement\": {{\"description\": \"the original statement, word-by-word\", \"title\": \"Statement\", \"type\": \"string\"}}, \"reason\": {{\"description\": \"the reason of the verdict\", \"title\": \"Reason\", \"type\": \"string\"}}, \"verdict\": {{\"description\": \"the verdict(0/1) of the faithfulness.\", \"title\": \"Verdict\", \"type\": \"integer\"}}}}, \"required\": [\"statement\", \"reason\", \"verdict\"], \"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\"}}}}, \"properties\": {{\"statements\": {{\"items\": {{\"$ref\": \"#/$defs/StatementFaithfulnessAnswer\"}}, \"title\": \"Statements\", \"type\": \"array\"}}}}, \"required\": [\"statements\"], \"title\": \"NLIStatementOutput\", \"type\": \"object\"}}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n","\n","--------EXAMPLES-----------\n","Example 1\n","Input: {{\n","    \"context\": \"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\",\n","    \"statements\": [\n","        \"John is majoring in Biology.\",\n","        \"John is taking a course on Artificial Intelligence.\",\n","        \"John is a dedicated student.\",\n","        \"John has a part-time job.\"\n","    ]\n","}}\n","Output: {{\n","    \"statements\": [\n","        {{\n","            \"statement\": \"John is majoring in Biology.\",\n","            \"reason\": \"John's major is explicitly stated as Computer Science, not Biology.\",\n","            \"verdict\": 0\n","        }},\n","        {{\n","            \"statement\": \"John is taking a course on Artificial Intelligence.\",\n","            \"reason\": \"The context mentions courses in Data Structures, Algorithms, and Database Management, but does not mention Artificial Intelligence.\",\n","            \"verdict\": 0\n","        }},\n","        {{\n","            \"statement\": \"John is a dedicated student.\",\n","            \"reason\": \"The context states that John is a diligent student who spends a significant amount of time studying and completing assignments.\",\n","            \"verdict\": 1\n","        }},\n","        {{\n","            \"statement\": \"John has a part-time job.\",\n","            \"reason\": \"There is no information in the context about John having a part-time job.\",\n","            \"verdict\": 0\n","        }}\n","    ]\n","}}\n","-----------------------------\n","\n","Now perform the same with the following input\n","input: {{\n","    \"context\": {safe_context},\n","    \"statements\": {safe_statements}\n","}}\n","Output: \"\"\"\n","# -------------------------------\n","# 4️⃣ Helper functions gọi Gemini Flash\n","# -------------------------------\n","def generate_statements(question: str, answer: str) -> list:\n","    prompt = statement_generator_prompt(question, answer)\n","    resp = model.generate_content(prompt)\n","    try:\n","        data = json.loads(resp.text.strip())\n","        return data.get(\"statements\", [])\n","    except:\n","        return []\n","\n","def judge_statements(context: str, statements: list) -> list:\n","    if not statements:\n","        return []\n","    prompt = nli_statement_prompt(context, statements)\n","    resp = model.generate_content(prompt)\n","    try:\n","        data = json.loads(resp.text.strip())\n","        verdicts = [s.get(\"verdict\", 0) for s in data.get(\"statements\", [])]\n","        return verdicts\n","    except:\n","        return [0] * len(statements)\n","\n","def compute_faithfulness(question: str, response: str, context: str) -> float:\n","    statements = generate_statements(question, response)\n","    verdicts = judge_statements(context, statements)\n","    if not verdicts:\n","        return 0.0\n","    return sum(verdicts) / len(verdicts)\n","\n","# -------------------------------\n","# 5️⃣ Load data CSV\n","# -------------------------------\n","csv_path = \"data/qwen2-3b_eval_data.csv\"\n","df = pd.read_csv(csv_path)\n","\n","# -------------------------------\n","# 6️⃣ Chuẩn bị output batch\n","# -------------------------------\n","os.makedirs(\"eval_result\", exist_ok=True)\n","output_csv = \"eval_result/qwen2_3b_faithfulness.csv\"\n","\n","# Không xóa file — vì ta đang resume\n","BATCH_SIZE = 10\n","batch_results = []\n","\n","# Resume từ index 61\n","START_INDEX = 61\n","\n","print(f\"⚠️ Resuming from index {START_INDEX} ...\")\n","\n","# -------------------------------\n","# 7️⃣ Chạy evaluation từ START_INDEX\n","# -------------------------------\n","for idx in tqdm(range(START_INDEX, len(df))):\n","    row = df.iloc[idx]\n","\n","    question = str(row.get(\"question\", \"\"))\n","    response = str(row.get(\"response\", \"\"))\n","    context = str(row.get(\"final_text\", \"\"))\n","\n","    score = compute_faithfulness(question, response, context)\n","    print(f\"Index: {idx}, Faithfulness score: {score}\")\n","\n","    batch_results.append({\n","        \"question\": question,\n","        \"context\": context,\n","        \"response\": response,\n","        \"faithfulness_score\": score\n","    })\n","\n","    # Nếu đủ batch hoặc đến dòng cuối → append vào CSV\n","    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n","        df_batch = pd.DataFrame(batch_results)\n","        df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n","        batch_results = []\n","        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n","\n"]},{"cell_type":"markdown","source":["### Llama 3b"],"metadata":{"id":"le0JdT5-KGdf"},"id":"le0JdT5-KGdf"},{"cell_type":"code","execution_count":null,"id":"RbEOWnVk0LAr","metadata":{"id":"RbEOWnVk0LAr","executionInfo":{"status":"aborted","timestamp":1764633862359,"user_tz":-420,"elapsed":1146014,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}}},"outputs":[],"source":["# eval_faithfulness_flash_batch.py\n","import os\n","from dotenv import load_dotenv\n","import pandas as pd\n","import google.generativeai as genai\n","import json\n","from tqdm import tqdm\n","import typing as t\n","\n","# -------------------------------\n","# 1️⃣ Load API key từ .env\n","# -------------------------------\n","load_dotenv()\n","api_key = os.getenv(\"GEMINI_API_KEY\")\n","genai.configure(api_key=api_key)\n","\n","# -------------------------------\n","# 2️⃣ Tạo Gemini 2.5 Flash instance\n","# -------------------------------\n","model = genai.GenerativeModel(\"gemini-2.5-flash\")\n","\n","# -------------------------------\n","# 3️⃣ Prompt functions (Statement Generator & NLI Judge)\n","# -------------------------------\n","def statement_generator_prompt(question: str, answer: str) -> str:\n","    \"\"\"\n","    V1-identical statement generator - matches PydanticPrompt.to_string() exactly.\n","\n","    Args:\n","        question: The question being answered\n","        answer: The answer text to break down into statements\n","\n","    Returns:\n","        V1-identical prompt string for the LLM\n","    \"\"\"\n","    # Format inputs exactly like V1's model_dump_json(indent=4, exclude_none=True)\n","    safe_question = json.dumps(question)\n","    safe_answer = json.dumps(answer)\n","\n","    return f\"\"\"Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\n","Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n","{{\"properties\": {{\"statements\": {{\"description\": \"The generated statements\", \"items\": {{\"type\": \"string\"}}, \"title\": \"Statements\", \"type\": \"array\"}}}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n","\n","--------EXAMPLES-----------\n","Example 1\n","Input: {{\n","    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n","    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n","}}\n","Output: {{\n","    \"statements\": [\n","        \"Albert Einstein was a German-born theoretical physicist.\",\n","        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\n","        \"Albert Einstein was best known for developing the theory of relativity.\",\n","        \"Albert Einstein made important contributions to the development of the theory of quantum mechanics.\"\n","    ]\n","}}\n","-----------------------------\n","\n","Now perform the same with the following input\n","input: {{\n","    \"question\": {safe_question},\n","    \"answer\": {safe_answer}\n","}}\n","Output: \"\"\"\n","\n","def nli_statement_prompt(context: str, statements: t.List[str]) -> str:\n","    \"\"\"\n","    V1-identical NLI statement evaluation - matches PydanticPrompt.to_string() exactly.\n","\n","    Args:\n","        context: The context to evaluate statements against\n","        statements: The statements to judge for faithfulness\n","\n","    Returns:\n","        V1-identical prompt string for the LLM\n","    \"\"\"\n","    # Format inputs exactly like V1's model_dump_json(indent=4, exclude_none=True)\n","    safe_context = json.dumps(context)\n","    safe_statements = json.dumps(statements, indent=4).replace(\"\\n\", \"\\n    \")\n","\n","    return f\"\"\"Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n","Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n","{{\"$defs\": {{\"StatementFaithfulnessAnswer\": {{\"properties\": {{\"statement\": {{\"description\": \"the original statement, word-by-word\", \"title\": \"Statement\", \"type\": \"string\"}}, \"reason\": {{\"description\": \"the reason of the verdict\", \"title\": \"Reason\", \"type\": \"string\"}}, \"verdict\": {{\"description\": \"the verdict(0/1) of the faithfulness.\", \"title\": \"Verdict\", \"type\": \"integer\"}}}}, \"required\": [\"statement\", \"reason\", \"verdict\"], \"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\"}}}}, \"properties\": {{\"statements\": {{\"items\": {{\"$ref\": \"#/$defs/StatementFaithfulnessAnswer\"}}, \"title\": \"Statements\", \"type\": \"array\"}}}}, \"required\": [\"statements\"], \"title\": \"NLIStatementOutput\", \"type\": \"object\"}}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n","\n","--------EXAMPLES-----------\n","Example 1\n","Input: {{\n","    \"context\": \"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\",\n","    \"statements\": [\n","        \"John is majoring in Biology.\",\n","        \"John is taking a course on Artificial Intelligence.\",\n","        \"John is a dedicated student.\",\n","        \"John has a part-time job.\"\n","    ]\n","}}\n","Output: {{\n","    \"statements\": [\n","        {{\n","            \"statement\": \"John is majoring in Biology.\",\n","            \"reason\": \"John's major is explicitly stated as Computer Science, not Biology.\",\n","            \"verdict\": 0\n","        }},\n","        {{\n","            \"statement\": \"John is taking a course on Artificial Intelligence.\",\n","            \"reason\": \"The context mentions courses in Data Structures, Algorithms, and Database Management, but does not mention Artificial Intelligence.\",\n","            \"verdict\": 0\n","        }},\n","        {{\n","            \"statement\": \"John is a dedicated student.\",\n","            \"reason\": \"The context states that John is a diligent student who spends a significant amount of time studying and completing assignments.\",\n","            \"verdict\": 1\n","        }},\n","        {{\n","            \"statement\": \"John has a part-time job.\",\n","            \"reason\": \"There is no information in the context about John having a part-time job.\",\n","            \"verdict\": 0\n","        }}\n","    ]\n","}}\n","-----------------------------\n","\n","Now perform the same with the following input\n","input: {{\n","    \"context\": {safe_context},\n","    \"statements\": {safe_statements}\n","}}\n","Output: \"\"\"\n","# -------------------------------\n","# 4️⃣ Helper functions gọi Gemini Flash\n","# -------------------------------\n","def generate_statements(question: str, answer: str) -> list:\n","    prompt = statement_generator_prompt(question, answer)\n","    resp = model.generate_content(prompt)\n","    try:\n","        data = json.loads(resp.text.strip())\n","        return data.get(\"statements\", [])\n","    except:\n","        return []\n","\n","def judge_statements(context: str, statements: list) -> list:\n","    if not statements:\n","        return []\n","    prompt = nli_statement_prompt(context, statements)\n","    resp = model.generate_content(prompt)\n","    try:\n","        data = json.loads(resp.text.strip())\n","        verdicts = [s.get(\"verdict\", 0) for s in data.get(\"statements\", [])]\n","        return verdicts\n","    except:\n","        return [0] * len(statements)\n","\n","def compute_faithfulness(question: str, response: str, context: str) -> float:\n","    statements = generate_statements(question, response)\n","    verdicts = judge_statements(context, statements)\n","    if not verdicts:\n","        return 0.0\n","    return sum(verdicts) / len(verdicts)\n","\n","# -------------------------------\n","# 5️⃣ Load data CSV (toàn bộ file, không sample)\n","# -------------------------------\n","csv_path = \"data/llama_3b_eval_data.csv\"\n","df = pd.read_csv(csv_path)\n","\n","# -------------------------------\n","# 6️⃣ Chuẩn bị output batch\n","# -------------------------------\n","os.makedirs(\"eval_result\", exist_ok=True)\n","output_csv = \"eval_result/llama_3b_faithfulness.csv\"\n","\n","# Nếu file đã tồn tại, xóa để viết batch mới\n","if os.path.exists(output_csv):\n","    os.remove(output_csv)\n","\n","BATCH_SIZE = 10\n","batch_results = []\n","\n","# -------------------------------\n","# 7️⃣ Chạy evaluation theo batch 10 dòng\n","# -------------------------------\n","for idx, row in tqdm(df.iterrows(), total=len(df)):\n","    question = str(row.get(\"question\", \"\"))\n","    response = str(row.get(\"response\", \"\"))\n","    context = str(row.get(\"final_text\", \"\"))\n","\n","    score = compute_faithfulness(question, response, context)\n","    print(f\"Index: {idx}, Faithfulness score: {score}\")\n","\n","    batch_results.append({\n","        \"question\": question,\n","        \"context\": context,\n","        \"response\": response,\n","        \"faithfulness_score\": score\n","    })\n","\n","    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n","    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n","        df_batch = pd.DataFrame(batch_results)\n","        if os.path.exists(output_csv):\n","            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n","        else:\n","            df_batch.to_csv(output_csv, index=False)\n","        batch_results = []  # reset batch\n","        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n"]},{"cell_type":"markdown","source":["### Qwen2-7b"],"metadata":{"id":"rNwRJzjpKPxV"},"id":"rNwRJzjpKPxV"},{"cell_type":"code","source":["# eval_faithfulness_flash_batch.py\n","import os\n","from dotenv import load_dotenv\n","import pandas as pd\n","import google.generativeai as genai\n","import json\n","from tqdm import tqdm\n","import typing as t\n","\n","# -------------------------------\n","# 1️⃣ Load API key từ .env\n","# -------------------------------\n","load_dotenv()\n","api_key = os.getenv(\"GEMINI_API_KEY\")\n","genai.configure(api_key=api_key)\n","\n","# -------------------------------\n","# 2️⃣ Tạo Gemini 2.5 Flash instance\n","# -------------------------------\n","model = genai.GenerativeModel(\"gemini-2.5-flash\")\n","\n","# -------------------------------\n","# 3️⃣ Prompt functions (Statement Generator & NLI Judge)\n","# -------------------------------\n","def statement_generator_prompt(question: str, answer: str) -> str:\n","    \"\"\"\n","    V1-identical statement generator - matches PydanticPrompt.to_string() exactly.\n","\n","    Args:\n","        question: The question being answered\n","        answer: The answer text to break down into statements\n","\n","    Returns:\n","        V1-identical prompt string for the LLM\n","    \"\"\"\n","    # Format inputs exactly like V1's model_dump_json(indent=4, exclude_none=True)\n","    safe_question = json.dumps(question)\n","    safe_answer = json.dumps(answer)\n","\n","    return f\"\"\"Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\n","Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n","{{\"properties\": {{\"statements\": {{\"description\": \"The generated statements\", \"items\": {{\"type\": \"string\"}}, \"title\": \"Statements\", \"type\": \"array\"}}}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n","\n","--------EXAMPLES-----------\n","Example 1\n","Input: {{\n","    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n","    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n","}}\n","Output: {{\n","    \"statements\": [\n","        \"Albert Einstein was a German-born theoretical physicist.\",\n","        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\n","        \"Albert Einstein was best known for developing the theory of relativity.\",\n","        \"Albert Einstein made important contributions to the development of the theory of quantum mechanics.\"\n","    ]\n","}}\n","-----------------------------\n","\n","Now perform the same with the following input\n","input: {{\n","    \"question\": {safe_question},\n","    \"answer\": {safe_answer}\n","}}\n","Output: \"\"\"\n","\n","def nli_statement_prompt(context: str, statements: t.List[str]) -> str:\n","    \"\"\"\n","    V1-identical NLI statement evaluation - matches PydanticPrompt.to_string() exactly.\n","\n","    Args:\n","        context: The context to evaluate statements against\n","        statements: The statements to judge for faithfulness\n","\n","    Returns:\n","        V1-identical prompt string for the LLM\n","    \"\"\"\n","    # Format inputs exactly like V1's model_dump_json(indent=4, exclude_none=True)\n","    safe_context = json.dumps(context)\n","    safe_statements = json.dumps(statements, indent=4).replace(\"\\n\", \"\\n    \")\n","\n","    return f\"\"\"Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n","Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n","{{\"$defs\": {{\"StatementFaithfulnessAnswer\": {{\"properties\": {{\"statement\": {{\"description\": \"the original statement, word-by-word\", \"title\": \"Statement\", \"type\": \"string\"}}, \"reason\": {{\"description\": \"the reason of the verdict\", \"title\": \"Reason\", \"type\": \"string\"}}, \"verdict\": {{\"description\": \"the verdict(0/1) of the faithfulness.\", \"title\": \"Verdict\", \"type\": \"integer\"}}}}, \"required\": [\"statement\", \"reason\", \"verdict\"], \"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\"}}}}, \"properties\": {{\"statements\": {{\"items\": {{\"$ref\": \"#/$defs/StatementFaithfulnessAnswer\"}}, \"title\": \"Statements\", \"type\": \"array\"}}}}, \"required\": [\"statements\"], \"title\": \"NLIStatementOutput\", \"type\": \"object\"}}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n","\n","--------EXAMPLES-----------\n","Example 1\n","Input: {{\n","    \"context\": \"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\",\n","    \"statements\": [\n","        \"John is majoring in Biology.\",\n","        \"John is taking a course on Artificial Intelligence.\",\n","        \"John is a dedicated student.\",\n","        \"John has a part-time job.\"\n","    ]\n","}}\n","Output: {{\n","    \"statements\": [\n","        {{\n","            \"statement\": \"John is majoring in Biology.\",\n","            \"reason\": \"John's major is explicitly stated as Computer Science, not Biology.\",\n","            \"verdict\": 0\n","        }},\n","        {{\n","            \"statement\": \"John is taking a course on Artificial Intelligence.\",\n","            \"reason\": \"The context mentions courses in Data Structures, Algorithms, and Database Management, but does not mention Artificial Intelligence.\",\n","            \"verdict\": 0\n","        }},\n","        {{\n","            \"statement\": \"John is a dedicated student.\",\n","            \"reason\": \"The context states that John is a diligent student who spends a significant amount of time studying and completing assignments.\",\n","            \"verdict\": 1\n","        }},\n","        {{\n","            \"statement\": \"John has a part-time job.\",\n","            \"reason\": \"There is no information in the context about John having a part-time job.\",\n","            \"verdict\": 0\n","        }}\n","    ]\n","}}\n","-----------------------------\n","\n","Now perform the same with the following input\n","input: {{\n","    \"context\": {safe_context},\n","    \"statements\": {safe_statements}\n","}}\n","Output: \"\"\"\n","# -------------------------------\n","# 4️⃣ Helper functions gọi Gemini Flash\n","# -------------------------------\n","def generate_statements(question: str, answer: str) -> list:\n","    prompt = statement_generator_prompt(question, answer)\n","    resp = model.generate_content(prompt)\n","    try:\n","        data = json.loads(resp.text.strip())\n","        return data.get(\"statements\", [])\n","    except:\n","        return []\n","\n","def judge_statements(context: str, statements: list) -> list:\n","    if not statements:\n","        return []\n","    prompt = nli_statement_prompt(context, statements)\n","    resp = model.generate_content(prompt)\n","    try:\n","        data = json.loads(resp.text.strip())\n","        verdicts = [s.get(\"verdict\", 0) for s in data.get(\"statements\", [])]\n","        return verdicts\n","    except:\n","        return [0] * len(statements)\n","\n","def compute_faithfulness(question: str, response: str, context: str) -> float:\n","    statements = generate_statements(question, response)\n","    verdicts = judge_statements(context, statements)\n","    if not verdicts:\n","        return 0.0\n","    return sum(verdicts) / len(verdicts)\n","\n","# -------------------------------\n","# 5️⃣ Load data CSV (toàn bộ file, không sample)\n","# -------------------------------\n","csv_path = \"data/qwen2_7b_eval_data.csv\"\n","df = pd.read_csv(csv_path)\n","\n","# -------------------------------\n","# 6️⃣ Chuẩn bị output batch\n","# -------------------------------\n","os.makedirs(\"eval_result\", exist_ok=True)\n","output_csv = \"eval_result/qwen2_7b_faithfulness.csv\"\n","\n","# Nếu file đã tồn tại, xóa để viết batch mới\n","if os.path.exists(output_csv):\n","    os.remove(output_csv)\n","\n","BATCH_SIZE = 10\n","batch_results = []\n","\n","# -------------------------------\n","# 7️⃣ Chạy evaluation theo batch 10 dòng\n","# -------------------------------\n","for idx, row in tqdm(df.iterrows(), total=len(df)):\n","    question = str(row.get(\"question\", \"\"))\n","    response = str(row.get(\"response\", \"\"))\n","    context = str(row.get(\"final_text\", \"\"))\n","\n","    score = compute_faithfulness(question, response, context)\n","    print(f\"Index: {idx}, Faithfulness score: {score}\")\n","\n","    batch_results.append({\n","        \"question\": question,\n","        \"context\": context,\n","        \"response\": response,\n","        \"faithfulness_score\": score\n","    })\n","\n","    # Khi đủ batch hoặc dòng cuối, ghi vào CSV\n","    if len(batch_results) >= BATCH_SIZE or idx == len(df) - 1:\n","        df_batch = pd.DataFrame(batch_results)\n","        if os.path.exists(output_csv):\n","            df_batch.to_csv(output_csv, mode=\"a\", header=False, index=False)\n","        else:\n","            df_batch.to_csv(output_csv, index=False)\n","        batch_results = []  # reset batch\n","        print(f\"Saved batch ending at index {idx} to {output_csv}\")\n"],"metadata":{"id":"ptWEkLFUKRmy","executionInfo":{"status":"aborted","timestamp":1764632687067,"user_tz":-420,"elapsed":29,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}}},"id":"ptWEkLFUKRmy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Comparison"],"metadata":{"id":"bm_9e1xdKjWX"},"id":"bm_9e1xdKjWX"},{"cell_type":"code","source":["import pandas as pd\n","from collections import Counter\n","from pathlib import Path\n","\n","# Thư mục chứa các file CSV\n","data_dir = Path(\"eval_result\")\n","\n","# Lấy danh sách file CSV\n","csv_files = sorted(data_dir.glob(\"*.csv\"))\n","\n","if not csv_files:\n","    print(\"Không tìm thấy file CSV nào trong thư mục:\", data_dir)\n","else:\n","    for csv_file in csv_files:\n","        print(f\"\\n=== File: {csv_file.name} ===\")\n","        try:\n","            df = pd.read_csv(csv_file)\n","        except Exception as e:\n","            print(\"Lỗi khi đọc file:\", e)\n","            continue\n","\n","        # Kiểm tra cột groundedness_score\n","        if \"groundedness_score\" not in df.columns:\n","            print(\"Cột 'groundedness_score' không tồn tại trong file này!\")\n","            continue\n","\n","        # Thống kê số lượng từng điểm\n","        score_counts = Counter(df[\"groundedness_score\"])\n","        if not score_counts:\n","            print(\"File rỗng hoặc không có giá trị groundedness_score\")\n","            continue\n","\n","        print(\"=== Groundedness Score Distribution ===\")\n","        for score_level in sorted(score_counts.keys()):\n","            print(f\"Score {score_level}: {score_counts[score_level]} responses\")\n","\n","        # Tính tổng điểm\n","        total_score = df[\"groundedness_score\"].sum()\n","        print(f\"Total Groundedness score (sum): {total_score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7EtN4twKmOI","executionInfo":{"status":"ok","timestamp":1764632378755,"user_tz":-420,"elapsed":451,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"f532f164-ee77-48db-910e-48a004a20924"},"id":"f7EtN4twKmOI","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== File: llama_3b_groundedness.csv ===\n","=== Groundedness Score Distribution ===\n","Score 0.0: 47 responses\n","Score 0.25: 21 responses\n","Score 0.5: 93 responses\n","Score 0.75: 14 responses\n","Score 1.0: 125 responses\n","Total Groundedness score (sum): 187.25\n","\n","=== File: qwen2_3b_groundedness.csv ===\n","=== Groundedness Score Distribution ===\n","Score 0.0: 37 responses\n","Score 0.25: 12 responses\n","Score 0.5: 83 responses\n","Score 0.75: 30 responses\n","Score 1.0: 138 responses\n","Total Groundedness score (sum): 205.0\n","\n","=== File: qwen2_7b_groundedness.csv ===\n","=== Groundedness Score Distribution ===\n","Score 0.0: 46 responses\n","Score 0.25: 7 responses\n","Score 0.5: 25 responses\n","Score 0.75: 11 responses\n","Score 1.0: 211 responses\n","Total Groundedness score (sum): 233.5\n"]}]},{"cell_type":"markdown","source":["# BertScore"],"metadata":{"id":"S-2lFckBaDy8"},"id":"S-2lFckBaDy8"},{"cell_type":"code","source":["!pip install bert-score\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Pn9D3Cfawwf","executionInfo":{"status":"ok","timestamp":1764634615751,"user_tz":-420,"elapsed":15978,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"446671ff-c83f-46ff-d88b-07a596693db7"},"id":"5Pn9D3Cfawwf","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (0.3.13)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.5)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (2025.11.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.11.12)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n"]}]},{"cell_type":"markdown","source":["### Qwen2-3b"],"metadata":{"id":"e7qgWeAJcTGY"},"id":"e7qgWeAJcTGY"},{"cell_type":"code","source":["import pandas as pd\n","from bert_score import score\n","\n","# ===============================\n","# 1. Load data\n","# ===============================\n","df_pred = pd.read_csv(\"data/qwen2-3b_eval_data.csv\")\n","df_ref  = pd.read_csv(\"data/merged_all.csv\")\n","\n","# ===============================\n","# 2. Lấy NUM_ROWS reference làm ground truth\n","# ===============================\n","NUM_ROWS = 300\n","SEED = 42\n","\n","df_ref_sample = df_ref.sample(n=NUM_ROWS, random_state=SEED)\n","df_pred_slice = df_pred.iloc[:NUM_ROWS]\n","\n","assert len(df_pred_slice) == len(df_ref_sample)\n","\n","predictions = df_pred_slice[\"response\"].astype(str).tolist()\n","references  = df_ref_sample[\"response\"].astype(str).tolist()\n","\n","# ===============================\n","# 4. Tính BERTScore cho tiếng Việt\n","# ===============================\n","P, R, F1 = score(\n","    predictions,\n","    references,\n","    lang=\"vi\",                   # tokenizer cho tiếng Việt\n","    model_type=\"xlm-roberta-large\",  # embedder tốt nhất cho tiếng Việt\n","    verbose=True\n",")\n","\n","# ===============================\n","# 5. Gộp vào DataFrame kết quả\n","# ===============================\n","df_result = df_pred_slice.copy()\n","df_result[\"bert_precision\"] = P.tolist()\n","df_result[\"bert_recall\"] = R.tolist()\n","df_result[\"bert_f1\"] = F1.tolist()\n","\n","# ===============================\n","# 6. Lưu file\n","# ===============================\n","output_csv = \"eval_result/qwen2_3b_bertscore.csv\"\n","df_result.to_csv(output_csv, index=False)\n","\n","print(f\"\\n🎉 Done! Saved BERTScore to {output_csv}\")\n","print(f\"Average BERTScore F1 = {df_result['bert_f1'].mean():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467,"referenced_widgets":["b8709d28191b469a8d5b8fab479bb364","dc87772e5d7848b99ab1ebf75edd2612","2c29805fd1094301a167189fde2cf244","7928d7606596458181030d323ad3abce","9417557d0ca04c048a9a2e14bad9a890","fb4b9243ce5a4d8db6873ae8a5f90e54","b04e5ceaf22741da84badf64393864f1","75503fc761f14aecb52dbb740e519f85","3fbe15b086a340c1887e6550f4cc31df","7b3bedfb1d4f4224b2f8b69b55a689fd","b8e405c0d1144608b94514dc9b70585d","75467439ce9648a1b98b924dab837f9c","b62a76fc88514fb7843d7f732a6e1a1b","f90a5438241f4a498e08e7d35be195ef","69a6ae7f34a24b1587e6ce7d384db1d0","d0eafe4a35f54d0fbb3ef7cd8e87b7d5","e84de7a4b20c45cf962c29b135b1aee9","bea47811ae534b2aad7aeeea46cad61c","f2e65cd458434424896badf9a4ddf448","c1eb019363454812988a06ee1f009044","5456f763206549a19d72b1d2e4dc3402","c6c18efd956c434083a23c8a1bf92ad3","881675cc267545d0b2de7b300e4c572d","9e6f852a76dd428681b3e49564c1289b","3b7e531d43e241cc9976d66b40f424f3","e9f12c7e19eb4901a27c15028e90ee8b","071edd89644542459fb919039f198360","8109bce9787c42e2ac461a661bbce9e0","b1c28cdf89bc45faa0ab13513aef81ce","7a676307472a49e691dc4d1e85f6bdfb","f7762a5aae36413694714d79b1e73442","d65bf7d1c85944f8af37fa7e0c232419","e938d5409f6443218ccf350fee5a3587","4c31647eba1a4da6b3e7a569bb7666d1","66ea5e2852f647cb8228ba76e9307d25","361f4a5861734a20beaeb7fc3f7892c4","f9e60c4a04864bb9bf270e75dc68b2ae","8de663c91e4b4a498abc1dc80a6a78cd","054caeb02b0a4b3aa7c96bc7943c3269","b21c581a84674e088201502a30f91ea1","091d27fdc26d4a6691ff2e1874bb2e91","198995d834b34f89be4e9c453a091172","9d63fc949c9443568caeec63b5606f38","6d8496e6045d45d297c7550b5a86e949","939dc75a99064db0aa2d164457a88cf7","522041bc0081443f8862ff6648bcc68a","ac01f4106398437986c28a5aebb9f6d2","3b88998df4204d06b0af68c8c3a02e16","a17a44146974428ba27dff77d74773b5","cb62faa616294ba5880c1c83daac3ccd","1914094976044017a9575713a0f591fa","de2c1ca249fa448c9a9f7df9ec1ead78","d087635be115419b899d9336db4e0f69","93054a5c135b45178cfbaa3706015785","f0b7e9079b5640aeba6b6c30696a97f7","b72b54982dc14b519eca100f5de93aae","f3a33c2382d24382b2f445243cf0bc93","2070f65496814228b91505bca22a40e6","83e140c096e04f3b9eb9b0fcefcd68d7","6ac3e4b106884cdeb5d950e07a1604bc","3b4bc11d246648669e1fd3be7327555c","330cef4fc39149cebf675acbb0411e15","6054fc43e8b146598c24c3ce30fdfd4a","00ae88b453c0482aa042bacd09eaa251","82fd749cfac548a4a4691dd82b9d7437","d98923453b734a749d3dcb1a000409c2","908dd1fa8ee44b3ebc0e0218e0ff764a","673753e5955b48a5bb4d041a5d06e63a","1154678a17a14e8390346c2659dca857","2753dee5dd254ace852251a9ab8bc0d5","6cddca2465c249a2b26a73dddddee383","c7fb49789a01479e87d946d4e160f61f","0e4574d3f3b64f2a984838a9dc869a94","e7a6bba068f04708a8a17e2b6c1809ff","64d46af1bef5423f80704f9e32bebb8c","a163556d8efd4daea4fecc03bc40428d","3f106e58e3954af1a142d9496de46d5a"]},"id":"Civ7sNnGRv55","executionInfo":{"status":"ok","timestamp":1764635142883,"user_tz":-420,"elapsed":79037,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"e7ce94cd-1a10-4100-b234-c3286c0eda2c"},"id":"Civ7sNnGRv55","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8709d28191b469a8d5b8fab479bb364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75467439ce9648a1b98b924dab837f9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"881675cc267545d0b2de7b300e4c572d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c31647eba1a4da6b3e7a569bb7666d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939dc75a99064db0aa2d164457a88cf7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["calculating scores...\n","computing bert embedding.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/9 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b72b54982dc14b519eca100f5de93aae"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["computing greedy matching.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"908dd1fa8ee44b3ebc0e0218e0ff764a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["done in 8.37 seconds, 35.86 sentences/sec\n","\n","🎉 Done! Saved BERTScore to eval_result/qwen2_3b_bertscore.csv\n","Average BERTScore F1 = 0.9323\n"]}]},{"cell_type":"markdown","source":["### Llama 3b"],"metadata":{"id":"h-Hb9zqxcVXd"},"id":"h-Hb9zqxcVXd"},{"cell_type":"code","source":["import pandas as pd\n","from bert_score import score\n","\n","# ===============================\n","# 1. Load data\n","# ===============================\n","df_pred = pd.read_csv(\"data/llama_3b_eval_data.csv\")\n","df_ref  = pd.read_csv(\"data/merged_all.csv\")\n","\n","# ===============================\n","# 2. Lấy NUM_ROWS reference làm ground truth\n","# ===============================\n","NUM_ROWS = 300\n","SEED = 42\n","\n","df_ref_sample = df_ref.sample(n=NUM_ROWS, random_state=SEED)\n","df_pred_slice = df_pred.iloc[:NUM_ROWS]\n","\n","assert len(df_pred_slice) == len(df_ref_sample)\n","\n","predictions = df_pred_slice[\"response\"].astype(str).tolist()\n","references  = df_ref_sample[\"response\"].astype(str).tolist()\n","\n","# ===============================\n","# 4. Tính BERTScore cho tiếng Việt\n","# ===============================\n","P, R, F1 = score(\n","    predictions,\n","    references,\n","    lang=\"vi\",                   # tokenizer cho tiếng Việt\n","    model_type=\"xlm-roberta-large\",  # embedder tốt nhất cho tiếng Việt\n","    verbose=True\n",")\n","\n","# ===============================\n","# 5. Gộp vào DataFrame kết quả\n","# ===============================\n","df_result = df_pred_slice.copy()\n","df_result[\"bert_precision\"] = P.tolist()\n","df_result[\"bert_recall\"] = R.tolist()\n","df_result[\"bert_f1\"] = F1.tolist()\n","\n","# ===============================\n","# 6. Lưu file\n","# ===============================\n","output_csv = \"eval_result/llama_3b_bertscore.csv\"\n","df_result.to_csv(output_csv, index=False)\n","\n","print(f\"\\n🎉 Done! Saved BERTScore to {output_csv}\")\n","print(f\"Average BERTScore F1 = {df_result['bert_f1'].mean():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203,"referenced_widgets":["9568336b9ae74caab914f832741217a4","837959fa83024865a41c03b1a93a2443","56eb974edf9342bea0e94aa60c65a6f1","9f5512648c1a45a693001e60d635bb28","5ff69c5014ea4496b12a10ba6d83f971","a8fb045db80f4289b04c72dcb5595e8a","2211a39ad09f4e1b84d59edf670e47ff","dd38441ec4da4f999dbfa9be01e64262","12a920410811414fa19372eabb16eecd","c13203382d6c49c5b565d147b2f8374f","ccf875d9eec44e888ca54f3f7dd4da2a","a016768197e841b092f7d43c10938690","7cbb6d7eb1754cd9a12d06b99c4cf087","50ef3f7e86b3490ab109b62e015262c6","f3d1a444a0724fd4a8ff3ca998e5775d","38a0e06219ca4098b718d6318a9915ba","04303a92657e47dfb501707075fdcb8b","3716bdde1a1343f3a955288bce3d5754","55687145083f4fa1bf2d830f160fbef6","c0ebe50012ab45539807af56d105811a","5653be52a5554668bd4ebec680555a4a","4e75906132354b2e80d50b071ce27ffb"]},"id":"6I3xCeO3bRPC","executionInfo":{"status":"ok","timestamp":1764635226542,"user_tz":-420,"elapsed":34431,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"a1b844f6-79b5-4252-8042-35bf5eefbce5"},"id":"6I3xCeO3bRPC","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["calculating scores...\n","computing bert embedding.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9568336b9ae74caab914f832741217a4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["computing greedy matching.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a016768197e841b092f7d43c10938690"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["done in 20.13 seconds, 14.90 sentences/sec\n","\n","🎉 Done! Saved BERTScore to eval_result/llama_3b_bertscore.csv\n","Average BERTScore F1 = 0.8973\n"]}]},{"cell_type":"markdown","source":["### Qwen2 7b"],"metadata":{"id":"83VLsIPUdHXq"},"id":"83VLsIPUdHXq"},{"cell_type":"code","source":["import pandas as pd\n","from bert_score import score\n","\n","# ===============================\n","# 1. Load data\n","# ===============================\n","df_pred = pd.read_csv(\"data/qwen2_7b_eval_data.csv\")\n","df_ref  = pd.read_csv(\"data/merged_all.csv\")\n","\n","# ===============================\n","# 2. Lấy NUM_ROWS reference làm ground truth\n","# ===============================\n","NUM_ROWS = 300\n","SEED = 42\n","\n","df_ref_sample = df_ref.sample(n=NUM_ROWS, random_state=SEED)\n","df_pred_slice = df_pred.iloc[:NUM_ROWS]\n","\n","assert len(df_pred_slice) == len(df_ref_sample)\n","\n","predictions = df_pred_slice[\"response\"].astype(str).tolist()\n","references  = df_ref_sample[\"response\"].astype(str).tolist()\n","\n","# ===============================\n","# 4. Tính BERTScore cho tiếng Việt\n","# ===============================\n","P, R, F1 = score(\n","    predictions,\n","    references,\n","    lang=\"vi\",                   # tokenizer cho tiếng Việt\n","    model_type=\"xlm-roberta-large\",  # embedder tốt nhất cho tiếng Việt\n","    verbose=True\n",")\n","\n","# ===============================\n","# 5. Gộp vào DataFrame kết quả\n","# ===============================\n","df_result = df_pred_slice.copy()\n","df_result[\"bert_precision\"] = P.tolist()\n","df_result[\"bert_recall\"] = R.tolist()\n","df_result[\"bert_f1\"] = F1.tolist()\n","\n","# ===============================\n","# 6. Lưu file\n","# ===============================\n","output_csv = \"eval_result/qwen2_7b_bertscore.csv\"\n","df_result.to_csv(output_csv, index=False)\n","\n","print(f\"\\n🎉 Done! Saved BERTScore to {output_csv}\")\n","print(f\"Average BERTScore F1 = {df_result['bert_f1'].mean():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203,"referenced_widgets":["df4d893372e74658af4b08748809f476","c7d3d8d34e04437f8e899dc2f112f19e","6f56ab5a26444c82958421803b861795","722ed9a0447443d39d5c17e30b496288","d472b588a6f2431a8cb6ea64e62200b6","7b16dde3fc784c7baa0b97b9b8c35a74","f8196072fea244b89690d2bed3f59e3c","a7d3a1cce57c4caa822a1e5420b8a8e3","dd0c87c8c53e43aeb3628729a0ab1ae2","412ee84c3c804576bf93da3708bfdc0d","18a7a38fcf2c4d85ad8f0abb011bb641","71e871bebffa4340b00738c52f64d28a","cdc37dba25ab4013a8a26538437c5d0b","e79bd1e3d8b8483d8b995b4163cddf14","7b1d0af7e60e41249bd20205c82f1e42","a6e6b64c20d5430c925c4cb49ce3040b","a785249d63424f559d4b294817ee0a88","ef0b523d646e4c9b920076cdd80fd83f","562528df95ea4332a8a072fac8dfb6c0","b2d8968ffea9457c90a6e189a5785a3a","34c769be5f4e47a989e1646d6da9e893","ec19693e0b6d40a687713664d921e22a"]},"id":"x15QkS9AdCoe","executionInfo":{"status":"ok","timestamp":1764635268953,"user_tz":-420,"elapsed":14233,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"f1a036fe-cfc1-4914-8542-c9ecca775f24"},"id":"x15QkS9AdCoe","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["calculating scores...\n","computing bert embedding.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/9 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4d893372e74658af4b08748809f476"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["computing greedy matching.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71e871bebffa4340b00738c52f64d28a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["done in 8.57 seconds, 34.99 sentences/sec\n","\n","🎉 Done! Saved BERTScore to eval_result/qwen2_7b_bertscore.csv\n","Average BERTScore F1 = 0.9462\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# -------------------------------\n","# 1️⃣ Danh sách file\n","# -------------------------------\n","files = {\n","    \"qwen2-3b\": \"eval_result/qwen2_3b_bertscore.csv\",\n","    \"llama-3b\": \"eval_result/llama_3b_bertscore.csv\",\n","    \"qwen2-7b\": \"eval_result/qwen2_7b_bertscore.csv\"\n","}\n","\n","# -------------------------------\n","# 2️⃣ Thống kê mean từng file\n","# -------------------------------\n","for name, path in files.items():\n","    print(f\"\\n=== Mean BERTScore for {name} ===\")\n","    df = pd.read_csv(path)\n","\n","    cols = [\"bert_precision\", \"bert_recall\", \"bert_f1\"]\n","\n","    for col in cols:\n","        if col in df.columns:\n","            mean_val = df[col].mean()\n","            print(f\"{col:15s}: {mean_val:.4f}\")\n","        else:\n","            print(f\"{col:15s}: column not found\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYCrJnBWdR6Q","executionInfo":{"status":"ok","timestamp":1764647316029,"user_tz":-420,"elapsed":276,"user":{"displayName":"Dailymate","userId":"09240786015871031354"}},"outputId":"461666a9-e967-434f-b4cf-05f692d826c7"},"id":"NYCrJnBWdR6Q","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Mean BERTScore for qwen2-3b ===\n","bert_precision : 0.9159\n","bert_recall    : 0.9503\n","bert_f1        : 0.9323\n","\n","=== Mean BERTScore for llama-3b ===\n","bert_precision : 0.8575\n","bert_recall    : 0.9438\n","bert_f1        : 0.8973\n","\n","=== Mean BERTScore for qwen2-7b ===\n","bert_precision : 0.9350\n","bert_recall    : 0.9588\n","bert_f1        : 0.9462\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"f-Z53cBiKraP"},"id":"f-Z53cBiKraP","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["K4hTE-oSoCEi","MPpC57BbqaTt","WPVcyXxU3-VR","r9a9jvX8xWbw","NCxTdjXCzSN2","h-Hb9zqxcVXd"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b8709d28191b469a8d5b8fab479bb364":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc87772e5d7848b99ab1ebf75edd2612","IPY_MODEL_2c29805fd1094301a167189fde2cf244","IPY_MODEL_7928d7606596458181030d323ad3abce"],"layout":"IPY_MODEL_9417557d0ca04c048a9a2e14bad9a890"}},"dc87772e5d7848b99ab1ebf75edd2612":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb4b9243ce5a4d8db6873ae8a5f90e54","placeholder":"​","style":"IPY_MODEL_b04e5ceaf22741da84badf64393864f1","value":"tokenizer_config.json: 100%"}},"2c29805fd1094301a167189fde2cf244":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75503fc761f14aecb52dbb740e519f85","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fbe15b086a340c1887e6550f4cc31df","value":25}},"7928d7606596458181030d323ad3abce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b3bedfb1d4f4224b2f8b69b55a689fd","placeholder":"​","style":"IPY_MODEL_b8e405c0d1144608b94514dc9b70585d","value":" 25.0/25.0 [00:00&lt;00:00, 470B/s]"}},"9417557d0ca04c048a9a2e14bad9a890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb4b9243ce5a4d8db6873ae8a5f90e54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b04e5ceaf22741da84badf64393864f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75503fc761f14aecb52dbb740e519f85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fbe15b086a340c1887e6550f4cc31df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b3bedfb1d4f4224b2f8b69b55a689fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8e405c0d1144608b94514dc9b70585d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75467439ce9648a1b98b924dab837f9c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b62a76fc88514fb7843d7f732a6e1a1b","IPY_MODEL_f90a5438241f4a498e08e7d35be195ef","IPY_MODEL_69a6ae7f34a24b1587e6ce7d384db1d0"],"layout":"IPY_MODEL_d0eafe4a35f54d0fbb3ef7cd8e87b7d5"}},"b62a76fc88514fb7843d7f732a6e1a1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e84de7a4b20c45cf962c29b135b1aee9","placeholder":"​","style":"IPY_MODEL_bea47811ae534b2aad7aeeea46cad61c","value":"config.json: 100%"}},"f90a5438241f4a498e08e7d35be195ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2e65cd458434424896badf9a4ddf448","max":616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1eb019363454812988a06ee1f009044","value":616}},"69a6ae7f34a24b1587e6ce7d384db1d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5456f763206549a19d72b1d2e4dc3402","placeholder":"​","style":"IPY_MODEL_c6c18efd956c434083a23c8a1bf92ad3","value":" 616/616 [00:00&lt;00:00, 18.5kB/s]"}},"d0eafe4a35f54d0fbb3ef7cd8e87b7d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e84de7a4b20c45cf962c29b135b1aee9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bea47811ae534b2aad7aeeea46cad61c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2e65cd458434424896badf9a4ddf448":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1eb019363454812988a06ee1f009044":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5456f763206549a19d72b1d2e4dc3402":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6c18efd956c434083a23c8a1bf92ad3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"881675cc267545d0b2de7b300e4c572d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e6f852a76dd428681b3e49564c1289b","IPY_MODEL_3b7e531d43e241cc9976d66b40f424f3","IPY_MODEL_e9f12c7e19eb4901a27c15028e90ee8b"],"layout":"IPY_MODEL_071edd89644542459fb919039f198360"}},"9e6f852a76dd428681b3e49564c1289b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8109bce9787c42e2ac461a661bbce9e0","placeholder":"​","style":"IPY_MODEL_b1c28cdf89bc45faa0ab13513aef81ce","value":"sentencepiece.bpe.model: 100%"}},"3b7e531d43e241cc9976d66b40f424f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a676307472a49e691dc4d1e85f6bdfb","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7762a5aae36413694714d79b1e73442","value":5069051}},"e9f12c7e19eb4901a27c15028e90ee8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d65bf7d1c85944f8af37fa7e0c232419","placeholder":"​","style":"IPY_MODEL_e938d5409f6443218ccf350fee5a3587","value":" 5.07M/5.07M [00:00&lt;00:00, 13.5MB/s]"}},"071edd89644542459fb919039f198360":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8109bce9787c42e2ac461a661bbce9e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1c28cdf89bc45faa0ab13513aef81ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a676307472a49e691dc4d1e85f6bdfb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7762a5aae36413694714d79b1e73442":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d65bf7d1c85944f8af37fa7e0c232419":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e938d5409f6443218ccf350fee5a3587":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c31647eba1a4da6b3e7a569bb7666d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66ea5e2852f647cb8228ba76e9307d25","IPY_MODEL_361f4a5861734a20beaeb7fc3f7892c4","IPY_MODEL_f9e60c4a04864bb9bf270e75dc68b2ae"],"layout":"IPY_MODEL_8de663c91e4b4a498abc1dc80a6a78cd"}},"66ea5e2852f647cb8228ba76e9307d25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_054caeb02b0a4b3aa7c96bc7943c3269","placeholder":"​","style":"IPY_MODEL_b21c581a84674e088201502a30f91ea1","value":"tokenizer.json: 100%"}},"361f4a5861734a20beaeb7fc3f7892c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_091d27fdc26d4a6691ff2e1874bb2e91","max":9096718,"min":0,"orientation":"horizontal","style":"IPY_MODEL_198995d834b34f89be4e9c453a091172","value":9096718}},"f9e60c4a04864bb9bf270e75dc68b2ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d63fc949c9443568caeec63b5606f38","placeholder":"​","style":"IPY_MODEL_6d8496e6045d45d297c7550b5a86e949","value":" 9.10M/9.10M [00:00&lt;00:00, 20.8MB/s]"}},"8de663c91e4b4a498abc1dc80a6a78cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"054caeb02b0a4b3aa7c96bc7943c3269":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b21c581a84674e088201502a30f91ea1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"091d27fdc26d4a6691ff2e1874bb2e91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"198995d834b34f89be4e9c453a091172":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d63fc949c9443568caeec63b5606f38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d8496e6045d45d297c7550b5a86e949":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"939dc75a99064db0aa2d164457a88cf7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_522041bc0081443f8862ff6648bcc68a","IPY_MODEL_ac01f4106398437986c28a5aebb9f6d2","IPY_MODEL_3b88998df4204d06b0af68c8c3a02e16"],"layout":"IPY_MODEL_a17a44146974428ba27dff77d74773b5"}},"522041bc0081443f8862ff6648bcc68a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb62faa616294ba5880c1c83daac3ccd","placeholder":"​","style":"IPY_MODEL_1914094976044017a9575713a0f591fa","value":"model.safetensors: 100%"}},"ac01f4106398437986c28a5aebb9f6d2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de2c1ca249fa448c9a9f7df9ec1ead78","max":2244817354,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d087635be115419b899d9336db4e0f69","value":2244817354}},"3b88998df4204d06b0af68c8c3a02e16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93054a5c135b45178cfbaa3706015785","placeholder":"​","style":"IPY_MODEL_f0b7e9079b5640aeba6b6c30696a97f7","value":" 2.24G/2.24G [00:21&lt;00:00, 150MB/s]"}},"a17a44146974428ba27dff77d74773b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb62faa616294ba5880c1c83daac3ccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1914094976044017a9575713a0f591fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de2c1ca249fa448c9a9f7df9ec1ead78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d087635be115419b899d9336db4e0f69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93054a5c135b45178cfbaa3706015785":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0b7e9079b5640aeba6b6c30696a97f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b72b54982dc14b519eca100f5de93aae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3a33c2382d24382b2f445243cf0bc93","IPY_MODEL_2070f65496814228b91505bca22a40e6","IPY_MODEL_83e140c096e04f3b9eb9b0fcefcd68d7"],"layout":"IPY_MODEL_6ac3e4b106884cdeb5d950e07a1604bc"}},"f3a33c2382d24382b2f445243cf0bc93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b4bc11d246648669e1fd3be7327555c","placeholder":"​","style":"IPY_MODEL_330cef4fc39149cebf675acbb0411e15","value":"100%"}},"2070f65496814228b91505bca22a40e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6054fc43e8b146598c24c3ce30fdfd4a","max":9,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00ae88b453c0482aa042bacd09eaa251","value":9}},"83e140c096e04f3b9eb9b0fcefcd68d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82fd749cfac548a4a4691dd82b9d7437","placeholder":"​","style":"IPY_MODEL_d98923453b734a749d3dcb1a000409c2","value":" 9/9 [00:08&lt;00:00,  1.86it/s]"}},"6ac3e4b106884cdeb5d950e07a1604bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b4bc11d246648669e1fd3be7327555c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"330cef4fc39149cebf675acbb0411e15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6054fc43e8b146598c24c3ce30fdfd4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00ae88b453c0482aa042bacd09eaa251":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82fd749cfac548a4a4691dd82b9d7437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d98923453b734a749d3dcb1a000409c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"908dd1fa8ee44b3ebc0e0218e0ff764a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_673753e5955b48a5bb4d041a5d06e63a","IPY_MODEL_1154678a17a14e8390346c2659dca857","IPY_MODEL_2753dee5dd254ace852251a9ab8bc0d5"],"layout":"IPY_MODEL_6cddca2465c249a2b26a73dddddee383"}},"673753e5955b48a5bb4d041a5d06e63a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7fb49789a01479e87d946d4e160f61f","placeholder":"​","style":"IPY_MODEL_0e4574d3f3b64f2a984838a9dc869a94","value":"100%"}},"1154678a17a14e8390346c2659dca857":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7a6bba068f04708a8a17e2b6c1809ff","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64d46af1bef5423f80704f9e32bebb8c","value":5}},"2753dee5dd254ace852251a9ab8bc0d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a163556d8efd4daea4fecc03bc40428d","placeholder":"​","style":"IPY_MODEL_3f106e58e3954af1a142d9496de46d5a","value":" 5/5 [00:00&lt;00:00,  4.54it/s]"}},"6cddca2465c249a2b26a73dddddee383":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7fb49789a01479e87d946d4e160f61f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e4574d3f3b64f2a984838a9dc869a94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7a6bba068f04708a8a17e2b6c1809ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64d46af1bef5423f80704f9e32bebb8c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a163556d8efd4daea4fecc03bc40428d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f106e58e3954af1a142d9496de46d5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9568336b9ae74caab914f832741217a4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_837959fa83024865a41c03b1a93a2443","IPY_MODEL_56eb974edf9342bea0e94aa60c65a6f1","IPY_MODEL_9f5512648c1a45a693001e60d635bb28"],"layout":"IPY_MODEL_5ff69c5014ea4496b12a10ba6d83f971"}},"837959fa83024865a41c03b1a93a2443":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8fb045db80f4289b04c72dcb5595e8a","placeholder":"​","style":"IPY_MODEL_2211a39ad09f4e1b84d59edf670e47ff","value":"100%"}},"56eb974edf9342bea0e94aa60c65a6f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd38441ec4da4f999dbfa9be01e64262","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_12a920410811414fa19372eabb16eecd","value":10}},"9f5512648c1a45a693001e60d635bb28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c13203382d6c49c5b565d147b2f8374f","placeholder":"​","style":"IPY_MODEL_ccf875d9eec44e888ca54f3f7dd4da2a","value":" 10/10 [00:19&lt;00:00,  1.01it/s]"}},"5ff69c5014ea4496b12a10ba6d83f971":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8fb045db80f4289b04c72dcb5595e8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2211a39ad09f4e1b84d59edf670e47ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd38441ec4da4f999dbfa9be01e64262":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12a920410811414fa19372eabb16eecd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c13203382d6c49c5b565d147b2f8374f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccf875d9eec44e888ca54f3f7dd4da2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a016768197e841b092f7d43c10938690":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7cbb6d7eb1754cd9a12d06b99c4cf087","IPY_MODEL_50ef3f7e86b3490ab109b62e015262c6","IPY_MODEL_f3d1a444a0724fd4a8ff3ca998e5775d"],"layout":"IPY_MODEL_38a0e06219ca4098b718d6318a9915ba"}},"7cbb6d7eb1754cd9a12d06b99c4cf087":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04303a92657e47dfb501707075fdcb8b","placeholder":"​","style":"IPY_MODEL_3716bdde1a1343f3a955288bce3d5754","value":"100%"}},"50ef3f7e86b3490ab109b62e015262c6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_55687145083f4fa1bf2d830f160fbef6","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c0ebe50012ab45539807af56d105811a","value":5}},"f3d1a444a0724fd4a8ff3ca998e5775d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5653be52a5554668bd4ebec680555a4a","placeholder":"​","style":"IPY_MODEL_4e75906132354b2e80d50b071ce27ffb","value":" 5/5 [00:00&lt;00:00, 23.00it/s]"}},"38a0e06219ca4098b718d6318a9915ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04303a92657e47dfb501707075fdcb8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3716bdde1a1343f3a955288bce3d5754":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55687145083f4fa1bf2d830f160fbef6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0ebe50012ab45539807af56d105811a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5653be52a5554668bd4ebec680555a4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e75906132354b2e80d50b071ce27ffb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df4d893372e74658af4b08748809f476":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7d3d8d34e04437f8e899dc2f112f19e","IPY_MODEL_6f56ab5a26444c82958421803b861795","IPY_MODEL_722ed9a0447443d39d5c17e30b496288"],"layout":"IPY_MODEL_d472b588a6f2431a8cb6ea64e62200b6"}},"c7d3d8d34e04437f8e899dc2f112f19e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b16dde3fc784c7baa0b97b9b8c35a74","placeholder":"​","style":"IPY_MODEL_f8196072fea244b89690d2bed3f59e3c","value":"100%"}},"6f56ab5a26444c82958421803b861795":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7d3a1cce57c4caa822a1e5420b8a8e3","max":9,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd0c87c8c53e43aeb3628729a0ab1ae2","value":9}},"722ed9a0447443d39d5c17e30b496288":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_412ee84c3c804576bf93da3708bfdc0d","placeholder":"​","style":"IPY_MODEL_18a7a38fcf2c4d85ad8f0abb011bb641","value":" 9/9 [00:08&lt;00:00,  2.14it/s]"}},"d472b588a6f2431a8cb6ea64e62200b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b16dde3fc784c7baa0b97b9b8c35a74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8196072fea244b89690d2bed3f59e3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7d3a1cce57c4caa822a1e5420b8a8e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd0c87c8c53e43aeb3628729a0ab1ae2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"412ee84c3c804576bf93da3708bfdc0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18a7a38fcf2c4d85ad8f0abb011bb641":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71e871bebffa4340b00738c52f64d28a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cdc37dba25ab4013a8a26538437c5d0b","IPY_MODEL_e79bd1e3d8b8483d8b995b4163cddf14","IPY_MODEL_7b1d0af7e60e41249bd20205c82f1e42"],"layout":"IPY_MODEL_a6e6b64c20d5430c925c4cb49ce3040b"}},"cdc37dba25ab4013a8a26538437c5d0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a785249d63424f559d4b294817ee0a88","placeholder":"​","style":"IPY_MODEL_ef0b523d646e4c9b920076cdd80fd83f","value":"100%"}},"e79bd1e3d8b8483d8b995b4163cddf14":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_562528df95ea4332a8a072fac8dfb6c0","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2d8968ffea9457c90a6e189a5785a3a","value":5}},"7b1d0af7e60e41249bd20205c82f1e42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34c769be5f4e47a989e1646d6da9e893","placeholder":"​","style":"IPY_MODEL_ec19693e0b6d40a687713664d921e22a","value":" 5/5 [00:00&lt;00:00, 32.51it/s]"}},"a6e6b64c20d5430c925c4cb49ce3040b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a785249d63424f559d4b294817ee0a88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef0b523d646e4c9b920076cdd80fd83f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"562528df95ea4332a8a072fac8dfb6c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2d8968ffea9457c90a6e189a5785a3a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34c769be5f4e47a989e1646d6da9e893":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec19693e0b6d40a687713664d921e22a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}